# PA2018-2019-KA

## Provenance

Ce dépôt contient les sources de code prisent sur le blog de M. Simonini ayant trait à l'algorithme de Double Duelling Q-Network with Prioritized Experience Replay aux l'adresses :

  - https://medium.freecodecamp.org/improvements-in-deep-q-learning-dueling-double-dqn-prioritized-experience-replay-and-fixed-58b130cc5682 
  - https://github.com/simoninithomas/Deep_reinforcement_learning_Course/blob/master/Dueling%20Double%20DQN%20with%20PER%20and%20fixed-q%20targets/Dueling%20Deep%20Q%20Learning%20with%20Doom%20%28%2B%20double%20DQNs%20and%20Prioritized%20Experience%20Replay%29.ipynb

Le code de l'algorithme C51 venant d'un dépôt Github :

  - https://github.com/flyyufelix/C51-DDQN-Keras.

Ce dépôt contient le code lancé avec certaines configurations de paramètre décrite dans le document de projet.

## Résultats

Ce dossier contient les fichiers **.txt** où nous avons de statistiques et des fichiers **.pickle** qui devaient sauvegarder les mêmes résultas mais cela semble avoir échoué pour une reaison inconnue.

Le dossier **models** contient les modèles de paramètres obtenus par la librairie **tensorflow** ainsi que le **tensorboard** où nous aovns des données graphiques qui est l'erreur (loss) dans notre cas.

Karim Azzab, le 14 janvier 2019
