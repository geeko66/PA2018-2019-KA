{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DDDQN  (Double Dueling Deep Q Learning with Prioritized Experience Replay)  DoomüïπÔ∏è\n",
    "In this notebook we'll implement an agent <b>that plays Doom by using a Dueling Double Deep Q learning architecture with Prioritized Experience Replay.</b> <br>\n",
    "\n",
    "Our agent playing Doom after 3 hours of training of **CPU**, remember that our agent needs about 2 days of **GPU** to have optimal score, we'll train from beginning to end the most important architectures (PPO with transfer):\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/simoninithomas/Deep_reinforcement_learning_Course/master/docs/assets/img/projects/doomdeathmatc.gif\" alt=\"Doom Deathmatch\"/>\n",
    "\n",
    "But we can see that our agent **understand that he needs to kill enemies before being able to move forward (if he moves forward without killing ennemies he will be killed before getting the vest)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is a notebook from [Deep Reinforcement Learning Course with Tensorflow](https://simoninithomas.github.io/Deep_reinforcement_learning_Course/)\n",
    "<img src=\"https://raw.githubusercontent.com/simoninithomas/Deep_reinforcement_learning_Course/master/docs/assets/img/DRLC%20Environments.png\" alt=\"Deep Reinforcement Course\"/>\n",
    "<br>\n",
    "<p>  Deep Reinforcement Learning Course is a free series of articles and videos tutorials üÜï about Deep Reinforcement Learning, where **we'll learn the main algorithms (Q-learning, Deep Q Nets, Dueling Deep Q Nets, Policy Gradients, A2C, Proximal Policy Gradients‚Ä¶), and how to implement them with Tensorflow.**\n",
    "<br><br>\n",
    "    \n",
    "üìúThe articles explain the architectures from the big picture to the mathematical details behind them.\n",
    "<br>\n",
    "üìπ The videos explain how to build the agents with Tensorflow </b></p>\n",
    "<br>\n",
    "This course will give you a **solid foundation for understanding and implementing the future state of the art algorithms**. And, you'll build a strong professional portfolio by creating **agents that learn to play awesome environments**: Doom¬© üëπ, Space invaders üëæ, Outrun, Sonic the Hedgehog¬©, Michael Jackson‚Äôs Moonwalker, agents that will be able to navigate in 3D environments with DeepMindLab (Quake) and able to walk with Mujoco. \n",
    "<br><br>\n",
    "</p> \n",
    "\n",
    "## üìö The complete [Syllabus HERE](https://simoninithomas.github.io/Deep_reinforcement_learning_Course/)\n",
    "\n",
    "\n",
    "## Any questions üë®‚Äçüíª\n",
    "<p> If you have any questions, feel free to ask me: </p>\n",
    "<p> üìß: <a href=\"mailto:hello@simoninithomas.com\">hello@simoninithomas.com</a>  </p>\n",
    "<p> Github: https://github.com/simoninithomas/Deep_reinforcement_learning_Course </p>\n",
    "<p> üåê : https://simoninithomas.github.io/Deep_reinforcement_learning_Course/ </p>\n",
    "<p> Twitter: <a href=\"https://twitter.com/ThomasSimonini\">@ThomasSimonini</a> </p>\n",
    "<p> Don't forget to <b> follow me on <a href=\"https://twitter.com/ThomasSimonini\">twitter</a>, <a href=\"https://github.com/simoninithomas/Deep_reinforcement_learning_Course\">github</a> and <a href=\"https://medium.com/@thomassimonini\">Medium</a> to be alerted of the new articles that I publish </b></p>\n",
    "    \n",
    "## How to help  üôå\n",
    "3 ways:\n",
    "- **Clap our articles and like our videos a lot**:Clapping in Medium means that you really like our articles. And the more claps we have, the more our article is shared Liking our videos help them to be much more visible to the deep learning community.\n",
    "- **Share and speak about our articles and videos**: By sharing our articles and videos you help us to spread the word. \n",
    "- **Improve our notebooks**: if you found a bug or **a better implementation** you can send a pull request.\n",
    "<br>\n",
    "\n",
    "## Important note ü§î\n",
    "<b> You can run it on your computer but it's better to run it on GPU based services</b>, personally I use Microsoft Azure and their Deep Learning Virtual Machine (they offer 170$)\n",
    "https://azuremarketplace.microsoft.com/en-us/marketplace/apps/microsoft-ads.dsvm-deep-learning\n",
    "<br>\n",
    "‚ö†Ô∏è I don't have any business relations with them. I just loved their excellent customer service.\n",
    "\n",
    "If you have some troubles to use Microsoft Azure follow the explainations of this excellent article here (without last the part fast.ai): https://medium.com/@manikantayadunanda/setting-up-deeplearning-machine-and-fast-ai-on-azure-a22eb6bd6429"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites üèóÔ∏è\n",
    "Before diving on the notebook **you need to understand**:\n",
    "- The foundations of Reinforcement learning (MC, TD, Rewards hypothesis...) [Article](https://medium.freecodecamp.org/an-introduction-to-reinforcement-learning-4339519de419)\n",
    "- Q-learning [Article](https://medium.freecodecamp.org/diving-deeper-into-reinforcement-learning-with-q-learning-c18d0db58efe)\n",
    "- Deep Q-Learning [Article](https://medium.freecodecamp.org/an-introduction-to-deep-q-learning-lets-play-doom-54d02d8017d8)\n",
    "- Improvments in Deep Q-learning [Article]()\n",
    "- You can follow this notebook using my [video tutorial](https://www.youtube.com/embed/-Ynjw0Vl3i4?showinfo=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda_3_5_1_0_64bits\\envs\\rl-tf-gpu-opengym\\lib\\site-packages\\IPython\\core\\display.py:689: UserWarning: Consider using IPython.display.IFrame instead\n",
      "  warnings.warn(\"Consider using IPython.display.IFrame instead\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/-Ynjw0Vl3i4?showinfo=0\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/-Ynjw0Vl3i4?showinfo=0\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import the libraries üìö"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf      # Deep Learning library\n",
    "import numpy as np           # Handle matrices\n",
    "from vizdoom import *        # Doom Environment\n",
    "\n",
    "import random                # Handling random number generation\n",
    "import time                  # Handling time calculation\n",
    "from skimage import transform# Help us to preprocess the frames\n",
    "\n",
    "from collections import deque# Ordered collection with ends\n",
    "import matplotlib.pyplot as plt # Display graphs\n",
    "\n",
    "import pickle\n",
    "\n",
    "import warnings # This ignore all the warning messages that are normally printed during the training because of skiimage\n",
    "warnings.filterwarnings('ignore') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Create our environment üéÆ\n",
    "- Now that we imported the libraries/dependencies, we will create our environment.\n",
    "- Doom environment takes:\n",
    "    - A `configuration file` that **handle all the options** (size of the frame, possible actions...)\n",
    "    - A `scenario file`: that **generates the correct scenario** (in our case basic **but you're invited to try other scenarios**).\n",
    "- Note: We have 7 possible actions: turn left, turn right, move left, move right, shoot (attack)...`[[0,0,0,0,1]...]` so we don't need to do one hot encoding (thanks to <a href=\"https://stackoverflow.com/users/2237916/silgon\">silgon</a> for figuring out). \n",
    "\n",
    "### Our environment\n",
    "<img src=\"https://simoninithomas.github.io/Deep_reinforcement_learning_Course/assets/img/video%20projects/deadlycorridor.png\" style=\"max-width:500px;\" alt=\"Vizdoom deadly corridor\"/>\n",
    "\n",
    "The purpose of this scenario is to teach the agent to navigate towards his fundamental goal (the vest) and make sure he survives at the same time.\n",
    "\n",
    "- Map is a corridor with shooting monsters on both sides (6 monsters in total). \n",
    "- A green vest is placed at the oposite end of the corridor. \n",
    "- **Reward is proportional (negative or positive) to change of the distance between the player and the vest.** \n",
    "- If player ignores monsters on the sides and runs straight for the vest he will be killed somewhere along the way. \n",
    "- To ensure this behavior doom_skill = 5 (config) is needed.\n",
    "\n",
    "<br>\n",
    "REWARDS:\n",
    "\n",
    "- +dX for getting closer to the vest. -dX for getting further from the vest.\n",
    "- death penalty = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Here we create our environment\n",
    "\"\"\"\n",
    "def create_environment():\n",
    "    game = DoomGame()\n",
    "    \n",
    "    # Load the correct configuration\n",
    "    game.load_config(\"defend_the_center.cfg\")\n",
    "    \n",
    "    # Load the correct scenario (in our case deadly_corridor scenario)\n",
    "    game.set_doom_scenario_path(\"defend_the_center.wad\")\n",
    "    game.set_sound_enabled(False)\n",
    "    game.set_screen_resolution(ScreenResolution.RES_640X480)\n",
    "    game.set_window_visible(False)\n",
    "    \n",
    "    game.init()\n",
    "\n",
    "    # Here we create an hot encoded version of our actions (5 possible actions)\n",
    "    # possible_actions = [[1, 0, 0, 0, 0], [0, 1, 0, 0, 0]...]\n",
    "    possible_actions = np.identity(3,dtype=int).tolist()\n",
    "    \n",
    "    return game, possible_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "game, possible_actions = create_environment()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Define the preprocessing functions ‚öôÔ∏è\n",
    "### preprocess_frame\n",
    "Preprocessing is an important step, <b>because we want to reduce the complexity of our states to reduce the computation time needed for training.</b>\n",
    "<br><br>\n",
    "Our steps:\n",
    "- Grayscale each of our frames (because <b> color does not add important information </b>). But this is already done by the config file.\n",
    "- Crop the screen (in our case we remove the roof because it contains no information)\n",
    "- We normalize pixel values\n",
    "- Finally we resize the preprocessed frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    preprocess_frame:\n",
    "    Take a frame.\n",
    "    Resize it.\n",
    "        __________________\n",
    "        |                 |\n",
    "        |                 |\n",
    "        |                 |\n",
    "        |                 |\n",
    "        |_________________|\n",
    "        \n",
    "        to\n",
    "        _____________\n",
    "        |            |\n",
    "        |            |\n",
    "        |            |\n",
    "        |____________|\n",
    "    Normalize it.\n",
    "    \n",
    "    return preprocessed_frame\n",
    "    \n",
    "    \"\"\"\n",
    "def preprocess_frame(frame):\n",
    "    # Crop the screen (remove part that contains no information)\n",
    "    # [Up: Down, Left: right]\n",
    "    # cropped_frame = frame[15:-5,20:-20]\n",
    "    \n",
    "    # Normalize Pixel Values\n",
    "    # normalized_frame = cropped_frame/255.0\n",
    "    \n",
    "    # Resize\n",
    "    preprocessed_frame = transform.resize(frame, [64, 64])\n",
    "    \n",
    "    return preprocessed_frame # 100x120x1 frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### stack_frames\n",
    "üëè This part was made possible thanks to help of <a href=\"https://github.com/Miffyli\">Anssi</a><br>\n",
    "\n",
    "As explained in this really <a href=\"https://danieltakeshi.github.io/2016/11/25/frame-skipping-and-preprocessing-for-deep-q-networks-on-atari-2600-games/\">  good article </a> we stack frames.\n",
    "\n",
    "Stacking frames is really important because it helps us to **give have a sense of motion to our Neural Network.**\n",
    "\n",
    "- First we preprocess frame\n",
    "- Then we append the frame to the deque that automatically **removes the oldest frame**\n",
    "- Finally we **build the stacked state**\n",
    "\n",
    "This is how work stack:\n",
    "- For the first frame, we feed 4 frames\n",
    "- At each timestep, **we add the new frame to deque and then we stack them to form a new stacked frame**\n",
    "- And so on\n",
    "<img src=\"https://raw.githubusercontent.com/simoninithomas/Deep_reinforcement_learning_Course/master/DQN/Space%20Invaders/assets/stack_frames.png\" alt=\"stack\">\n",
    "- If we're done, **we create a new stack with 4 new frames (because we are in a new episode)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack_size = 4 # We stack 4 frames\n",
    "\n",
    "# Initialize deque with zero-images one array for each image\n",
    "stacked_frames  =  deque([np.zeros((64,64), dtype=np.int) for i in range(stack_size)], maxlen=4) \n",
    "\n",
    "def stack_frames(stacked_frames, state, is_new_episode):\n",
    "    # Preprocess frame\n",
    "    frame = preprocess_frame(state)\n",
    "    \n",
    "    if is_new_episode:\n",
    "        # Clear our stacked_frames\n",
    "        stacked_frames = deque([np.zeros((64,64), dtype=np.int) for i in range(stack_size)], maxlen=4)\n",
    "        \n",
    "        # Because we're in a new episode, copy the same frame 4x\n",
    "        stacked_frames.append(frame)\n",
    "        stacked_frames.append(frame)\n",
    "        stacked_frames.append(frame)\n",
    "        stacked_frames.append(frame)\n",
    "        \n",
    "        # Stack the frames\n",
    "        stacked_state = np.stack(stacked_frames, axis=2)\n",
    "\n",
    "    else:\n",
    "        # Append frame to deque, automatically removes the oldest frame\n",
    "        stacked_frames.append(frame)\n",
    "\n",
    "        # Build the stacked state (first dimension specifies different frames)\n",
    "        stacked_state = np.stack(stacked_frames, axis=2) \n",
    "    \n",
    "    return stacked_state, stacked_frames\n",
    "\n",
    "# Add by Karim\n",
    "# We want to adapt the computation of our reward with this function\n",
    "def shape_reward(r_t, misc, prev_misc):\n",
    "    \"\"\"\n",
    "    Reward design:\n",
    "        Will be the inverted time in Bonseyes (x = -x) because\n",
    "        the time is the thing we want to minimize, therrefore we\n",
    "        maximize the invert time\n",
    "    \"\"\"\n",
    "    # Check any kill count\n",
    "    if (misc[0] > prev_misc[0]):\n",
    "        r_t = r_t + 1\n",
    "\n",
    "    if (misc[1] < prev_misc[1]): # Use ammo\n",
    "        r_t = r_t - 0.1\n",
    "\n",
    "    if (misc[2] < prev_misc[2]): # Loss HEALTH\n",
    "        r_t = r_t - 0.1\n",
    "\n",
    "    return r_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Set up our hyperparameters ‚öóÔ∏è\n",
    "In this part we'll set up our different hyperparameters. But when you implement a Neural Network by yourself you will **not implement hyperparamaters at once but progressively**.\n",
    "\n",
    "- First, you begin by defining the neural networks hyperparameters when you implement the model.\n",
    "- Then, you'll add the training hyperparameters when you implement the training algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MODEL HYPERPARAMETERS\n",
    "state_size = [64,64,4]      # Our input is a stack of 4 frames hence 100x120x4 (Width, height, channels) \n",
    "action_size = game.get_available_buttons_size()              # 7 possible actions\n",
    "learning_rate =  0.00025      # Alpha (aka learning rate)\n",
    "\n",
    "### TRAINING HYPERPARAMETERS\n",
    "total_episodes = 12000         # Total episodes for training\n",
    "max_steps = 8000               # Max possible steps in an episode 500\n",
    "batch_size = 64                # 64\n",
    "\n",
    "# FIXED Q TARGETS HYPERPARAMETERS \n",
    "max_tau = 10000 #Tau is the C step where we update our target network\n",
    "\n",
    "# EXPLORATION HYPERPARAMETERS for epsilon greedy strategy\n",
    "explore_start = 1.0            # exploration probability at start\n",
    "explore_stop = 0.01            # minimum exploration probability \n",
    "decay_rate = 0.00005            # exponential decay rate for exploration prob\n",
    "\n",
    "# Q LEARNING hyperparameters\n",
    "gamma = 0.95               # Discounting rate\n",
    "\n",
    "### MEMORY HYPERPARAMETERS\n",
    "## If you have GPU change to 1million\n",
    "pretrain_length = 300000   # Number of experiences stored in the Memory when initialized for the first time\n",
    "memory_size = 300000      # Number of experiences the Memory can keep\n",
    "\n",
    "### MODIFY THIS TO FALSE IF YOU JUST WANT TO SEE THE TRAINED AGENT\n",
    "training = True\n",
    "\n",
    "## TURN THIS TO TRUE IF YOU WANT TO RENDER THE ENVIRONMENT\n",
    "episode_render = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Create our Dueling Double Deep Q-learning Neural Network model (aka DDDQN) üß†\n",
    "<img src=\"https://cdn-images-1.medium.com/max/1500/1*FkHqwA2eSGixdS-3dvVoMA.png\" alt=\"Dueling Double Deep Q Learning Model\" />\n",
    "This is our Dueling Double Deep Q-learning model:\n",
    "- We take a stack of 4 frames as input\n",
    "- It passes through 3 convnets\n",
    "- Then it is flatened\n",
    "- Then it is passed through 2 streams\n",
    "    - One that calculates V(s)\n",
    "    - The other that calculates A(s,a)\n",
    "- Finally an agregating layer\n",
    "- It outputs a Q value for each actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDDQNNet:\n",
    "    def __init__(self, state_size, action_size, learning_rate, name):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.name = name\n",
    "        \n",
    "        \n",
    "        # We use tf.variable_scope here to know which network we're using (DQN or target_net)\n",
    "        # it will be useful when we will update our w- parameters (by copy the DQN parameters)\n",
    "        with tf.variable_scope(self.name):\n",
    "            \n",
    "            # We create the placeholders\n",
    "            # *state_size means that we take each elements of state_size in tuple hence is like if we wrote\n",
    "            # [None, 100, 120, 4]\n",
    "            self.inputs_ = tf.placeholder(tf.float32, [None, *state_size], name=\"inputs\")\n",
    "            \n",
    "            #\n",
    "            self.ISWeights_ = tf.placeholder(tf.float32, [None,1], name='IS_weights')\n",
    "            \n",
    "            self.actions_ = tf.placeholder(tf.float32, [None, action_size], name=\"actions_\")\n",
    "            \n",
    "            # Remember that target_Q is the R(s,a) + ymax Qhat(s', a')\n",
    "            self.target_Q = tf.placeholder(tf.float32, [None], name=\"target\")\n",
    "            \n",
    "            \"\"\"\n",
    "            First convnet:\n",
    "            CNN\n",
    "            ELU\n",
    "            \"\"\"\n",
    "            # Input is 100x120x4\n",
    "            self.conv1 = tf.layers.conv2d(inputs = self.inputs_,\n",
    "                                         filters = 32,\n",
    "                                         kernel_size = [8,8],\n",
    "                                         strides = [4,4],\n",
    "                                         padding = \"VALID\",\n",
    "                                         kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d(),\n",
    "                                         name = \"conv1\")\n",
    "            \n",
    "            self.conv1_out = tf.nn.elu(self.conv1, name=\"conv1_out\")\n",
    "            \n",
    "            \n",
    "            \"\"\"\n",
    "            Second convnet:\n",
    "            CNN\n",
    "            ELU\n",
    "            \"\"\"\n",
    "            self.conv2 = tf.layers.conv2d(inputs = self.conv1_out,\n",
    "                                 filters = 64,\n",
    "                                 kernel_size = [4,4],\n",
    "                                 strides = [2,2],\n",
    "                                 padding = \"VALID\",\n",
    "                                 kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d(),\n",
    "                                 name = \"conv2\")\n",
    "\n",
    "            self.conv2_out = tf.nn.elu(self.conv2, name=\"conv2_out\")\n",
    "            \n",
    "            \n",
    "            \"\"\"\n",
    "            Third convnet:\n",
    "            CNN\n",
    "            ELU\n",
    "            \"\"\"\n",
    "            self.conv3 = tf.layers.conv2d(inputs = self.conv2_out,\n",
    "                                 filters = 64,\n",
    "                                 kernel_size = [3,3],\n",
    "                                 strides = [1,1],\n",
    "                                 padding = \"VALID\",\n",
    "                                kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d(),\n",
    "                                 name = \"conv3\")\n",
    "\n",
    "            self.conv3_out = tf.nn.elu(self.conv3, name=\"conv3_out\")\n",
    "            \n",
    "            \n",
    "            self.flatten = tf.layers.flatten(self.conv3_out)\n",
    "            \n",
    "            \n",
    "            ## Here we separate into two streams\n",
    "            # The one that calculate V(s)\n",
    "            self.value_fc = tf.layers.dense(inputs = self.flatten,\n",
    "                                  units = 512,\n",
    "                                  activation = tf.nn.elu,\n",
    "                                       kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                name=\"value_fc\")\n",
    "            \n",
    "            self.value = tf.layers.dense(inputs = self.value_fc,\n",
    "                                        units = 1,\n",
    "                                        activation = None,\n",
    "                                        kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                name=\"value\")\n",
    "            \n",
    "            # The one that calculate A(s,a)\n",
    "            self.advantage_fc = tf.layers.dense(inputs = self.flatten,\n",
    "                                  units = 512,\n",
    "                                  activation = tf.nn.elu,\n",
    "                                       kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                name=\"advantage_fc\")\n",
    "            \n",
    "            self.advantage = tf.layers.dense(inputs = self.advantage_fc,\n",
    "                                        units = self.action_size,\n",
    "                                        activation = None,\n",
    "                                        kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                name=\"advantages\")\n",
    "            \n",
    "            # Agregating layer\n",
    "            # Q(s,a) = V(s) + (A(s,a) - 1/|A| * sum A(s,a'))\n",
    "            self.output = self.value + tf.subtract(self.advantage, tf.reduce_mean(self.advantage, axis=1, keepdims=True))\n",
    "              \n",
    "            # Q is our predicted Q value.\n",
    "            self.Q = tf.reduce_sum(tf.multiply(self.output, self.actions_), axis=1)\n",
    "            \n",
    "            # The loss is modified because of PER \n",
    "            self.absolute_errors = tf.abs(self.target_Q - self.Q)# for updating Sumtree\n",
    "            \n",
    "            self.loss = tf.reduce_mean(self.ISWeights_ * tf.squared_difference(self.target_Q, self.Q))\n",
    "            \n",
    "            self.optimizer = tf.train.RMSPropOptimizer(self.learning_rate).minimize(self.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the graph\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Instantiate the DQNetwork\n",
    "DQNetwork = DDDQNNet(state_size, action_size, learning_rate, name=\"DQNetwork\")\n",
    "\n",
    "# Instantiate the target network\n",
    "TargetNetwork = DDDQNNet(state_size, action_size, learning_rate, name=\"TargetNetwork\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Prioritized Experience Replay üîÅ\n",
    "Now that we create our Neural Network, **we need to implement the Prioritized Experience Replay method.** <br>\n",
    "\n",
    "As explained in the article, **we can't use a simple array to do that because sampling from it will be not efficient, so we use a binary tree data type (in a binary tree each node has no + than 2 children).** More precisely, a sumtree, which is a binary tree where parents nodes are the sum of the children nodes.\n",
    "\n",
    "If you don't know what is a binary tree check this awesome video https://www.youtube.com/watch?v=oSWTXtMglKE\n",
    "\n",
    "\n",
    "This SumTree implementation was taken from Morvan Zhou in his chinese course about Reinforcement Learning\n",
    "\n",
    "To summarize:\n",
    "- **Step 1**: We construct a SumTree, which is a Binary Sum tree where leaves contains the priorities and a data array where index points to the index of leaves.\n",
    "    <img src=\"https://cdn-images-1.medium.com/max/1200/1*Go9DNr7YY-wMGdIQ7HQduQ.png\" alt=\"SumTree\"/>\n",
    "    <br><br>\n",
    "    - **def __init__**: Initialize our SumTree data object with all nodes = 0 and data (data array) with all = 0.\n",
    "    - **def add**: add our priority score in the sumtree leaf and experience (S, A, R, S', Done) in data.\n",
    "    - **def update**: we update the leaf priority score and propagate through tree.\n",
    "    - **def get_leaf**: retrieve priority score, index and experience associated with a leaf.\n",
    "    - **def total_priority**: get the root node value to calculate the total priority score of our replay buffer.\n",
    "<br><br>\n",
    "- **Step 2**: We create a Memory object that will contain our sumtree and data.\n",
    "    - **def __init__**: generates our sumtree and data by instantiating the SumTree object.\n",
    "    - **def store**: we store a new experience in our tree. Each new experience will **have priority = max_priority** (and then this priority will be corrected during the training (when we'll calculating the TD error hence the priority score).\n",
    "    - **def sample**:\n",
    "         - First, to sample a minibatch of k size, the range [0, priority_total] is / into k ranges.\n",
    "         - Then a value is uniformly sampled from each range\n",
    "         - We search in the sumtree, the experience where priority score correspond to sample values are retrieved from.\n",
    "         - Then, we calculate IS weights for each minibatch element\n",
    "    - **def update_batch**: update the priorities on the tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SumTree(object):\n",
    "    \"\"\"\n",
    "    This SumTree code is modified version of Morvan Zhou: \n",
    "    https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow/blob/master/contents/5.2_Prioritized_Replay_DQN/RL_brain.py\n",
    "    \"\"\"\n",
    "    data_pointer = 0\n",
    "    \n",
    "    \"\"\"\n",
    "    Here we initialize the tree with all nodes = 0, and initialize the data with all values = 0\n",
    "    \"\"\"\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity # Number of leaf nodes (final nodes) that contains experiences\n",
    "        \n",
    "        # Generate the tree with all nodes values = 0\n",
    "        # To understand this calculation (2 * capacity - 1) look at the schema above\n",
    "        # Remember we are in a binary node (each node has max 2 children) so 2x size of leaf (capacity) - 1 (root node)\n",
    "        # Parent nodes = capacity - 1\n",
    "        # Leaf nodes = capacity\n",
    "        self.tree = np.zeros(2 * capacity - 1)\n",
    "        \n",
    "        \"\"\" tree:\n",
    "            0\n",
    "           / \\\n",
    "          0   0\n",
    "         / \\ / \\\n",
    "        0  0 0  0  [Size: capacity] it's at this line that there is the priorities score (aka pi)\n",
    "        \"\"\"\n",
    "        \n",
    "        # Contains the experiences (so the size of data is capacity)\n",
    "        self.data = np.zeros(capacity, dtype=object)\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    Here we add our priority score in the sumtree leaf and add the experience in data\n",
    "    \"\"\"\n",
    "    def add(self, priority, data):\n",
    "        # Look at what index we want to put the experience\n",
    "        tree_index = self.data_pointer + self.capacity - 1\n",
    "        \n",
    "        \"\"\" tree:\n",
    "            0\n",
    "           / \\\n",
    "          0   0\n",
    "         / \\ / \\\n",
    "tree_index  0 0  0  We fill the leaves from left to right\n",
    "        \"\"\"\n",
    "        \n",
    "        # Update data frame\n",
    "        self.data[self.data_pointer] = data\n",
    "        \n",
    "        # Update the leaf\n",
    "        self.update (tree_index, priority)\n",
    "        \n",
    "        # Add 1 to data_pointer\n",
    "        self.data_pointer += 1\n",
    "        \n",
    "        if self.data_pointer >= self.capacity:  # If we're above the capacity, you go back to first index (we overwrite)\n",
    "            self.data_pointer = 0\n",
    "            \n",
    "    \n",
    "    \"\"\"\n",
    "    Update the leaf priority score and propagate the change through tree\n",
    "    \"\"\"\n",
    "    def update(self, tree_index, priority):\n",
    "        # Change = new priority score - former priority score\n",
    "        change = priority - self.tree[tree_index]\n",
    "        self.tree[tree_index] = priority\n",
    "        \n",
    "        # then propagate the change through tree\n",
    "        while tree_index != 0:    # this method is faster than the recursive loop in the reference code\n",
    "            \n",
    "            \"\"\"\n",
    "            Here we want to access the line above\n",
    "            THE NUMBERS IN THIS TREE ARE THE INDEXES NOT THE PRIORITY VALUES\n",
    "            \n",
    "                0\n",
    "               / \\\n",
    "              1   2\n",
    "             / \\ / \\\n",
    "            3  4 5  [6] \n",
    "            \n",
    "            If we are in leaf at index 6, we updated the priority score\n",
    "            We need then to update index 2 node\n",
    "            So tree_index = (tree_index - 1) // 2\n",
    "            tree_index = (6-1)//2\n",
    "            tree_index = 2 (because // round the result)\n",
    "            \"\"\"\n",
    "            tree_index = (tree_index - 1) // 2\n",
    "            self.tree[tree_index] += change\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    Here we get the leaf_index, priority value of that leaf and experience associated with that index\n",
    "    \"\"\"\n",
    "    def get_leaf(self, v):\n",
    "        \"\"\"\n",
    "        Tree structure and array storage:\n",
    "        Tree index:\n",
    "             0         -> storing priority sum\n",
    "            / \\\n",
    "          1     2\n",
    "         / \\   / \\\n",
    "        3   4 5   6    -> storing priority for experiences\n",
    "        Array type for storing:\n",
    "        [0,1,2,3,4,5,6]\n",
    "        \"\"\"\n",
    "        parent_index = 0\n",
    "        \n",
    "        while True: # the while loop is faster than the method in the reference code\n",
    "            left_child_index = 2 * parent_index + 1\n",
    "            right_child_index = left_child_index + 1\n",
    "            \n",
    "            # If we reach bottom, end the search\n",
    "            if left_child_index >= len(self.tree):\n",
    "                leaf_index = parent_index\n",
    "                break\n",
    "            \n",
    "            else: # downward search, always search for a higher priority node\n",
    "                \n",
    "                if v <= self.tree[left_child_index]:\n",
    "                    parent_index = left_child_index\n",
    "                    \n",
    "                else:\n",
    "                    v -= self.tree[left_child_index]\n",
    "                    parent_index = right_child_index\n",
    "            \n",
    "        data_index = leaf_index - self.capacity + 1\n",
    "\n",
    "        return leaf_index, self.tree[leaf_index], self.data[data_index]\n",
    "    \n",
    "    @property\n",
    "    def total_priority(self):\n",
    "        return self.tree[0] # Returns the root node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we don't use deque anymore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory(object):  # stored as ( s, a, r, s_ ) in SumTree\n",
    "    \"\"\"\n",
    "    This SumTree code is modified version and the original code is from:\n",
    "    https://github.com/jaara/AI-blog/blob/master/Seaquest-DDQN-PER.py\n",
    "    \"\"\"\n",
    "    PER_e = 0.01  # Hyperparameter that we use to avoid some experiences to have 0 probability of being taken\n",
    "    PER_a = 0.6  # Hyperparameter that we use to make a tradeoff between taking only exp with high priority and sampling randomly\n",
    "    PER_b = 0.4  # importance-sampling, from initial value increasing to 1\n",
    "    \n",
    "    PER_b_increment_per_sampling = 0.001\n",
    "    \n",
    "    absolute_error_upper = 1.  # clipped abs error\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        # Making the tree \n",
    "        \"\"\"\n",
    "        Remember that our tree is composed of a sum tree that contains the priority scores at his leaf\n",
    "        And also a data array\n",
    "        We don't use deque because it means that at each timestep our experiences change index by one.\n",
    "        We prefer to use a simple array and to overwrite when the memory is full.\n",
    "        \"\"\"\n",
    "        self.tree = SumTree(capacity)\n",
    "        \n",
    "    \"\"\"\n",
    "    Store a new experience in our tree\n",
    "    Each new experience have a score of max_prority (it will be then improved when we use this exp to train our DDQN)\n",
    "    \"\"\"\n",
    "    def store(self, experience):\n",
    "        # Find the max priority\n",
    "        max_priority = np.max(self.tree.tree[-self.tree.capacity:])\n",
    "        \n",
    "        # If the max priority = 0 we can't put priority = 0 since this exp will never have a chance to be selected\n",
    "        # So we use a minimum priority\n",
    "        if max_priority == 0:\n",
    "            max_priority = self.absolute_error_upper\n",
    "        \n",
    "        self.tree.add(max_priority, experience)   # set the max p for new p\n",
    "\n",
    "        \n",
    "    \"\"\"\n",
    "    - First, to sample a minibatch of k size, the range [0, priority_total] is / into k ranges.\n",
    "    - Then a value is uniformly sampled from each range\n",
    "    - We search in the sumtree, the experience where priority score correspond to sample values are retrieved from.\n",
    "    - Then, we calculate IS weights for each minibatch element\n",
    "    \"\"\"\n",
    "    def sample(self, n):\n",
    "        # Create a sample array that will contains the minibatch\n",
    "        memory_b = []\n",
    "        \n",
    "        b_idx, b_ISWeights = np.empty((n,), dtype=np.int32), np.empty((n, 1), dtype=np.float32)\n",
    "        \n",
    "        # Calculate the priority segment\n",
    "        # Here, as explained in the paper, we divide the Range[0, ptotal] into n ranges\n",
    "        priority_segment = self.tree.total_priority / n       # priority segment\n",
    "    \n",
    "        # Here we increasing the PER_b each time we sample a new minibatch\n",
    "        self.PER_b = np.min([1., self.PER_b + self.PER_b_increment_per_sampling])  # max = 1\n",
    "        \n",
    "        # Calculating the max_weight\n",
    "        p_min = np.min(self.tree.tree[-self.tree.capacity:]) / self.tree.total_priority\n",
    "        max_weight = (p_min * n) ** (-self.PER_b)\n",
    "        \n",
    "        for i in range(n):\n",
    "            \"\"\"\n",
    "            A value is uniformly sample from each range\n",
    "            \"\"\"\n",
    "            a, b = priority_segment * i, priority_segment * (i + 1)\n",
    "            value = np.random.uniform(a, b)\n",
    "            \n",
    "            \"\"\"\n",
    "            Experience that correspond to each value is retrieved\n",
    "            \"\"\"\n",
    "            index, priority, data = self.tree.get_leaf(value)\n",
    "            \n",
    "            #P(j)\n",
    "            sampling_probabilities = priority / self.tree.total_priority\n",
    "            \n",
    "            #  IS = (1/N * 1/P(i))**b /max wi == (N*P(i))**-b  /max wi\n",
    "            b_ISWeights[i, 0] = np.power(n * sampling_probabilities, -self.PER_b)/ max_weight\n",
    "                                   \n",
    "            b_idx[i]= index\n",
    "            \n",
    "            experience = [data]\n",
    "            \n",
    "            memory_b.append(experience)\n",
    "        \n",
    "        return b_idx, memory_b, b_ISWeights\n",
    "    \n",
    "    \"\"\"\n",
    "    Update the priorities on the tree\n",
    "    \"\"\"\n",
    "    def batch_update(self, tree_idx, abs_errors):\n",
    "        abs_errors += self.PER_e  # convert to abs and avoid 0\n",
    "        clipped_errors = np.minimum(abs_errors, self.absolute_error_upper)\n",
    "        ps = np.power(clipped_errors, self.PER_a)\n",
    "\n",
    "        for ti, p in zip(tree_idx, ps):\n",
    "            self.tree.update(ti, p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we'll **deal with the empty memory problem**: we pre-populate our memory by taking random actions and storing the experience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 0 items\n",
      "processed 30000 items\n",
      "processed 60000 items\n",
      "processed 90000 items\n",
      "processed 120000 items\n",
      "processed 150000 items\n",
      "processed 180000 items\n",
      "processed 210000 items\n",
      "processed 240000 items\n",
      "processed 270000 items\n"
     ]
    }
   ],
   "source": [
    "# Instantiate memory\n",
    "memory = Memory(memory_size)\n",
    "\n",
    "# Render the environment\n",
    "game.new_episode()\n",
    "reward = 0\n",
    "\n",
    "game_state = game.get_state()\n",
    "misc = game_state.game_variables  # [KILLCOUNT, AMMO, HEALTH]\n",
    "prev_misc = misc\n",
    "\n",
    "for i in range(pretrain_length):\n",
    "    # If it's the first step\n",
    "    if i == 0:\n",
    "        # First we need a state\n",
    "        state = game.get_state().screen_buffer\n",
    "        state, stacked_frames = stack_frames(stacked_frames, state, True)  \n",
    "    # Random action\n",
    "    action = random.choice(possible_actions)\n",
    "    # Add by Karim\n",
    "    game_state = game.get_state()\n",
    "    misc = game_state.game_variables  # [KILLCOUNT, AMMO, HEALTH]\n",
    "    game.set_action(action)\n",
    "    # 4 for the skip rate as we stack 4 frames, we ask to take the same action four times\n",
    "    game.advance_action(4)     \n",
    "    game.get_state()\n",
    "    reward = game.get_last_reward()\n",
    "    # Add by Karim\n",
    "    game_state = game.get_state()  \n",
    "    reward = shape_reward(reward, misc, prev_misc)\n",
    "    # Get the rewards\n",
    "    # reward = game.make_action(action)\n",
    "    \n",
    "    # Look if the episode is finished\n",
    "    done = game.is_episode_finished()\n",
    "\n",
    "    # If we're dead\n",
    "    if done:\n",
    "        # We finished the episode\n",
    "        next_state = np.zeros(state.shape)\n",
    "        \n",
    "        # Add experience to memory\n",
    "        #experience = np.hstack((state, [action, reward], next_state, done))\n",
    "        \n",
    "        experience = state, action, reward, next_state, done\n",
    "        memory.store(experience)\n",
    "        reward = 0\n",
    "        # Start a new episode\n",
    "        game.new_episode()\n",
    "        \n",
    "        # First we need a state\n",
    "        state = game.get_state().screen_buffer\n",
    "        \n",
    "        # Stack the frames\n",
    "        state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
    "        \n",
    "    else:\n",
    "        # Get the next state\n",
    "        next_state = game.get_state().screen_buffer\n",
    "        next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
    "        \n",
    "        # Add experience to memory\n",
    "        experience = state, action, reward, next_state, done\n",
    "        memory.store(experience)\n",
    "        \n",
    "        # Our state is now the next_state\n",
    "        state = next_state\n",
    "    if i % 30000 == 0:\n",
    "        print(\"processed \" + str(i) + \" items\")\n",
    "    # print(\"MISC: \" + str(misc) + \"\\tPrev Misc: \" + str(prev_misc))\n",
    "    prev_misc = misc    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Set up Tensorboard üìä\n",
    "For more information about tensorboard, please watch this <a href=\"https://www.youtube.com/embed/eBbEDRsCmv4\">excellent 30min tutorial</a> <br><br>\n",
    "To launch tensorboard : `tensorboard --logdir=/tensorboard/dddqn/1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup TensorBoard Writer\n",
    "writer = tf.summary.FileWriter(r\".\\tensorboard\\dddqn\\1\")\n",
    "\n",
    "## Losses\n",
    "tf.summary.scalar(\"Loss\", DQNetwork.loss)\n",
    "\n",
    "write_op = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Train our Agent üèÉ‚Äç‚ôÇÔ∏è\n",
    "\n",
    "Our algorithm:\n",
    "<br>\n",
    "* Initialize the weights for DQN\n",
    "* Initialize target value weights w- <- w\n",
    "* Init the environment\n",
    "* Initialize the decay rate (that will use to reduce epsilon) \n",
    "<br><br>\n",
    "* **For** episode to max_episode **do** \n",
    "    * Make new episode\n",
    "    * Set step to 0\n",
    "    * Observe the first state $s_0$\n",
    "    <br><br>\n",
    "    * **While** step < max_steps **do**:\n",
    "        * Increase decay_rate\n",
    "        * With $\\epsilon$ select a random action $a_t$, otherwise select $a_t = \\mathrm{argmax}_a Q(s_t,a)$\n",
    "        * Execute action $a_t$ in simulator and observe reward $r_{t+1}$ and new state $s_{t+1}$\n",
    "        * Store transition $<s_t, a_t, r_{t+1}, s_{t+1}>$ in memory $D$\n",
    "        \n",
    "        * Sample random mini-batch from $D$: $<s, a, r, s'>$\n",
    "        * Set target $\\hat{Q} = r$ if the episode ends at $+1$, otherwise set $\\hat{Q} = r + \\gamma Q(s',argmax_{a'}{Q(s', a', w), w^-)}$\n",
    "        * Make a gradient descent step with loss $(\\hat{Q} - Q(s, a))^2$\n",
    "        * Every C steps, reset: $w^- \\leftarrow w$\n",
    "    * **endfor**\n",
    "    <br><br>\n",
    "* **endfor**\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This function will do the part\n",
    "With œµ select a random action atat, otherwise select at=argmaxaQ(st,a)\n",
    "\"\"\"\n",
    "def predict_action(explore_start, explore_stop, decay_rate, decay_step, state, actions):\n",
    "    ## EPSILON GREEDY STRATEGY\n",
    "    # Choose action a from state s using epsilon greedy.\n",
    "    ## First we randomize a number\n",
    "    exp_exp_tradeoff = np.random.rand()\n",
    "\n",
    "    # Here we'll use an improved version of our epsilon greedy strategy used in Q-learning notebook\n",
    "    explore_probability = explore_stop + (explore_start - explore_stop) * np.exp(-decay_rate * decay_step)\n",
    "    \n",
    "    if (explore_probability > exp_exp_tradeoff):\n",
    "        # Make a random action (exploration)\n",
    "        action = random.choice(possible_actions)\n",
    "        \n",
    "    else:\n",
    "        # Get action from Q-network (exploitation)\n",
    "        # Estimate the Qs values state\n",
    "        Qs = sess.run(DQNetwork.output, feed_dict = {DQNetwork.inputs_: state.reshape((1, *state.shape))})\n",
    "        \n",
    "        # Take the biggest Q value (= the best action)\n",
    "        choice = np.argmax(Qs)\n",
    "        action = possible_actions[int(choice)]\n",
    "                \n",
    "    return action, explore_probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function helps us to copy one set of variables to another\n",
    "# In our case we use it when we want to copy the parameters of DQN to Target_network\n",
    "# Thanks of the very good implementation of Arthur Juliani https://github.com/awjuliani\n",
    "def update_target_graph():\n",
    "    \n",
    "    # Get the parameters of our DQNNetwork\n",
    "    from_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"DQNetwork\")\n",
    "    \n",
    "    # Get the parameters of our Target_network\n",
    "    to_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"TargetNetwork\")\n",
    "\n",
    "    op_holder = []\n",
    "    \n",
    "    # Update our target_network parameters with DQNNetwork parameters\n",
    "    for from_var,to_var in zip(from_vars,to_vars):\n",
    "        op_holder.append(to_var.assign(from_var))\n",
    "    return op_holder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode Finish  [ 0. 14. 24.]\n",
      "Episode: 0 Total reward: -2.5 Training loss: 0.0175 Epsilon: 0.9967 Life: 65\n",
      "Model Saved at 0\n",
      "Stats Saved at 0\n",
      "Episode Finish  [ 3.  6. 24.]\n",
      "Episode: 1 Total reward: 2.3 Training loss: 0.0045 Epsilon: 0.9917 Life: 102\n",
      "Episode Finish  [ 1. 12. 24.]\n",
      "Episode: 2 Total reward: -0.6999999999999998 Training loss: 0.0150 Epsilon: 0.9877 Life: 102\n",
      "Episode Finish  [ 0. 15. 24.]\n",
      "Episode: 3 Total reward: -2.4 Training loss: 0.0021 Epsilon: 0.9845 Life: 102\n",
      "Episode Finish  [ 1. 10. 36.]\n",
      "Episode: 4 Total reward: -1.0 Training loss: 0.0084 Epsilon: 0.9806 Life: 102\n",
      "Episode Finish  [ 1. 16. 24.]\n",
      "Episode: 5 Total reward: -0.30000000000000004 Training loss: 0.0052 Epsilon: 0.9774 Life: 102\n",
      "Episode Finish  [ 1. 16. 24.]\n",
      "Episode: 6 Total reward: -0.30000000000000004 Training loss: 0.0034 Epsilon: 0.9743 Life: 102\n",
      "Episode Finish  [ 0. 15. 24.]\n",
      "Episode: 7 Total reward: -2.4 Training loss: 0.0041 Epsilon: 0.9711 Life: 102\n",
      "Episode Finish  [ 2.  8. 10.]\n",
      "Episode: 8 Total reward: 0.7 Training loss: 0.0041 Epsilon: 0.9662 Life: 102\n",
      "Episode Finish  [ 0. 13. 24.]\n",
      "Episode: 9 Total reward: -2.6 Training loss: 0.0034 Epsilon: 0.9631 Life: 102\n",
      "Episode Finish  [ 1. 11.  2.]\n",
      "Episode: 10 Total reward: -1.0000000000000004 Training loss: 0.0006 Epsilon: 0.9592 Life: 102\n",
      "Episode Finish  [ 1. 13. 48.]\n",
      "Episode: 11 Total reward: -0.5 Training loss: 0.0011 Epsilon: 0.9562 Life: 102\n",
      "Episode Finish  [ 1. 10. 24.]\n",
      "Episode: 12 Total reward: -0.9 Training loss: 0.0064 Epsilon: 0.9524 Life: 102\n",
      "Episode Finish  [ 1. 13. 24.]\n",
      "Episode: 13 Total reward: -0.6000000000000001 Training loss: 0.0022 Epsilon: 0.9493 Life: 102\n",
      "Episode Finish  [ 0. 12.  8.]\n",
      "Episode: 14 Total reward: -2.8000000000000003 Training loss: 0.0029 Epsilon: 0.9456 Life: 102\n",
      "Episode Finish  [ 0. 17. 24.]\n",
      "Episode: 15 Total reward: -2.2 Training loss: 0.0035 Epsilon: 0.9425 Life: 102\n",
      "Episode Finish  [ 0. 18.  4.]\n",
      "Episode: 16 Total reward: -2.1 Training loss: 0.0095 Epsilon: 0.9392 Life: 102\n",
      "Episode Finish  [ 0. 15. 24.]\n",
      "Episode: 17 Total reward: -2.4000000000000004 Training loss: 0.0031 Epsilon: 0.9362 Life: 102\n",
      "Episode Finish  [ 0. 16. 24.]\n",
      "Episode: 18 Total reward: -2.3 Training loss: 0.0040 Epsilon: 0.9331 Life: 102\n",
      "Episode Finish  [2. 9. 8.]\n",
      "Episode: 19 Total reward: 0.6000000000000002 Training loss: 0.0024 Epsilon: 0.9283 Life: 103\n",
      "Episode Finish  [ 1. 10. 12.]\n",
      "Episode: 20 Total reward: -1.0 Training loss: 0.0014 Epsilon: 0.9243 Life: 103\n",
      "Episode Finish  [ 3. 11. 16.]\n",
      "Episode: 21 Total reward: 3.1999999999999993 Training loss: 0.0029 Epsilon: 0.9203 Life: 103\n",
      "Episode Finish  [ 1. 12. 24.]\n",
      "Episode: 22 Total reward: -0.7 Training loss: 0.0019 Epsilon: 0.9173 Life: 103\n",
      "Episode Finish  [ 0. 16. 40.]\n",
      "Episode: 23 Total reward: -2.3 Training loss: 0.0036 Epsilon: 0.9141 Life: 103\n",
      "Episode Finish  [ 1. 13. 16.]\n",
      "Episode: 24 Total reward: -0.6000000000000002 Training loss: 0.0040 Epsilon: 0.9109 Life: 103\n",
      "Episode Finish  [ 0. 11. 44.]\n",
      "Episode: 25 Total reward: -2.7 Training loss: 0.0034 Epsilon: 0.9078 Life: 103\n",
      "Episode Finish  [ 2. 10. 24.]\n",
      "Episode: 26 Total reward: 1.1000000000000005 Training loss: 0.0023 Epsilon: 0.9042 Life: 103\n",
      "Episode Finish  [ 0. 16. 24.]\n",
      "Episode: 27 Total reward: -2.3000000000000003 Training loss: 0.0021 Epsilon: 0.9012 Life: 103\n",
      "Episode Finish  [ 1. 11. 24.]\n",
      "Episode: 28 Total reward: -0.8 Training loss: 0.0040 Epsilon: 0.8982 Life: 103\n",
      "Episode Finish  [ 0. 16. 24.]\n",
      "Episode: 29 Total reward: -2.3 Training loss: 0.0030 Epsilon: 0.8952 Life: 103\n",
      "Episode Finish  [ 2.  8. 20.]\n",
      "Episode: 30 Total reward: 0.8999999999999999 Training loss: 0.0082 Epsilon: 0.8911 Life: 103\n",
      "Episode Finish  [ 1. 13. 24.]\n",
      "Episode: 31 Total reward: -0.6 Training loss: 0.0104 Epsilon: 0.8875 Life: 103\n",
      "Episode Finish  [ 1. 15. 24.]\n",
      "Episode: 32 Total reward: -0.40000000000000013 Training loss: 0.0089 Epsilon: 0.8846 Life: 103\n",
      "Episode Finish  [ 1. 16. 24.]\n",
      "Episode: 33 Total reward: -0.2999999999999998 Training loss: 0.0051 Epsilon: 0.8811 Life: 103\n",
      "Episode Finish  [ 2. 11. 24.]\n",
      "Episode: 34 Total reward: 1.1999999999999997 Training loss: 0.0084 Epsilon: 0.8776 Life: 103\n",
      "Episode Finish  [ 0. 13. 24.]\n",
      "Episode: 35 Total reward: -2.5 Training loss: 0.0056 Epsilon: 0.8748 Life: 103\n",
      "Episode Finish  [4. 4. 8.]\n",
      "Episode: 36 Total reward: 4.4 Training loss: 0.0098 Epsilon: 0.8700 Life: 110\n",
      "Episode Finish  [ 1. 10. 12.]\n",
      "Episode: 37 Total reward: -1.1000000000000003 Training loss: 0.0058 Epsilon: 0.8665 Life: 110\n",
      "Episode Finish  [ 2. 12. 24.]\n",
      "Episode: 38 Total reward: 1.2999999999999998 Training loss: 0.0110 Epsilon: 0.8631 Life: 110\n",
      "Episode Finish  [ 2.  6. 28.]\n",
      "Episode: 39 Total reward: 0.6000000000000001 Training loss: 0.0088 Epsilon: 0.8587 Life: 110\n",
      "Episode Finish  [ 1. 16.  4.]\n",
      "Episode: 40 Total reward: -0.5 Training loss: 0.0028 Epsilon: 0.8554 Life: 110\n",
      "Episode Finish  [ 1.  9. 16.]\n",
      "Episode: 41 Total reward: -0.9999999999999999 Training loss: 0.0057 Epsilon: 0.8519 Life: 110\n",
      "Episode Finish  [ 2. 14. 24.]\n",
      "Episode: 42 Total reward: 1.5 Training loss: 0.0114 Epsilon: 0.8490 Life: 110\n",
      "Episode Finish  [ 0. 16. 16.]\n",
      "Episode: 43 Total reward: -2.3000000000000003 Training loss: 0.0132 Epsilon: 0.8461 Life: 110\n",
      "Episode Finish  [ 0. 13. 24.]\n",
      "Episode: 44 Total reward: -2.6 Training loss: 0.0046 Epsilon: 0.8434 Life: 110\n",
      "Episode Finish  [ 2. 10. 10.]\n",
      "Episode: 45 Total reward: 0.9999999999999996 Training loss: 0.0065 Epsilon: 0.8399 Life: 110\n",
      "Episode Finish  [ 0. 13. 24.]\n",
      "Episode: 46 Total reward: -2.6 Training loss: 0.0065 Epsilon: 0.8372 Life: 110\n",
      "Episode Finish  [ 0. 11.  4.]\n",
      "Episode: 47 Total reward: -2.8000000000000003 Training loss: 0.0136 Epsilon: 0.8341 Life: 110\n",
      "Episode Finish  [ 4.  6. 10.]\n",
      "Episode: 48 Total reward: 4.6000000000000005 Training loss: 0.0091 Epsilon: 0.8302 Life: 110\n",
      "Episode Finish  [ 1.  8. 24.]\n",
      "Episode: 49 Total reward: -1.1 Training loss: 0.0135 Epsilon: 0.8269 Life: 110\n",
      "Episode Finish  [ 3. 13. 12.]\n",
      "Episode: 50 Total reward: 3.3000000000000007 Training loss: 0.0048 Epsilon: 0.8233 Life: 110\n",
      "Stats Saved at 50\n",
      "Episode Finish  [ 0. 14. 24.]\n",
      "Episode: 51 Total reward: -2.5 Training loss: 0.0223 Epsilon: 0.8206 Life: 110\n",
      "Episode Finish  [2. 7. 4.]\n",
      "Episode: 52 Total reward: 0.7 Training loss: 0.0151 Epsilon: 0.8170 Life: 110\n",
      "Episode Finish  [ 3.  7. 12.]\n",
      "Episode: 53 Total reward: 2.7 Training loss: 0.0088 Epsilon: 0.8128 Life: 110\n",
      "Episode Finish  [4. 8. 4.]\n",
      "Episode: 54 Total reward: 4.6000000000000005 Training loss: 0.0060 Epsilon: 0.8087 Life: 110\n",
      "Episode Finish  [2. 9. 4.]\n",
      "Episode: 55 Total reward: 0.7999999999999998 Training loss: 0.0088 Epsilon: 0.8047 Life: 110\n",
      "Episode Finish  [ 0. 17. 24.]\n",
      "Episode: 56 Total reward: -2.2 Training loss: 0.0168 Epsilon: 0.8020 Life: 110\n",
      "Episode Finish  [ 3.  9. 12.]\n",
      "Episode: 57 Total reward: 2.8000000000000003 Training loss: 0.0087 Epsilon: 0.7984 Life: 110\n",
      "Episode Finish  [ 1. 12.  8.]\n",
      "Episode: 58 Total reward: -0.7000000000000001 Training loss: 0.0047 Epsilon: 0.7953 Life: 110\n",
      "Episode Finish  [ 1. 10. 24.]\n",
      "Episode: 59 Total reward: -0.9000000000000001 Training loss: 0.0168 Epsilon: 0.7922 Life: 110\n",
      "Episode Finish  [ 1. 14.  8.]\n",
      "Episode: 60 Total reward: -0.5000000000000001 Training loss: 0.0111 Epsilon: 0.7894 Life: 110\n",
      "Episode Finish  [ 0. 13.  8.]\n",
      "Episode: 61 Total reward: -2.7 Training loss: 0.0076 Epsilon: 0.7867 Life: 110\n",
      "Episode Finish  [ 2. 11.  8.]\n",
      "Episode: 62 Total reward: 1.0 Training loss: 0.0141 Epsilon: 0.7836 Life: 110\n",
      "Episode Finish  [ 2.  9. 16.]\n",
      "Episode: 63 Total reward: 0.6000000000000001 Training loss: 0.0151 Epsilon: 0.7797 Life: 110\n",
      "Episode Finish  [ 1. 14. 24.]\n",
      "Episode: 64 Total reward: -0.5000000000000002 Training loss: 0.0094 Epsilon: 0.7771 Life: 110\n",
      "Episode Finish  [ 1. 14. 24.]\n",
      "Episode: 65 Total reward: -0.5000000000000001 Training loss: 0.0077 Epsilon: 0.7745 Life: 110\n",
      "Episode Finish  [ 1. 13. 24.]\n",
      "Episode: 66 Total reward: -0.6 Training loss: 0.0079 Epsilon: 0.7720 Life: 110\n",
      "Episode Finish  [ 2.  9. 20.]\n",
      "Episode: 67 Total reward: 1.0 Training loss: 0.0174 Epsilon: 0.7689 Life: 110\n",
      "Episode Finish  [ 0. 14. 24.]\n",
      "Episode: 68 Total reward: -2.5 Training loss: 0.0162 Epsilon: 0.7664 Life: 110\n",
      "Episode Finish  [ 1. 13. 24.]\n",
      "Episode: 69 Total reward: -0.5999999999999998 Training loss: 0.0196 Epsilon: 0.7633 Life: 110\n",
      "Episode Finish  [ 2. 11. 12.]\n",
      "Episode: 70 Total reward: 1.0999999999999996 Training loss: 0.0084 Epsilon: 0.7604 Life: 110\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode Finish  [2. 8. 4.]\n",
      "Episode: 71 Total reward: 0.6000000000000001 Training loss: 0.0042 Epsilon: 0.7566 Life: 110\n",
      "Episode Finish  [2. 8. 6.]\n",
      "Episode: 72 Total reward: 0.7000000000000002 Training loss: 0.0179 Epsilon: 0.7527 Life: 110\n",
      "Episode Finish  [ 1.  8. 24.]\n",
      "Episode: 73 Total reward: -1.0999999999999999 Training loss: 0.0095 Epsilon: 0.7497 Life: 110\n",
      "Episode Finish  [ 2. 10. 24.]\n",
      "Episode: 74 Total reward: 1.1000000000000005 Training loss: 0.0133 Epsilon: 0.7465 Life: 110\n",
      "Episode Finish  [ 0. 10.  8.]\n",
      "Episode: 75 Total reward: -3.1 Training loss: 0.0135 Epsilon: 0.7434 Life: 110\n",
      "Episode Finish  [ 2. 14. 20.]\n",
      "Episode: 76 Total reward: 1.4999999999999996 Training loss: 0.0149 Epsilon: 0.7406 Life: 110\n",
      "Episode Finish  [ 4.  6. 10.]\n",
      "Episode: 77 Total reward: 4.5 Training loss: 0.0219 Epsilon: 0.7361 Life: 123\n",
      "Episode Finish  [ 3.  3. 10.]\n",
      "Episode: 78 Total reward: 2.3000000000000003 Training loss: 0.0220 Epsilon: 0.7325 Life: 123\n",
      "Episode Finish  [ 0. 14. 24.]\n",
      "Episode: 79 Total reward: -2.5 Training loss: 0.0213 Epsilon: 0.7301 Life: 123\n",
      "Episode Finish  [ 2. 11. 12.]\n",
      "Episode: 80 Total reward: 1.2000000000000002 Training loss: 0.0234 Epsilon: 0.7270 Life: 123\n",
      "Episode Finish  [ 0. 12. 24.]\n",
      "Episode: 81 Total reward: -2.7 Training loss: 0.0203 Epsilon: 0.7247 Life: 123\n",
      "Episode Finish  [ 4. 11. 10.]\n",
      "Episode: 82 Total reward: 5.1 Training loss: 0.0156 Epsilon: 0.7212 Life: 123\n",
      "Episode Finish  [ 1. 13. 24.]\n",
      "Episode: 83 Total reward: -0.6000000000000001 Training loss: 0.0274 Epsilon: 0.7188 Life: 123\n",
      "Episode Finish  [ 4.  9. 16.]\n",
      "Episode: 84 Total reward: 4.900000000000001 Training loss: 0.0255 Epsilon: 0.7152 Life: 123\n",
      "Episode Finish  [ 2. 10.  4.]\n",
      "Episode: 85 Total reward: 0.8999999999999995 Training loss: 0.0181 Epsilon: 0.7117 Life: 123\n",
      "Episode Finish  [ 1. 12. 24.]\n",
      "Episode: 86 Total reward: -0.7 Training loss: 0.0212 Epsilon: 0.7088 Life: 123\n",
      "Episode Finish  [ 1. 11. 24.]\n",
      "Episode: 87 Total reward: -0.8000000000000002 Training loss: 0.0304 Epsilon: 0.7064 Life: 123\n",
      "Episode Finish  [2. 8. 4.]\n",
      "Episode: 88 Total reward: 0.5999999999999996 Training loss: 0.0218 Epsilon: 0.7027 Life: 123\n",
      "Episode Finish  [ 2. 12.  4.]\n",
      "Episode: 89 Total reward: 2.3 Training loss: 0.0185 Epsilon: 0.6998 Life: 123\n",
      "Episode Finish  [ 1. 12. 24.]\n",
      "Episode: 90 Total reward: -0.7 Training loss: 0.0253 Epsilon: 0.6973 Life: 123\n",
      "Episode Finish  [ 1. 14.  8.]\n",
      "Episode: 91 Total reward: -0.5000000000000001 Training loss: 0.0214 Epsilon: 0.6951 Life: 123\n",
      "Episode Finish  [ 1. 13. 24.]\n",
      "Episode: 92 Total reward: -0.6 Training loss: 0.0158 Epsilon: 0.6929 Life: 123\n",
      "Episode Finish  [ 4. 12. 24.]\n",
      "Episode: 93 Total reward: 5.3 Training loss: 0.0207 Epsilon: 0.6901 Life: 123\n",
      "Episode Finish  [ 0. 15.  8.]\n",
      "Episode: 94 Total reward: -2.5999999999999996 Training loss: 0.0254 Epsilon: 0.6877 Life: 123\n",
      "Episode Finish  [4. 3. 2.]\n",
      "Episode: 95 Total reward: 3.999999999999999 Training loss: 0.0142 Epsilon: 0.6838 Life: 123\n",
      "Episode Finish  [ 1. 13. 24.]\n",
      "Episode: 96 Total reward: -0.5999999999999999 Training loss: 0.0176 Epsilon: 0.6811 Life: 123\n",
      "Episode Finish  [ 2. 12. 12.]\n",
      "Episode: 97 Total reward: 1.2999999999999998 Training loss: 0.0216 Epsilon: 0.6786 Life: 123\n",
      "Episode Finish  [ 0. 10.  4.]\n",
      "Episode: 98 Total reward: -2.9000000000000004 Training loss: 0.0180 Epsilon: 0.6761 Life: 123\n",
      "Episode Finish  [ 1. 14.  4.]\n",
      "Episode: 99 Total reward: -0.5 Training loss: 0.0407 Epsilon: 0.6736 Life: 123\n",
      "Episode Finish  [ 2.  7. 16.]\n",
      "Episode: 100 Total reward: 0.5999999999999998 Training loss: 0.0272 Epsilon: 0.6707 Life: 123\n",
      "Stats Saved at 100\n",
      "Episode Finish  [ 1. 16. 24.]\n",
      "Episode: 101 Total reward: -0.2999999999999997 Training loss: 0.0181 Epsilon: 0.6685 Life: 123\n",
      "Episode Finish  [ 1. 12. 24.]\n",
      "Episode: 102 Total reward: -0.7 Training loss: 0.0198 Epsilon: 0.6664 Life: 123\n",
      "Episode Finish  [ 2. 14. 24.]\n",
      "Episode: 103 Total reward: 1.4 Training loss: 0.0277 Epsilon: 0.6638 Life: 123\n",
      "Episode Finish  [ 2. 15. 24.]\n",
      "Episode: 104 Total reward: 1.6 Training loss: 0.0192 Epsilon: 0.6616 Life: 123\n",
      "Episode Finish  [ 1. 16. 20.]\n",
      "Episode: 105 Total reward: -0.2999999999999997 Training loss: 0.0151 Epsilon: 0.6600 Life: 123\n",
      "Episode Finish  [ 0. 10. 24.]\n",
      "Episode: 106 Total reward: -2.9 Training loss: 0.0266 Epsilon: 0.6578 Life: 123\n",
      "Episode Finish  [ 1. 12. 10.]\n",
      "Episode: 107 Total reward: -0.8 Training loss: 0.0298 Epsilon: 0.6552 Life: 123\n",
      "Episode Finish  [ 0. 16. 24.]\n",
      "Episode: 108 Total reward: -2.3 Training loss: 0.0181 Epsilon: 0.6530 Life: 123\n",
      "Episode Finish  [ 1. 13. 40.]\n",
      "Episode: 109 Total reward: -0.7000000000000002 Training loss: 0.0246 Epsilon: 0.6509 Life: 123\n",
      "Episode Finish  [ 1. 15. 12.]\n",
      "Episode: 110 Total reward: -0.4 Training loss: 0.0151 Epsilon: 0.6486 Life: 123\n",
      "Episode Finish  [3. 9. 8.]\n",
      "Episode: 111 Total reward: 2.6999999999999997 Training loss: 0.0364 Epsilon: 0.6452 Life: 123\n",
      "Episode Finish  [ 1. 12. 24.]\n",
      "Episode: 112 Total reward: -0.7000000000000001 Training loss: 0.0283 Epsilon: 0.6428 Life: 123\n",
      "Episode Finish  [ 0. 15.  4.]\n",
      "Episode: 113 Total reward: -2.5 Training loss: 0.0243 Epsilon: 0.6407 Life: 123\n",
      "Episode Finish  [5. 5. 4.]\n",
      "Episode: 114 Total reward: 6.3 Training loss: 0.0161 Epsilon: 0.6368 Life: 123\n",
      "Episode Finish  [ 1. 14. 28.]\n",
      "Episode: 115 Total reward: -0.6000000000000001 Training loss: 0.0141 Epsilon: 0.6348 Life: 123\n",
      "Episode Finish  [3. 6. 4.]\n",
      "Episode: 116 Total reward: 2.4 Training loss: 0.0252 Epsilon: 0.6314 Life: 123\n",
      "Episode Finish  [ 1.  8. 24.]\n",
      "Episode: 117 Total reward: -1.1000000000000003 Training loss: 0.0228 Epsilon: 0.6289 Life: 123\n",
      "Episode Finish  [ 3.  8. 24.]\n",
      "Episode: 118 Total reward: 2.9 Training loss: 0.0261 Epsilon: 0.6263 Life: 123\n",
      "Episode Finish  [ 1. 14. 32.]\n",
      "Episode: 119 Total reward: -0.4 Training loss: 0.0341 Epsilon: 0.6244 Life: 123\n",
      "Episode Finish  [3. 9. 4.]\n",
      "Episode: 120 Total reward: 2.6999999999999997 Training loss: 0.0151 Epsilon: 0.6209 Life: 123\n",
      "Episode Finish  [ 2. 10. 20.]\n",
      "Episode: 121 Total reward: 1.0 Training loss: 0.0208 Epsilon: 0.6184 Life: 123\n",
      "Episode Finish  [ 1. 11. 24.]\n",
      "Episode: 122 Total reward: -0.8999999999999999 Training loss: 0.0246 Epsilon: 0.6159 Life: 123\n",
      "Episode Finish  [4. 7. 4.]\n",
      "Episode: 123 Total reward: 4.4 Training loss: 0.0247 Epsilon: 0.6125 Life: 123\n",
      "Episode Finish  [ 1. 12. 24.]\n",
      "Episode: 124 Total reward: -0.7000000000000002 Training loss: 0.0253 Epsilon: 0.6106 Life: 123\n",
      "Model updated\n",
      "Episode Finish  [ 0. 11. 20.]\n",
      "Episode: 125 Total reward: -2.9000000000000004 Training loss: 0.0412 Epsilon: 0.6083 Life: 123\n",
      "Episode Finish  [ 1. 13. 24.]\n",
      "Episode: 126 Total reward: -0.5999999999999999 Training loss: 0.0203 Epsilon: 0.6066 Life: 123\n",
      "Episode Finish  [ 1. 13. 32.]\n",
      "Episode: 127 Total reward: -0.5 Training loss: 0.0178 Epsilon: 0.6048 Life: 123\n",
      "Episode Finish  [ 2.  9. 24.]\n",
      "Episode: 128 Total reward: 1.0 Training loss: 0.0213 Epsilon: 0.6024 Life: 123\n",
      "Episode Finish  [ 2. 12. 24.]\n",
      "Episode: 129 Total reward: 1.3000000000000003 Training loss: 0.0172 Epsilon: 0.6000 Life: 123\n",
      "Episode Finish  [ 2. 11. 12.]\n",
      "Episode: 130 Total reward: 1.2000000000000002 Training loss: 0.0314 Epsilon: 0.5975 Life: 123\n",
      "Episode Finish  [ 3.  7. 24.]\n",
      "Episode: 131 Total reward: 2.8000000000000003 Training loss: 0.0361 Epsilon: 0.5951 Life: 123\n",
      "Episode Finish  [ 1. 14. 16.]\n",
      "Episode: 132 Total reward: -0.5 Training loss: 0.0230 Epsilon: 0.5930 Life: 123\n",
      "Episode Finish  [ 3.  6. 10.]\n",
      "Episode: 133 Total reward: 2.5999999999999996 Training loss: 0.0161 Epsilon: 0.5901 Life: 123\n",
      "Episode Finish  [ 2. 10. 12.]\n",
      "Episode: 134 Total reward: 1.1 Training loss: 0.0267 Epsilon: 0.5877 Life: 123\n",
      "Episode Finish  [ 0.  9. 12.]\n",
      "Episode: 135 Total reward: -3.1 Training loss: 0.0172 Epsilon: 0.5852 Life: 123\n",
      "Episode Finish  [ 1. 12. 18.]\n",
      "Episode: 136 Total reward: -0.7999999999999999 Training loss: 0.0382 Epsilon: 0.5829 Life: 123\n",
      "Episode Finish  [ 1. 12. 20.]\n",
      "Episode: 137 Total reward: -0.8000000000000003 Training loss: 0.0261 Epsilon: 0.5810 Life: 123\n",
      "Episode Finish  [ 1. 18. 24.]\n",
      "Episode: 138 Total reward: -0.09999999999999998 Training loss: 0.0228 Epsilon: 0.5793 Life: 123\n",
      "Episode Finish  [ 2.  7. 16.]\n",
      "Episode: 139 Total reward: 0.5 Training loss: 0.0278 Epsilon: 0.5768 Life: 123\n",
      "Episode Finish  [ 1. 12. 20.]\n",
      "Episode: 140 Total reward: -0.7999999999999997 Training loss: 0.0303 Epsilon: 0.5745 Life: 123\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode Finish  [ 1. 10.  8.]\n",
      "Episode: 141 Total reward: -1.0999999999999996 Training loss: 0.0333 Epsilon: 0.5720 Life: 123\n",
      "Episode Finish  [2. 6. 4.]\n",
      "Episode: 142 Total reward: 0.49999999999999956 Training loss: 0.0239 Epsilon: 0.5693 Life: 123\n",
      "Episode Finish  [ 1. 12. 28.]\n",
      "Episode: 143 Total reward: -0.9 Training loss: 0.0228 Epsilon: 0.5677 Life: 123\n",
      "Episode Finish  [ 1. 11. 24.]\n",
      "Episode: 144 Total reward: -0.8 Training loss: 0.0204 Epsilon: 0.5654 Life: 123\n",
      "Episode Finish  [4. 9. 8.]\n",
      "Episode: 145 Total reward: 4.5 Training loss: 0.0309 Epsilon: 0.5623 Life: 123\n",
      "Episode Finish  [ 1. 15. 24.]\n",
      "Episode: 146 Total reward: -0.3999999999999999 Training loss: 0.0261 Epsilon: 0.5608 Life: 123\n",
      "Episode Finish  [ 2. 11. 24.]\n",
      "Episode: 147 Total reward: 1.2000000000000002 Training loss: 0.0252 Epsilon: 0.5588 Life: 123\n",
      "Episode Finish  [2. 3. 8.]\n",
      "Episode: 148 Total reward: 0.09999999999999987 Training loss: 0.0351 Epsilon: 0.5561 Life: 123\n",
      "Episode Finish  [ 0. 14.  8.]\n",
      "Episode: 149 Total reward: -2.5999999999999996 Training loss: 0.0253 Epsilon: 0.5544 Life: 123\n",
      "Episode Finish  [ 3.  8. 24.]\n",
      "Episode: 150 Total reward: 2.9 Training loss: 0.0171 Epsilon: 0.5520 Life: 123\n",
      "Stats Saved at 150\n",
      "Episode Finish  [ 2.  3. 28.]\n",
      "Episode: 151 Total reward: 0.10000000000000009 Training loss: 0.0229 Epsilon: 0.5494 Life: 123\n",
      "Episode Finish  [ 1.  8. 32.]\n",
      "Episode: 152 Total reward: -1.0999999999999999 Training loss: 0.0206 Epsilon: 0.5473 Life: 123\n",
      "Episode Finish  [ 1. 13. 24.]\n",
      "Episode: 153 Total reward: -0.6 Training loss: 0.0200 Epsilon: 0.5455 Life: 123\n",
      "Episode Finish  [ 2.  9. 24.]\n",
      "Episode: 154 Total reward: 1.0 Training loss: 0.0365 Epsilon: 0.5433 Life: 123\n",
      "Episode Finish  [ 0. 15. 24.]\n",
      "Episode: 155 Total reward: -2.4 Training loss: 0.0285 Epsilon: 0.5416 Life: 123\n",
      "Episode Finish  [ 1. 12. 16.]\n",
      "Episode: 156 Total reward: -0.7999999999999998 Training loss: 0.0203 Epsilon: 0.5396 Life: 123\n",
      "Episode Finish  [ 0. 14. 24.]\n",
      "Episode: 157 Total reward: -2.5 Training loss: 0.0403 Epsilon: 0.5379 Life: 123\n",
      "Episode Finish  [ 1. 15. 24.]\n",
      "Episode: 158 Total reward: -0.40000000000000013 Training loss: 0.0261 Epsilon: 0.5364 Life: 123\n",
      "Episode Finish  [ 1. 11. 24.]\n",
      "Episode: 159 Total reward: -0.7999999999999999 Training loss: 0.0312 Epsilon: 0.5347 Life: 123\n",
      "Episode Finish  [ 1. 11. 24.]\n",
      "Episode: 160 Total reward: -0.8000000000000002 Training loss: 0.0305 Epsilon: 0.5329 Life: 123\n",
      "Episode Finish  [3. 4. 4.]\n",
      "Episode: 161 Total reward: 2.1999999999999997 Training loss: 0.0192 Epsilon: 0.5301 Life: 123\n",
      "Episode Finish  [5. 1. 4.]\n",
      "Episode: 162 Total reward: 5.800000000000001 Training loss: 0.0260 Epsilon: 0.5269 Life: 123\n",
      "Episode Finish  [ 1. 15. 24.]\n",
      "Episode: 163 Total reward: -0.2999999999999998 Training loss: 0.0163 Epsilon: 0.5254 Life: 123\n",
      "Episode Finish  [ 2.  9. 28.]\n",
      "Episode: 164 Total reward: 0.7999999999999998 Training loss: 0.0395 Epsilon: 0.5231 Life: 123\n",
      "Episode Finish  [2. 3. 8.]\n",
      "Episode: 165 Total reward: 0.10000000000000009 Training loss: 0.0253 Epsilon: 0.5203 Life: 123\n",
      "Episode Finish  [4. 5. 8.]\n",
      "Episode: 166 Total reward: 4.300000000000002 Training loss: 0.0224 Epsilon: 0.5175 Life: 123\n",
      "Episode Finish  [ 1. 15. 16.]\n",
      "Episode: 167 Total reward: -0.3999999999999999 Training loss: 0.0246 Epsilon: 0.5161 Life: 123\n",
      "Episode Finish  [ 0. 14. 24.]\n",
      "Episode: 168 Total reward: -2.5 Training loss: 0.0330 Epsilon: 0.5145 Life: 123\n",
      "Episode Finish  [ 2. 11. 24.]\n",
      "Episode: 169 Total reward: 1.3000000000000003 Training loss: 0.0325 Epsilon: 0.5128 Life: 123\n",
      "Episode Finish  [ 1. 13. 32.]\n",
      "Episode: 170 Total reward: -0.6 Training loss: 0.0221 Epsilon: 0.5112 Life: 123\n",
      "Episode Finish  [ 2. 14. 16.]\n",
      "Episode: 171 Total reward: 1.5 Training loss: 0.0246 Epsilon: 0.5094 Life: 123\n",
      "Episode Finish  [ 1. 16. 24.]\n",
      "Episode: 172 Total reward: -0.30000000000000004 Training loss: 0.0281 Epsilon: 0.5078 Life: 123\n",
      "Episode Finish  [ 2. 13. 24.]\n",
      "Episode: 173 Total reward: 1.4 Training loss: 0.0174 Epsilon: 0.5057 Life: 123\n",
      "Episode Finish  [ 2. 12. 24.]\n",
      "Episode: 174 Total reward: 1.3000000000000003 Training loss: 0.0293 Epsilon: 0.5036 Life: 123\n",
      "Episode Finish  [ 2. 15. 12.]\n",
      "Episode: 175 Total reward: 1.6 Training loss: 0.0354 Epsilon: 0.5018 Life: 123\n",
      "Episode Finish  [ 0. 15. 24.]\n",
      "Episode: 176 Total reward: -2.4000000000000004 Training loss: 0.0229 Epsilon: 0.5002 Life: 123\n",
      "Episode Finish  [ 1. 15. 12.]\n",
      "Episode: 177 Total reward: -0.5 Training loss: 0.0257 Epsilon: 0.4986 Life: 123\n",
      "Episode Finish  [ 2. 11. 16.]\n",
      "Episode: 178 Total reward: 1.1 Training loss: 0.0271 Epsilon: 0.4966 Life: 123\n",
      "Episode Finish  [ 2. 10. 16.]\n",
      "Episode: 179 Total reward: 1.1 Training loss: 0.0359 Epsilon: 0.4948 Life: 123\n",
      "Episode Finish  [ 1. 13. 24.]\n",
      "Episode: 180 Total reward: -0.5999999999999999 Training loss: 0.0184 Epsilon: 0.4932 Life: 123\n",
      "Episode Finish  [ 2. 12.  4.]\n",
      "Episode: 181 Total reward: 1.2999999999999998 Training loss: 0.0270 Epsilon: 0.4914 Life: 123\n",
      "Episode Finish  [ 2. 12. 24.]\n",
      "Episode: 182 Total reward: 1.3000000000000003 Training loss: 0.0192 Epsilon: 0.4893 Life: 123\n",
      "Episode Finish  [ 2.  9. 24.]\n",
      "Episode: 183 Total reward: 0.8999999999999999 Training loss: 0.0322 Epsilon: 0.4874 Life: 123\n",
      "Episode Finish  [ 1. 11. 24.]\n",
      "Episode: 184 Total reward: -0.8000000000000002 Training loss: 0.0242 Epsilon: 0.4854 Life: 123\n",
      "Episode Finish  [ 3.  9. 12.]\n",
      "Episode: 185 Total reward: 2.9 Training loss: 0.0272 Epsilon: 0.4831 Life: 123\n",
      "Episode Finish  [3. 2. 6.]\n",
      "Episode: 186 Total reward: 1.9 Training loss: 0.0221 Epsilon: 0.4806 Life: 123\n",
      "Episode Finish  [ 2. 12. 28.]\n",
      "Episode: 187 Total reward: 1.2999999999999998 Training loss: 0.0194 Epsilon: 0.4786 Life: 123\n",
      "Episode Finish  [ 2. 11. 12.]\n",
      "Episode: 188 Total reward: 1.1999999999999997 Training loss: 0.0109 Epsilon: 0.4769 Life: 123\n",
      "Episode Finish  [ 2. 13. 24.]\n",
      "Episode: 189 Total reward: 1.4000000000000004 Training loss: 0.0193 Epsilon: 0.4752 Life: 123\n",
      "Episode Finish  [ 1. 13.  4.]\n",
      "Episode: 190 Total reward: -0.6000000000000001 Training loss: 0.0246 Epsilon: 0.4737 Life: 123\n",
      "Episode Finish  [ 1. 13. 24.]\n",
      "Episode: 191 Total reward: -0.6000000000000001 Training loss: 0.0275 Epsilon: 0.4722 Life: 123\n",
      "Episode Finish  [ 1. 10. 24.]\n",
      "Episode: 192 Total reward: -0.8999999999999999 Training loss: 0.0115 Epsilon: 0.4703 Life: 123\n",
      "Episode Finish  [ 1. 14. 24.]\n",
      "Episode: 193 Total reward: -0.4999999999999998 Training loss: 0.0209 Epsilon: 0.4687 Life: 123\n",
      "Episode Finish  [ 2. 13. 24.]\n",
      "Episode: 194 Total reward: 1.5000000000000004 Training loss: 0.0248 Epsilon: 0.4671 Life: 123\n",
      "Episode Finish  [ 4. 13.  4.]\n",
      "Episode: 195 Total reward: 5.500000000000001 Training loss: 0.0196 Epsilon: 0.4655 Life: 123\n",
      "Episode Finish  [ 0. 15. 20.]\n",
      "Episode: 196 Total reward: -2.4 Training loss: 0.0173 Epsilon: 0.4640 Life: 123\n",
      "Episode Finish  [ 2.  9. 16.]\n",
      "Episode: 197 Total reward: 0.7999999999999998 Training loss: 0.0265 Epsilon: 0.4621 Life: 123\n",
      "Episode Finish  [ 1. 14. 24.]\n",
      "Episode: 198 Total reward: -0.4999999999999998 Training loss: 0.0329 Epsilon: 0.4605 Life: 123\n",
      "Episode Finish  [ 3. 10. 24.]\n",
      "Episode: 199 Total reward: 3.100000000000001 Training loss: 0.0331 Epsilon: 0.4587 Life: 123\n",
      "Episode Finish  [ 3.  8. 32.]\n",
      "Episode: 200 Total reward: 2.6999999999999997 Training loss: 0.0200 Epsilon: 0.4566 Life: 123\n",
      "Model Saved at 200\n",
      "Stats Saved at 200\n",
      "Episode Finish  [ 1. 11. 24.]\n",
      "Episode: 201 Total reward: -0.7999999999999998 Training loss: 0.0336 Epsilon: 0.4550 Life: 123\n",
      "Episode Finish  [1. 9. 8.]\n",
      "Episode: 202 Total reward: -1.1 Training loss: 0.0318 Epsilon: 0.4531 Life: 123\n",
      "Episode Finish  [3. 6. 4.]\n",
      "Episode: 203 Total reward: 2.3999999999999995 Training loss: 0.0265 Epsilon: 0.4506 Life: 123\n",
      "Episode Finish  [3. 0. 4.]\n",
      "Episode: 204 Total reward: 1.7999999999999994 Training loss: 0.0266 Epsilon: 0.4481 Life: 123\n",
      "Episode Finish  [ 2.  9. 12.]\n",
      "Episode: 205 Total reward: 0.6999999999999993 Training loss: 0.0209 Epsilon: 0.4466 Life: 123\n",
      "Episode Finish  [ 2.  6. 20.]\n",
      "Episode: 206 Total reward: 0.5 Training loss: 0.0189 Epsilon: 0.4447 Life: 123\n",
      "Episode Finish  [ 1. 14. 24.]\n",
      "Episode: 207 Total reward: -0.4 Training loss: 0.0179 Epsilon: 0.4433 Life: 123\n",
      "Episode Finish  [4. 2. 8.]\n",
      "Episode: 208 Total reward: 4.000000000000002 Training loss: 0.0203 Epsilon: 0.4407 Life: 123\n",
      "Episode Finish  [ 1. 16. 20.]\n",
      "Episode: 209 Total reward: -0.4999999999999999 Training loss: 0.0315 Epsilon: 0.4393 Life: 123\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode Finish  [ 1. 14. 24.]\n",
      "Episode: 210 Total reward: -0.3999999999999997 Training loss: 0.0389 Epsilon: 0.4378 Life: 123\n",
      "Episode Finish  [ 2. 12. 24.]\n",
      "Episode: 211 Total reward: 1.3000000000000003 Training loss: 0.0371 Epsilon: 0.4363 Life: 123\n",
      "Episode Finish  [2. 8. 4.]\n",
      "Episode: 212 Total reward: 1.7999999999999998 Training loss: 0.0378 Epsilon: 0.4346 Life: 123\n",
      "Episode Finish  [ 1. 13.  4.]\n",
      "Episode: 213 Total reward: -0.6000000000000001 Training loss: 0.0306 Epsilon: 0.4334 Life: 123\n",
      "Episode Finish  [ 2. 10.  6.]\n",
      "Episode: 214 Total reward: 0.8999999999999999 Training loss: 0.0140 Epsilon: 0.4315 Life: 123\n",
      "Episode Finish  [ 4.  3. 28.]\n",
      "Episode: 215 Total reward: 4.200000000000001 Training loss: 0.0364 Epsilon: 0.4292 Life: 123\n",
      "Episode Finish  [ 1.  8. 24.]\n",
      "Episode: 216 Total reward: -1.0999999999999996 Training loss: 0.0170 Epsilon: 0.4275 Life: 123\n",
      "Episode Finish  [ 2. 11. 24.]\n",
      "Episode: 217 Total reward: 1.2000000000000002 Training loss: 0.0324 Epsilon: 0.4262 Life: 123\n",
      "Episode Finish  [ 1.  8. 10.]\n",
      "Episode: 218 Total reward: -1.2 Training loss: 0.0380 Epsilon: 0.4245 Life: 123\n",
      "Episode Finish  [ 3.  9. 24.]\n",
      "Episode: 219 Total reward: 3.0 Training loss: 0.0393 Epsilon: 0.4227 Life: 123\n",
      "Episode Finish  [ 2. 14. 28.]\n",
      "Episode: 220 Total reward: 1.4 Training loss: 0.0234 Epsilon: 0.4212 Life: 123\n",
      "Episode Finish  [ 3.  5. 10.]\n",
      "Episode: 221 Total reward: 2.5 Training loss: 0.0258 Epsilon: 0.4190 Life: 123\n",
      "Episode Finish  [ 4. 12.  8.]\n",
      "Episode: 222 Total reward: 5.200000000000001 Training loss: 0.0243 Epsilon: 0.4173 Life: 123\n",
      "Episode Finish  [ 0. 14. 24.]\n",
      "Episode: 223 Total reward: -2.5 Training loss: 0.0280 Epsilon: 0.4160 Life: 123\n",
      "Episode Finish  [ 2. 13. 24.]\n",
      "Episode: 224 Total reward: 1.3999999999999997 Training loss: 0.0264 Epsilon: 0.4143 Life: 123\n",
      "Episode Finish  [ 0. 14. 20.]\n",
      "Episode: 225 Total reward: -2.6 Training loss: 0.0244 Epsilon: 0.4130 Life: 123\n",
      "Episode Finish  [ 1. 12. 24.]\n",
      "Episode: 226 Total reward: -0.7 Training loss: 0.0264 Epsilon: 0.4117 Life: 123\n",
      "Episode Finish  [ 1. 14. 24.]\n",
      "Episode: 227 Total reward: -0.7000000000000002 Training loss: 0.0361 Epsilon: 0.4104 Life: 123\n",
      "Episode Finish  [ 2.  6. 10.]\n",
      "Episode: 228 Total reward: 0.5999999999999999 Training loss: 0.0146 Epsilon: 0.4082 Life: 123\n",
      "Episode Finish  [ 2. 14. 24.]\n",
      "Episode: 229 Total reward: 1.5 Training loss: 0.0213 Epsilon: 0.4069 Life: 123\n",
      "Episode Finish  [ 4.  1. 18.]\n",
      "Episode: 230 Total reward: 3.799999999999999 Training loss: 0.0191 Epsilon: 0.4046 Life: 123\n",
      "Episode Finish  [ 1. 11. 24.]\n",
      "Episode: 231 Total reward: -0.7999999999999999 Training loss: 0.0221 Epsilon: 0.4033 Life: 123\n",
      "Episode Finish  [ 2. 12. 16.]\n",
      "Episode: 232 Total reward: 1.0999999999999996 Training loss: 0.0276 Epsilon: 0.4018 Life: 123\n",
      "Episode Finish  [ 2.  6. 12.]\n",
      "Episode: 233 Total reward: 0.5999999999999996 Training loss: 0.0181 Epsilon: 0.3998 Life: 123\n",
      "Episode Finish  [2. 4. 8.]\n",
      "Episode: 234 Total reward: 0.30000000000000027 Training loss: 0.0255 Epsilon: 0.3977 Life: 123\n",
      "Episode Finish  [2. 6. 2.]\n",
      "Episode: 235 Total reward: 0.6000000000000001 Training loss: 0.0128 Epsilon: 0.3956 Life: 123\n",
      "Episode Finish  [ 2. 12. 24.]\n",
      "Episode: 236 Total reward: 1.2999999999999994 Training loss: 0.0173 Epsilon: 0.3944 Life: 123\n",
      "Episode Finish  [ 1. 14. 24.]\n",
      "Episode: 237 Total reward: -0.6000000000000001 Training loss: 0.0291 Epsilon: 0.3933 Life: 123\n",
      "Episode Finish  [ 2. 11. 24.]\n",
      "Episode: 238 Total reward: 1.2000000000000002 Training loss: 0.0238 Epsilon: 0.3917 Life: 123\n",
      "Episode Finish  [ 2. 12. 28.]\n",
      "Episode: 239 Total reward: 1.2999999999999998 Training loss: 0.0206 Epsilon: 0.3901 Life: 123\n",
      "Episode Finish  [ 2.  7. 24.]\n",
      "Episode: 240 Total reward: 0.4999999999999999 Training loss: 0.0205 Epsilon: 0.3884 Life: 123\n",
      "Episode Finish  [2. 4. 2.]\n",
      "Episode: 241 Total reward: -0.09999999999999976 Training loss: 0.0223 Epsilon: 0.3863 Life: 123\n",
      "Episode Finish  [ 1. 13. 24.]\n",
      "Episode: 242 Total reward: -0.6 Training loss: 0.0263 Epsilon: 0.3851 Life: 123\n",
      "Episode Finish  [ 1. 14. 24.]\n",
      "Episode: 243 Total reward: -0.4999999999999999 Training loss: 0.0160 Epsilon: 0.3839 Life: 123\n",
      "Episode Finish  [3. 7. 8.]\n",
      "Episode: 244 Total reward: 2.7 Training loss: 0.0230 Epsilon: 0.3823 Life: 123\n",
      "Episode Finish  [ 1.  8. 24.]\n",
      "Episode: 245 Total reward: -1.1 Training loss: 0.0109 Epsilon: 0.3808 Life: 123\n",
      "Episode Finish  [4. 3. 8.]\n",
      "Episode: 246 Total reward: 4.2 Training loss: 0.0232 Epsilon: 0.3787 Life: 123\n",
      "Episode Finish  [ 2.  9. 24.]\n",
      "Episode: 247 Total reward: 0.9999999999999996 Training loss: 0.0330 Epsilon: 0.3773 Life: 123\n",
      "Episode Finish  [ 2. 12. 24.]\n",
      "Episode: 248 Total reward: 1.399999999999999 Training loss: 0.0150 Epsilon: 0.3760 Life: 123\n",
      "Model updated\n",
      "Episode Finish  [ 6.  1. 20.]\n",
      "Episode: 249 Total reward: 8.3 Training loss: 0.0252 Epsilon: 0.3738 Life: 123\n",
      "Episode Finish  [ 2.  7. 24.]\n",
      "Episode: 250 Total reward: 0.7999999999999996 Training loss: 0.0299 Epsilon: 0.3723 Life: 123\n",
      "Stats Saved at 250\n",
      "Episode Finish  [3. 8. 8.]\n",
      "Episode: 251 Total reward: 2.6 Training loss: 0.0292 Epsilon: 0.3706 Life: 123\n",
      "Episode Finish  [ 1.  8. 20.]\n",
      "Episode: 252 Total reward: -1.3 Training loss: 0.0227 Epsilon: 0.3693 Life: 123\n",
      "Episode Finish  [2. 8. 4.]\n",
      "Episode: 253 Total reward: 0.8000000000000003 Training loss: 0.0205 Epsilon: 0.3677 Life: 123\n",
      "Episode Finish  [ 1. 14. 24.]\n",
      "Episode: 254 Total reward: -0.5000000000000001 Training loss: 0.0322 Epsilon: 0.3666 Life: 123\n",
      "Episode Finish  [ 2.  5. 24.]\n",
      "Episode: 255 Total reward: 0.5999999999999996 Training loss: 0.0131 Epsilon: 0.3650 Life: 123\n",
      "Episode Finish  [ 2.  6. 24.]\n",
      "Episode: 256 Total reward: 1.7 Training loss: 0.0260 Epsilon: 0.3636 Life: 123\n",
      "Episode Finish  [4. 5. 8.]\n",
      "Episode: 257 Total reward: 4.4 Training loss: 0.0219 Epsilon: 0.3616 Life: 123\n",
      "Episode Finish  [ 2.  7. 24.]\n",
      "Episode: 258 Total reward: 0.7999999999999998 Training loss: 0.0210 Epsilon: 0.3601 Life: 123\n",
      "Episode Finish  [ 3.  5. 12.]\n",
      "Episode: 259 Total reward: 2.0000000000000004 Training loss: 0.0300 Epsilon: 0.3582 Life: 123\n",
      "Episode Finish  [ 3.  1. 22.]\n",
      "Episode: 260 Total reward: 2.1000000000000005 Training loss: 0.0265 Epsilon: 0.3563 Life: 123\n",
      "Episode Finish  [ 1.  6. 10.]\n",
      "Episode: 261 Total reward: -1.4000000000000004 Training loss: 0.0197 Epsilon: 0.3548 Life: 123\n",
      "Episode Finish  [ 2.  9. 32.]\n",
      "Episode: 262 Total reward: 1.0 Training loss: 0.0194 Epsilon: 0.3537 Life: 123\n",
      "Episode Finish  [3. 4. 2.]\n",
      "Episode: 263 Total reward: 2.1 Training loss: 0.0236 Epsilon: 0.3519 Life: 123\n",
      "Episode Finish  [2. 3. 8.]\n",
      "Episode: 264 Total reward: 0.10000000000000009 Training loss: 0.0091 Epsilon: 0.3503 Life: 123\n",
      "Episode Finish  [4. 6. 8.]\n",
      "Episode: 265 Total reward: 4.4 Training loss: 0.0172 Epsilon: 0.3485 Life: 123\n",
      "Episode Finish  [2. 3. 4.]\n",
      "Episode: 266 Total reward: 0.20000000000000004 Training loss: 0.0196 Epsilon: 0.3468 Life: 123\n",
      "Episode Finish  [ 1. 12. 24.]\n",
      "Episode: 267 Total reward: -0.7 Training loss: 0.0129 Epsilon: 0.3457 Life: 123\n",
      "Episode Finish  [ 3.  1. 18.]\n",
      "Episode: 268 Total reward: 2.1 Training loss: 0.0235 Epsilon: 0.3439 Life: 123\n",
      "Episode Finish  [ 1. 13. 12.]\n",
      "Episode: 269 Total reward: -0.5999999999999999 Training loss: 0.0280 Epsilon: 0.3429 Life: 123\n",
      "Episode Finish  [ 3.  3. 12.]\n",
      "Episode: 270 Total reward: 2.0999999999999996 Training loss: 0.0198 Epsilon: 0.3410 Life: 123\n",
      "Episode Finish  [ 2. 16. 32.]\n",
      "Episode: 271 Total reward: 1.7999999999999998 Training loss: 0.0177 Epsilon: 0.3400 Life: 123\n",
      "Episode Finish  [ 2. 12. 24.]\n",
      "Episode: 272 Total reward: 1.3000000000000003 Training loss: 0.0311 Epsilon: 0.3389 Life: 123\n",
      "Episode Finish  [3. 5. 4.]\n",
      "Episode: 273 Total reward: 2.5 Training loss: 0.0178 Epsilon: 0.3375 Life: 123\n",
      "Episode Finish  [ 2.  8. 20.]\n",
      "Episode: 274 Total reward: 0.7000000000000002 Training loss: 0.0183 Epsilon: 0.3362 Life: 123\n",
      "Episode Finish  [ 2. 11. 10.]\n",
      "Episode: 275 Total reward: 1.1 Training loss: 0.0260 Epsilon: 0.3348 Life: 123\n",
      "Episode Finish  [ 2.  5. 16.]\n",
      "Episode: 276 Total reward: 0.39999999999999947 Training loss: 0.0167 Epsilon: 0.3333 Life: 123\n",
      "Episode Finish  [ 2. 10. 24.]\n",
      "Episode: 277 Total reward: 1.1 Training loss: 0.0225 Epsilon: 0.3322 Life: 123\n",
      "Episode Finish  [2. 7. 8.]\n",
      "Episode: 278 Total reward: 0.6000000000000001 Training loss: 0.0261 Epsilon: 0.3308 Life: 123\n",
      "Episode Finish  [ 1. 12. 12.]\n",
      "Episode: 279 Total reward: -0.8 Training loss: 0.0235 Epsilon: 0.3299 Life: 123\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode Finish  [ 2. 12.  8.]\n",
      "Episode: 280 Total reward: 1.2000000000000002 Training loss: 0.0255 Epsilon: 0.3290 Life: 123\n",
      "Episode Finish  [4. 3. 4.]\n",
      "Episode: 281 Total reward: 4.2 Training loss: 0.0234 Epsilon: 0.3272 Life: 123\n",
      "Episode Finish  [ 3.  9. 24.]\n",
      "Episode: 282 Total reward: 3.0 Training loss: 0.0245 Epsilon: 0.3259 Life: 123\n",
      "Episode Finish  [ 4.  0. 16.]\n",
      "Episode: 283 Total reward: 4.0 Training loss: 0.0259 Epsilon: 0.3242 Life: 123\n",
      "Episode Finish  [ 2. 12. 12.]\n",
      "Episode: 284 Total reward: 1.2000000000000002 Training loss: 0.0125 Epsilon: 0.3230 Life: 123\n",
      "Episode Finish  [ 4. 11. 24.]\n",
      "Episode: 285 Total reward: 5.2 Training loss: 0.0182 Epsilon: 0.3220 Life: 123\n",
      "Episode Finish  [ 1. 12. 24.]\n",
      "Episode: 286 Total reward: -0.7 Training loss: 0.0188 Epsilon: 0.3210 Life: 123\n",
      "Episode Finish  [ 1. 13. 24.]\n",
      "Episode: 287 Total reward: -0.5999999999999998 Training loss: 0.0284 Epsilon: 0.3201 Life: 123\n",
      "Episode Finish  [ 1. 15. 24.]\n",
      "Episode: 288 Total reward: -0.3999999999999998 Training loss: 0.0266 Epsilon: 0.3190 Life: 123\n",
      "Episode Finish  [ 1. 17. 24.]\n",
      "Episode: 289 Total reward: -0.20000000000000007 Training loss: 0.0165 Epsilon: 0.3181 Life: 123\n",
      "Episode Finish  [ 2. 12. 40.]\n",
      "Episode: 290 Total reward: 1.3000000000000003 Training loss: 0.0291 Epsilon: 0.3170 Life: 123\n",
      "Episode Finish  [ 1. 13. 24.]\n",
      "Episode: 291 Total reward: -0.5 Training loss: 0.0181 Epsilon: 0.3159 Life: 123\n",
      "Episode Finish  [ 2.  8. 40.]\n",
      "Episode: 292 Total reward: 0.7999999999999998 Training loss: 0.0154 Epsilon: 0.3148 Life: 123\n",
      "Episode Finish  [ 2.  7. 10.]\n",
      "Episode: 293 Total reward: 0.7000000000000002 Training loss: 0.0178 Epsilon: 0.3135 Life: 123\n",
      "Episode Finish  [ 1. 11. 16.]\n",
      "Episode: 294 Total reward: -0.8999999999999999 Training loss: 0.0294 Epsilon: 0.3124 Life: 123\n",
      "Episode Finish  [1. 7. 4.]\n",
      "Episode: 295 Total reward: -1.1999999999999997 Training loss: 0.0227 Epsilon: 0.3112 Life: 123\n",
      "Episode Finish  [ 0. 11. 24.]\n",
      "Episode: 296 Total reward: -2.8000000000000003 Training loss: 0.0189 Epsilon: 0.3102 Life: 123\n",
      "Episode Finish  [ 1.  8. 20.]\n",
      "Episode: 297 Total reward: -1.2 Training loss: 0.0192 Epsilon: 0.3091 Life: 123\n",
      "Episode Finish  [3. 6. 6.]\n",
      "Episode: 298 Total reward: 2.5000000000000004 Training loss: 0.0180 Epsilon: 0.3077 Life: 123\n",
      "Episode Finish  [ 2. 16. 24.]\n",
      "Episode: 299 Total reward: 1.7000000000000002 Training loss: 0.0220 Epsilon: 0.3068 Life: 123\n",
      "Episode Finish  [2. 8. 4.]\n",
      "Episode: 300 Total reward: 0.7000000000000001 Training loss: 0.0215 Epsilon: 0.3056 Life: 123\n",
      "Stats Saved at 300\n",
      "Episode Finish  [4. 3. 8.]\n",
      "Episode: 301 Total reward: 4.100000000000001 Training loss: 0.0128 Epsilon: 0.3039 Life: 123\n",
      "Episode Finish  [ 3.  0. 10.]\n",
      "Episode: 302 Total reward: 1.4 Training loss: 0.0197 Epsilon: 0.3023 Life: 123\n",
      "Episode Finish  [ 1. 12.  8.]\n",
      "Episode: 303 Total reward: -0.7999999999999999 Training loss: 0.0099 Epsilon: 0.3015 Life: 123\n",
      "Episode Finish  [ 2. 10. 24.]\n",
      "Episode: 304 Total reward: 1.2000000000000002 Training loss: 0.0264 Epsilon: 0.3005 Life: 123\n",
      "Episode Finish  [ 2.  6. 24.]\n",
      "Episode: 305 Total reward: 0.7000000000000002 Training loss: 0.0226 Epsilon: 0.2993 Life: 123\n",
      "Episode Finish  [ 2.  6. 24.]\n",
      "Episode: 306 Total reward: 0.7 Training loss: 0.0218 Epsilon: 0.2980 Life: 123\n",
      "Episode Finish  [3. 3. 8.]\n",
      "Episode: 307 Total reward: 2.0999999999999996 Training loss: 0.0204 Epsilon: 0.2965 Life: 123\n",
      "Episode Finish  [ 1. 11. 24.]\n",
      "Episode: 308 Total reward: -0.6999999999999997 Training loss: 0.0167 Epsilon: 0.2955 Life: 123\n",
      "Episode Finish  [4. 9. 4.]\n",
      "Episode: 309 Total reward: 4.8 Training loss: 0.0180 Epsilon: 0.2940 Life: 123\n",
      "Episode Finish  [ 1. 14. 36.]\n",
      "Episode: 310 Total reward: -0.5 Training loss: 0.0371 Epsilon: 0.2930 Life: 123\n",
      "Episode Finish  [ 1. 14. 20.]\n",
      "Episode: 311 Total reward: -0.6 Training loss: 0.0166 Epsilon: 0.2922 Life: 123\n",
      "Episode Finish  [ 2.  8. 20.]\n",
      "Episode: 312 Total reward: 1.0 Training loss: 0.0181 Epsilon: 0.2912 Life: 123\n",
      "Episode Finish  [2. 9. 4.]\n",
      "Episode: 313 Total reward: 1.0 Training loss: 0.0182 Epsilon: 0.2901 Life: 123\n",
      "Episode Finish  [5. 4. 8.]\n",
      "Episode: 314 Total reward: 6.2 Training loss: 0.0163 Epsilon: 0.2886 Life: 123\n",
      "Episode Finish  [ 3.  4. 24.]\n",
      "Episode: 315 Total reward: 2.5 Training loss: 0.0217 Epsilon: 0.2874 Life: 123\n",
      "Episode Finish  [ 1. 10. 24.]\n",
      "Episode: 316 Total reward: -0.7999999999999999 Training loss: 0.0227 Epsilon: 0.2865 Life: 123\n",
      "Episode Finish  [ 3.  8. 10.]\n",
      "Episode: 317 Total reward: 2.8 Training loss: 0.0207 Epsilon: 0.2852 Life: 123\n",
      "Episode Finish  [ 5.  7. 24.]\n",
      "Episode: 318 Total reward: 6.8 Training loss: 0.0240 Epsilon: 0.2840 Life: 123\n",
      "Episode Finish  [ 4. 11. 16.]\n",
      "Episode: 319 Total reward: 5.1000000000000005 Training loss: 0.0315 Epsilon: 0.2830 Life: 123\n",
      "Episode Finish  [ 1. 10. 24.]\n",
      "Episode: 320 Total reward: -0.8999999999999999 Training loss: 0.0217 Epsilon: 0.2821 Life: 123\n",
      "Episode Finish  [ 2.  8. 24.]\n",
      "Episode: 321 Total reward: 0.9000000000000004 Training loss: 0.0244 Epsilon: 0.2809 Life: 123\n",
      "Episode Finish  [ 5.  6. 10.]\n",
      "Episode: 322 Total reward: 6.6000000000000005 Training loss: 0.0199 Epsilon: 0.2796 Life: 123\n",
      "Episode Finish  [ 3.  6. 10.]\n",
      "Episode: 323 Total reward: 2.6 Training loss: 0.0282 Epsilon: 0.2784 Life: 123\n",
      "Episode Finish  [ 2. 11. 24.]\n",
      "Episode: 324 Total reward: 1.2000000000000002 Training loss: 0.0150 Epsilon: 0.2775 Life: 123\n",
      "Episode Finish  [ 4. 10. 24.]\n",
      "Episode: 325 Total reward: 5.1000000000000005 Training loss: 0.0233 Epsilon: 0.2765 Life: 123\n",
      "Episode Finish  [ 1. 10. 16.]\n",
      "Episode: 326 Total reward: -0.9999999999999998 Training loss: 0.0140 Epsilon: 0.2754 Life: 123\n",
      "Episode Finish  [5. 8. 2.]\n",
      "Episode: 327 Total reward: 6.7 Training loss: 0.0213 Epsilon: 0.2739 Life: 123\n",
      "Episode Finish  [ 3.  8. 24.]\n",
      "Episode: 328 Total reward: 2.899999999999999 Training loss: 0.0214 Epsilon: 0.2728 Life: 123\n",
      "Episode Finish  [ 1. 16. 24.]\n",
      "Episode: 329 Total reward: -0.2999999999999997 Training loss: 0.0144 Epsilon: 0.2720 Life: 123\n",
      "Episode Finish  [ 1. 16. 24.]\n",
      "Episode: 330 Total reward: -0.2999999999999997 Training loss: 0.0208 Epsilon: 0.2712 Life: 123\n",
      "Episode Finish  [ 2.  9. 12.]\n",
      "Episode: 331 Total reward: 0.8999999999999999 Training loss: 0.0214 Epsilon: 0.2701 Life: 123\n",
      "Episode Finish  [ 4.  9. 24.]\n",
      "Episode: 332 Total reward: 5.000000000000001 Training loss: 0.0270 Epsilon: 0.2688 Life: 123\n",
      "Episode Finish  [ 3. 10. 24.]\n",
      "Episode: 333 Total reward: 3.1000000000000005 Training loss: 0.0268 Epsilon: 0.2678 Life: 123\n",
      "Episode Finish  [ 2. 12.  8.]\n",
      "Episode: 334 Total reward: 1.2999999999999998 Training loss: 0.0211 Epsilon: 0.2669 Life: 123\n",
      "Episode Finish  [ 3.  6. 10.]\n",
      "Episode: 335 Total reward: 2.6 Training loss: 0.0152 Epsilon: 0.2658 Life: 123\n",
      "Episode Finish  [ 3. 12. 24.]\n",
      "Episode: 336 Total reward: 3.4000000000000004 Training loss: 0.0127 Epsilon: 0.2650 Life: 123\n",
      "Episode Finish  [ 2.  5. 10.]\n",
      "Episode: 337 Total reward: 0.30000000000000016 Training loss: 0.0251 Epsilon: 0.2636 Life: 123\n",
      "Episode Finish  [ 1. 10. 32.]\n",
      "Episode: 338 Total reward: -0.9000000000000001 Training loss: 0.0139 Epsilon: 0.2627 Life: 123\n",
      "Episode Finish  [ 2.  7. 24.]\n",
      "Episode: 339 Total reward: 0.7999999999999998 Training loss: 0.0214 Epsilon: 0.2617 Life: 123\n",
      "Episode Finish  [ 2. 11.  8.]\n",
      "Episode: 340 Total reward: 1.0999999999999996 Training loss: 0.0193 Epsilon: 0.2607 Life: 123\n",
      "Episode Finish  [ 1. 13.  4.]\n",
      "Episode: 341 Total reward: -0.7 Training loss: 0.0256 Epsilon: 0.2599 Life: 123\n",
      "Episode Finish  [ 3. 11. 36.]\n",
      "Episode: 342 Total reward: 3.3 Training loss: 0.0188 Epsilon: 0.2590 Life: 123\n",
      "Episode Finish  [ 1. 14. 24.]\n",
      "Episode: 343 Total reward: -0.5000000000000002 Training loss: 0.0273 Epsilon: 0.2583 Life: 123\n",
      "Episode Finish  [ 1.  9. 24.]\n",
      "Episode: 344 Total reward: -0.9 Training loss: 0.0202 Epsilon: 0.2575 Life: 123\n",
      "Episode Finish  [ 2.  8. 32.]\n",
      "Episode: 345 Total reward: 0.8999999999999999 Training loss: 0.0205 Epsilon: 0.2565 Life: 123\n",
      "Episode Finish  [4. 3. 8.]\n",
      "Episode: 346 Total reward: 4.1 Training loss: 0.0152 Epsilon: 0.2552 Life: 123\n",
      "Episode Finish  [ 2. 10. 16.]\n",
      "Episode: 347 Total reward: 0.8999999999999999 Training loss: 0.0135 Epsilon: 0.2540 Life: 123\n",
      "Episode Finish  [ 1. 11. 24.]\n",
      "Episode: 348 Total reward: -0.8000000000000003 Training loss: 0.0179 Epsilon: 0.2532 Life: 123\n",
      "Episode Finish  [ 3. 10. 24.]\n",
      "Episode: 349 Total reward: 3.0999999999999996 Training loss: 0.0148 Epsilon: 0.2523 Life: 123\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode Finish  [ 1. 11. 24.]\n",
      "Episode: 350 Total reward: -0.8 Training loss: 0.0265 Epsilon: 0.2514 Life: 123\n",
      "Stats Saved at 350\n",
      "Episode Finish  [ 2.  7. 24.]\n",
      "Episode: 351 Total reward: 0.8000000000000003 Training loss: 0.0114 Epsilon: 0.2505 Life: 123\n",
      "Episode Finish  [ 2.  9. 28.]\n",
      "Episode: 352 Total reward: 1.0 Training loss: 0.0338 Epsilon: 0.2496 Life: 123\n",
      "Episode Finish  [ 1. 14.  8.]\n",
      "Episode: 353 Total reward: -0.6 Training loss: 0.0127 Epsilon: 0.2488 Life: 123\n",
      "Episode Finish  [ 1. 13. 24.]\n",
      "Episode: 354 Total reward: -0.6000000000000001 Training loss: 0.0207 Epsilon: 0.2481 Life: 123\n",
      "Episode Finish  [ 2. 13.  4.]\n",
      "Episode: 355 Total reward: 1.2999999999999998 Training loss: 0.0185 Epsilon: 0.2473 Life: 123\n",
      "Episode Finish  [ 0. 13. 24.]\n",
      "Episode: 356 Total reward: -2.6 Training loss: 0.0189 Epsilon: 0.2465 Life: 123\n",
      "Episode Finish  [ 2. 12. 24.]\n",
      "Episode: 357 Total reward: 1.4000000000000004 Training loss: 0.0133 Epsilon: 0.2457 Life: 123\n",
      "Episode Finish  [4. 8. 8.]\n",
      "Episode: 358 Total reward: 4.7 Training loss: 0.0365 Epsilon: 0.2447 Life: 123\n",
      "Episode Finish  [3. 2. 6.]\n",
      "Episode: 359 Total reward: 2.1 Training loss: 0.0246 Epsilon: 0.2435 Life: 123\n",
      "Episode Finish  [ 1.  9. 32.]\n",
      "Episode: 360 Total reward: -1.0 Training loss: 0.0177 Epsilon: 0.2427 Life: 123\n",
      "Episode Finish  [2. 9. 4.]\n",
      "Episode: 361 Total reward: 0.6000000000000001 Training loss: 0.0193 Epsilon: 0.2416 Life: 123\n",
      "Episode Finish  [ 3.  6. 28.]\n",
      "Episode: 362 Total reward: 2.6999999999999997 Training loss: 0.0241 Epsilon: 0.2406 Life: 123\n",
      "Episode Finish  [ 5.  2. 10.]\n",
      "Episode: 363 Total reward: 6.199999999999999 Training loss: 0.0208 Epsilon: 0.2392 Life: 123\n",
      "Episode Finish  [ 2. 10. 24.]\n",
      "Episode: 364 Total reward: 1.1 Training loss: 0.0157 Epsilon: 0.2384 Life: 123\n",
      "Episode Finish  [ 2. 10. 32.]\n",
      "Episode: 365 Total reward: 0.9999999999999996 Training loss: 0.0161 Epsilon: 0.2376 Life: 123\n",
      "Episode Finish  [ 1. 10. 24.]\n",
      "Episode: 366 Total reward: -0.9 Training loss: 0.0137 Epsilon: 0.2368 Life: 123\n",
      "Episode Finish  [3. 2. 8.]\n",
      "Episode: 367 Total reward: 2.0 Training loss: 0.0137 Epsilon: 0.2356 Life: 123\n",
      "Episode Finish  [6. 2. 6.]\n",
      "Episode: 368 Total reward: 7.800000000000001 Training loss: 0.0318 Epsilon: 0.2341 Life: 131\n",
      "Episode Finish  [ 1. 11. 32.]\n",
      "Episode: 369 Total reward: -0.9000000000000001 Training loss: 0.0187 Epsilon: 0.2334 Life: 131\n",
      "Episode Finish  [ 3.  9. 24.]\n",
      "Episode: 370 Total reward: 3.000000000000001 Training loss: 0.0182 Epsilon: 0.2325 Life: 131\n",
      "Episode Finish  [ 3.  4. 16.]\n",
      "Episode: 371 Total reward: 2.3000000000000003 Training loss: 0.0164 Epsilon: 0.2313 Life: 131\n",
      "Model updated\n",
      "Episode Finish  [4. 8. 4.]\n",
      "Episode: 372 Total reward: 4.6 Training loss: 0.0229 Epsilon: 0.2300 Life: 131\n",
      "Episode Finish  [ 2. 11. 12.]\n",
      "Episode: 373 Total reward: 1.0000000000000004 Training loss: 0.0181 Epsilon: 0.2289 Life: 131\n",
      "Episode Finish  [ 2. 13. 24.]\n",
      "Episode: 374 Total reward: 1.4 Training loss: 0.0196 Epsilon: 0.2282 Life: 131\n",
      "Episode Finish  [ 4. 11.  8.]\n",
      "Episode: 375 Total reward: 5.0 Training loss: 0.0244 Epsilon: 0.2270 Life: 131\n",
      "Episode Finish  [ 1. 16. 24.]\n",
      "Episode: 376 Total reward: -0.40000000000000013 Training loss: 0.0218 Epsilon: 0.2262 Life: 131\n",
      "Episode Finish  [ 4. 11.  8.]\n",
      "Episode: 377 Total reward: 4.9 Training loss: 0.0273 Epsilon: 0.2250 Life: 131\n",
      "Episode Finish  [ 2. 12. 32.]\n",
      "Episode: 378 Total reward: 1.2000000000000002 Training loss: 0.0284 Epsilon: 0.2243 Life: 131\n",
      "Episode Finish  [5. 6. 8.]\n",
      "Episode: 379 Total reward: 6.4 Training loss: 0.0176 Epsilon: 0.2231 Life: 131\n",
      "Episode Finish  [ 4.  8. 10.]\n",
      "Episode: 380 Total reward: 4.8 Training loss: 0.0271 Epsilon: 0.2220 Life: 131\n",
      "Episode Finish  [ 5. 12. 24.]\n",
      "Episode: 381 Total reward: 7.199999999999999 Training loss: 0.0137 Epsilon: 0.2208 Life: 131\n",
      "Episode Finish  [ 3.  8. 10.]\n",
      "Episode: 382 Total reward: 2.8 Training loss: 0.0228 Epsilon: 0.2199 Life: 131\n",
      "Episode Finish  [ 8.  6. 16.]\n",
      "Episode: 383 Total reward: 12.500000000000002 Training loss: 0.0189 Epsilon: 0.2183 Life: 146\n",
      "Episode Finish  [ 2. 12. 20.]\n",
      "Episode: 384 Total reward: 1.3000000000000003 Training loss: 0.0224 Epsilon: 0.2176 Life: 146\n",
      "Episode Finish  [ 1. 20. 24.]\n",
      "Episode: 385 Total reward: 0.10000000000000009 Training loss: 0.0297 Epsilon: 0.2170 Life: 146\n",
      "Episode Finish  [ 3. 12. 40.]\n",
      "Episode: 386 Total reward: 3.2000000000000006 Training loss: 0.0286 Epsilon: 0.2162 Life: 146\n",
      "Episode Finish  [ 1. 20. 24.]\n",
      "Episode: 387 Total reward: 0.09999999999999987 Training loss: 0.0221 Epsilon: 0.2156 Life: 146\n",
      "Episode Finish  [4. 8. 4.]\n",
      "Episode: 388 Total reward: 4.600000000000001 Training loss: 0.0188 Epsilon: 0.2145 Life: 146\n",
      "Episode Finish  [9. 1. 6.]\n",
      "Episode: 389 Total reward: 13.999999999999998 Training loss: 0.0254 Epsilon: 0.2129 Life: 153\n",
      "Episode Finish  [ 1. 16. 24.]\n",
      "Episode: 390 Total reward: -0.2999999999999998 Training loss: 0.0230 Epsilon: 0.2123 Life: 153\n",
      "Episode Finish  [ 3. 13.  4.]\n",
      "Episode: 391 Total reward: 3.100000000000001 Training loss: 0.0163 Epsilon: 0.2112 Life: 153\n",
      "Episode Finish  [ 4. 10.  8.]\n",
      "Episode: 392 Total reward: 4.9 Training loss: 0.0233 Epsilon: 0.2101 Life: 153\n",
      "Episode Finish  [ 2. 16. 20.]\n",
      "Episode: 393 Total reward: 1.5 Training loss: 0.0135 Epsilon: 0.2094 Life: 153\n",
      "Episode Finish  [ 2. 15. 24.]\n",
      "Episode: 394 Total reward: 1.6000000000000005 Training loss: 0.0233 Epsilon: 0.2086 Life: 153\n",
      "Episode Finish  [3. 5. 2.]\n",
      "Episode: 395 Total reward: 2.1 Training loss: 0.0173 Epsilon: 0.2074 Life: 153\n",
      "Episode Finish  [ 1. 13. 24.]\n",
      "Episode: 396 Total reward: -0.5999999999999999 Training loss: 0.0159 Epsilon: 0.2068 Life: 153\n",
      "Episode Finish  [4. 6. 8.]\n",
      "Episode: 397 Total reward: 4.5 Training loss: 0.0265 Epsilon: 0.2056 Life: 153\n",
      "Episode Finish  [10.  0.  8.]\n",
      "Episode: 398 Total reward: 15.600000000000001 Training loss: 0.0217 Epsilon: 0.2036 Life: 209\n",
      "Episode Finish  [5. 8. 8.]\n",
      "Episode: 399 Total reward: 6.6 Training loss: 0.0125 Epsilon: 0.2025 Life: 209\n",
      "Episode Finish  [3. 9. 8.]\n",
      "Episode: 400 Total reward: 2.7 Training loss: 0.0121 Epsilon: 0.2014 Life: 209\n",
      "Model Saved at 400\n",
      "Stats Saved at 400\n",
      "Episode Finish  [ 2. 16. 16.]\n",
      "Episode: 401 Total reward: 1.7000000000000002 Training loss: 0.0144 Epsilon: 0.2008 Life: 209\n",
      "Episode Finish  [ 2. 12. 24.]\n",
      "Episode: 402 Total reward: 1.4 Training loss: 0.0228 Epsilon: 0.2002 Life: 209\n",
      "Episode Finish  [ 7.  6. 14.]\n",
      "Episode: 403 Total reward: 10.6 Training loss: 0.0220 Epsilon: 0.1990 Life: 209\n",
      "Episode Finish  [ 6.  7. 14.]\n",
      "Episode: 404 Total reward: 8.6 Training loss: 0.0177 Epsilon: 0.1978 Life: 209\n",
      "Episode Finish  [ 7.  3. 20.]\n",
      "Episode: 405 Total reward: 10.3 Training loss: 0.0260 Epsilon: 0.1965 Life: 209\n",
      "Episode Finish  [ 6.  5. 14.]\n",
      "Episode: 406 Total reward: 8.5 Training loss: 0.0166 Epsilon: 0.1954 Life: 209\n",
      "Episode Finish  [ 2. 12. 16.]\n",
      "Episode: 407 Total reward: 1.2 Training loss: 0.0383 Epsilon: 0.1945 Life: 209\n",
      "Episode Finish  [4. 8. 4.]\n",
      "Episode: 408 Total reward: 4.4 Training loss: 0.0307 Epsilon: 0.1935 Life: 209\n",
      "Episode Finish  [ 4.  7. 10.]\n",
      "Episode: 409 Total reward: 4.699999999999999 Training loss: 0.0214 Epsilon: 0.1926 Life: 209\n",
      "Episode Finish  [ 9.  6. 24.]\n",
      "Episode: 410 Total reward: 14.2 Training loss: 0.0231 Epsilon: 0.1913 Life: 209\n",
      "Episode Finish  [ 3. 13. 24.]\n",
      "Episode: 411 Total reward: 3.1000000000000005 Training loss: 0.0225 Epsilon: 0.1906 Life: 209\n",
      "Episode Finish  [ 3. 11. 24.]\n",
      "Episode: 412 Total reward: 3.2 Training loss: 0.0207 Epsilon: 0.1899 Life: 209\n",
      "Episode Finish  [6. 9. 8.]\n",
      "Episode: 413 Total reward: 8.6 Training loss: 0.0340 Epsilon: 0.1885 Life: 209\n",
      "Episode Finish  [7. 5. 2.]\n",
      "Episode: 414 Total reward: 10.0 Training loss: 0.0207 Epsilon: 0.1875 Life: 209\n",
      "Episode Finish  [ 2. 14. 24.]\n",
      "Episode: 415 Total reward: 1.5 Training loss: 0.0275 Epsilon: 0.1868 Life: 209\n",
      "Episode Finish  [ 3. 10. 24.]\n",
      "Episode: 416 Total reward: 3.0 Training loss: 0.0206 Epsilon: 0.1860 Life: 209\n",
      "Episode Finish  [ 4. 13.  4.]\n",
      "Episode: 417 Total reward: 5.1 Training loss: 0.0190 Epsilon: 0.1850 Life: 209\n",
      "Episode Finish  [ 3. 15. 28.]\n",
      "Episode: 418 Total reward: 3.5999999999999996 Training loss: 0.0216 Epsilon: 0.1844 Life: 209\n",
      "Episode Finish  [ 3. 16. 24.]\n",
      "Episode: 419 Total reward: 3.6999999999999997 Training loss: 0.0178 Epsilon: 0.1839 Life: 209\n",
      "Episode Finish  [ 7. 10.  6.]\n",
      "Episode: 420 Total reward: 11.100000000000001 Training loss: 0.0156 Epsilon: 0.1828 Life: 209\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode Finish  [ 2. 11. 24.]\n",
      "Episode: 421 Total reward: 1.1 Training loss: 0.0192 Epsilon: 0.1821 Life: 209\n",
      "Episode Finish  [ 5. 11.  6.]\n",
      "Episode: 422 Total reward: 7.000000000000001 Training loss: 0.0237 Epsilon: 0.1812 Life: 209\n",
      "Episode Finish  [ 8.  0. 28.]\n",
      "Episode: 423 Total reward: 11.999999999999998 Training loss: 0.0209 Epsilon: 0.1797 Life: 209\n",
      "Episode Finish  [ 2. 13. 24.]\n",
      "Episode: 424 Total reward: 1.2999999999999998 Training loss: 0.0184 Epsilon: 0.1790 Life: 209\n",
      "Episode Finish  [8. 4. 2.]\n",
      "Episode: 425 Total reward: 12.000000000000002 Training loss: 0.0254 Epsilon: 0.1777 Life: 209\n",
      "Episode Finish  [ 6.  7. 28.]\n",
      "Episode: 426 Total reward: 8.600000000000001 Training loss: 0.0249 Epsilon: 0.1767 Life: 209\n",
      "Episode Finish  [4. 7. 8.]\n",
      "Episode: 427 Total reward: 4.6000000000000005 Training loss: 0.0214 Epsilon: 0.1758 Life: 209\n",
      "Episode Finish  [ 2. 13. 24.]\n",
      "Episode: 428 Total reward: 1.4 Training loss: 0.0211 Epsilon: 0.1752 Life: 209\n",
      "Episode Finish  [ 3. 12.  8.]\n",
      "Episode: 429 Total reward: 3.3 Training loss: 0.0225 Epsilon: 0.1746 Life: 209\n",
      "Episode Finish  [ 4. 16. 24.]\n",
      "Episode: 430 Total reward: 5.7 Training loss: 0.0191 Epsilon: 0.1739 Life: 209\n",
      "Episode Finish  [ 2. 10.  8.]\n",
      "Episode: 431 Total reward: 0.9000000000000004 Training loss: 0.0361 Epsilon: 0.1732 Life: 209\n",
      "Episode Finish  [ 9.  2. 28.]\n",
      "Episode: 432 Total reward: 14.3 Training loss: 0.0190 Epsilon: 0.1719 Life: 209\n",
      "Episode Finish  [ 1. 17. 24.]\n",
      "Episode: 433 Total reward: -0.19999999999999996 Training loss: 0.0200 Epsilon: 0.1714 Life: 209\n",
      "Episode Finish  [ 4. 12. 20.]\n",
      "Episode: 434 Total reward: 5.300000000000001 Training loss: 0.0265 Epsilon: 0.1708 Life: 209\n",
      "Episode Finish  [ 4. 12.  4.]\n",
      "Episode: 435 Total reward: 5.0 Training loss: 0.0209 Epsilon: 0.1700 Life: 209\n",
      "Episode Finish  [ 2. 19.  8.]\n",
      "Episode: 436 Total reward: 1.9 Training loss: 0.0153 Epsilon: 0.1695 Life: 209\n",
      "Episode Finish  [ 1. 15. 24.]\n",
      "Episode: 437 Total reward: -0.2999999999999997 Training loss: 0.0134 Epsilon: 0.1689 Life: 209\n",
      "Episode Finish  [ 4. 13.  6.]\n",
      "Episode: 438 Total reward: 5.2 Training loss: 0.0190 Epsilon: 0.1681 Life: 209\n",
      "Episode Finish  [ 4. 10.  4.]\n",
      "Episode: 439 Total reward: 4.800000000000001 Training loss: 0.0283 Epsilon: 0.1672 Life: 209\n",
      "Episode Finish  [ 2. 12. 24.]\n",
      "Episode: 440 Total reward: 1.3000000000000003 Training loss: 0.0468 Epsilon: 0.1666 Life: 209\n",
      "Episode Finish  [10.  0. 20.]\n",
      "Episode: 441 Total reward: 15.900000000000002 Training loss: 0.0155 Epsilon: 0.1651 Life: 209\n",
      "Episode Finish  [ 3. 12. 32.]\n",
      "Episode: 442 Total reward: 3.1000000000000005 Training loss: 0.0190 Epsilon: 0.1643 Life: 209\n",
      "Episode Finish  [ 3. 14.  8.]\n",
      "Episode: 443 Total reward: 3.4000000000000004 Training loss: 0.0185 Epsilon: 0.1636 Life: 209\n",
      "Episode Finish  [ 3. 10. 18.]\n",
      "Episode: 444 Total reward: 3.0000000000000004 Training loss: 0.0226 Epsilon: 0.1629 Life: 209\n",
      "Episode Finish  [ 3. 14. 24.]\n",
      "Episode: 445 Total reward: 3.5 Training loss: 0.0359 Epsilon: 0.1622 Life: 209\n",
      "Episode Finish  [4. 6. 8.]\n",
      "Episode: 446 Total reward: 4.500000000000001 Training loss: 0.0125 Epsilon: 0.1614 Life: 209\n",
      "Episode Finish  [ 2.  6. 16.]\n",
      "Episode: 447 Total reward: 0.19999999999999973 Training loss: 0.0184 Epsilon: 0.1607 Life: 209\n",
      "Episode Finish  [4. 9. 8.]\n",
      "Episode: 448 Total reward: 4.799999999999999 Training loss: 0.0189 Epsilon: 0.1598 Life: 209\n",
      "Episode Finish  [ 2. 12. 24.]\n",
      "Episode: 449 Total reward: 1.3000000000000003 Training loss: 0.0189 Epsilon: 0.1592 Life: 209\n",
      "Episode Finish  [ 5.  8. 12.]\n",
      "Episode: 450 Total reward: 6.9 Training loss: 0.0276 Epsilon: 0.1584 Life: 209\n",
      "Stats Saved at 450\n",
      "Episode Finish  [4. 8. 8.]\n",
      "Episode: 451 Total reward: 4.6 Training loss: 0.0353 Epsilon: 0.1576 Life: 209\n",
      "Episode Finish  [7. 5. 4.]\n",
      "Episode: 452 Total reward: 10.3 Training loss: 0.0269 Epsilon: 0.1566 Life: 209\n",
      "Episode Finish  [4. 5. 2.]\n",
      "Episode: 453 Total reward: 4.300000000000001 Training loss: 0.0121 Epsilon: 0.1558 Life: 209\n",
      "Episode Finish  [ 4. 12. 24.]\n",
      "Episode: 454 Total reward: 5.200000000000001 Training loss: 0.0253 Epsilon: 0.1551 Life: 209\n",
      "Episode Finish  [ 1. 13.  8.]\n",
      "Episode: 455 Total reward: -0.7 Training loss: 0.0193 Epsilon: 0.1546 Life: 209\n",
      "Episode Finish  [ 2. 12. 24.]\n",
      "Episode: 456 Total reward: 1.2999999999999998 Training loss: 0.0188 Epsilon: 0.1541 Life: 209\n",
      "Episode Finish  [ 7. 11. 24.]\n",
      "Episode: 457 Total reward: 12.2 Training loss: 0.0186 Epsilon: 0.1533 Life: 209\n",
      "Episode Finish  [ 5. 11.  6.]\n",
      "Episode: 458 Total reward: 6.8999999999999995 Training loss: 0.0317 Epsilon: 0.1524 Life: 209\n",
      "Episode Finish  [ 3. 15. 24.]\n",
      "Episode: 459 Total reward: 3.6000000000000005 Training loss: 0.0178 Epsilon: 0.1519 Life: 209\n",
      "Episode Finish  [ 2. 17. 28.]\n",
      "Episode: 460 Total reward: 1.7 Training loss: 0.0303 Epsilon: 0.1513 Life: 209\n",
      "Episode Finish  [4. 8. 8.]\n",
      "Episode: 461 Total reward: 4.6000000000000005 Training loss: 0.0300 Epsilon: 0.1505 Life: 209\n",
      "Episode Finish  [ 3. 11.  4.]\n",
      "Episode: 462 Total reward: 2.8999999999999995 Training loss: 0.0192 Epsilon: 0.1498 Life: 209\n",
      "Episode Finish  [4. 8. 2.]\n",
      "Episode: 463 Total reward: 4.6000000000000005 Training loss: 0.0174 Epsilon: 0.1492 Life: 209\n",
      "Episode Finish  [8. 1. 8.]\n",
      "Episode: 464 Total reward: 11.9 Training loss: 0.0291 Epsilon: 0.1481 Life: 209\n",
      "Episode Finish  [6. 9. 8.]\n",
      "Episode: 465 Total reward: 8.5 Training loss: 0.0252 Epsilon: 0.1472 Life: 209\n",
      "Episode Finish  [5. 6. 4.]\n",
      "Episode: 466 Total reward: 6.4 Training loss: 0.0184 Epsilon: 0.1463 Life: 209\n",
      "Episode Finish  [3. 9. 8.]\n",
      "Episode: 467 Total reward: 2.7 Training loss: 0.0206 Epsilon: 0.1456 Life: 209\n",
      "Episode Finish  [ 2.  9. 24.]\n",
      "Episode: 468 Total reward: 1.0 Training loss: 0.0176 Epsilon: 0.1451 Life: 209\n",
      "Episode Finish  [4. 7. 4.]\n",
      "Episode: 469 Total reward: 4.5 Training loss: 0.0203 Epsilon: 0.1443 Life: 209\n",
      "Model updated\n",
      "Episode Finish  [8. 8. 2.]\n",
      "Episode: 470 Total reward: 12.299999999999999 Training loss: 0.0294 Epsilon: 0.1436 Life: 209\n",
      "Episode Finish  [ 4. 10. 24.]\n",
      "Episode: 471 Total reward: 5.0 Training loss: 0.0303 Epsilon: 0.1430 Life: 209\n",
      "Episode Finish  [ 2. 14. 24.]\n",
      "Episode: 472 Total reward: 1.5 Training loss: 0.0195 Epsilon: 0.1424 Life: 209\n",
      "Episode Finish  [ 8.  8. 24.]\n",
      "Episode: 473 Total reward: 12.9 Training loss: 0.0206 Epsilon: 0.1416 Life: 209\n",
      "Episode Finish  [4. 7. 8.]\n",
      "Episode: 474 Total reward: 4.5 Training loss: 0.0310 Epsilon: 0.1409 Life: 209\n",
      "Episode Finish  [ 1. 15. 12.]\n",
      "Episode: 475 Total reward: -0.4999999999999999 Training loss: 0.0235 Epsilon: 0.1405 Life: 209\n",
      "Episode Finish  [ 4.  7. 28.]\n",
      "Episode: 476 Total reward: 4.700000000000001 Training loss: 0.0305 Epsilon: 0.1398 Life: 209\n",
      "Episode Finish  [10.  0.  4.]\n",
      "Episode: 477 Total reward: 15.600000000000001 Training loss: 0.0161 Epsilon: 0.1385 Life: 209\n",
      "Episode Finish  [ 3. 14. 24.]\n",
      "Episode: 478 Total reward: 3.6000000000000005 Training loss: 0.0274 Epsilon: 0.1381 Life: 209\n",
      "Episode Finish  [ 9.  5. 24.]\n",
      "Episode: 479 Total reward: 14.4 Training loss: 0.0337 Epsilon: 0.1371 Life: 209\n",
      "Episode Finish  [5. 6. 8.]\n",
      "Episode: 480 Total reward: 6.4 Training loss: 0.0278 Epsilon: 0.1365 Life: 209\n",
      "Episode Finish  [5. 4. 4.]\n",
      "Episode: 481 Total reward: 6.300000000000001 Training loss: 0.0262 Epsilon: 0.1358 Life: 209\n",
      "Episode Finish  [ 7.  7. 14.]\n",
      "Episode: 482 Total reward: 10.5 Training loss: 0.0251 Epsilon: 0.1350 Life: 209\n",
      "Episode Finish  [ 5.  6. 30.]\n",
      "Episode: 483 Total reward: 6.7 Training loss: 0.0212 Epsilon: 0.1343 Life: 209\n",
      "Episode Finish  [ 4. 10. 12.]\n",
      "Episode: 484 Total reward: 5.0 Training loss: 0.0255 Epsilon: 0.1337 Life: 209\n",
      "Episode Finish  [ 6.  7. 12.]\n",
      "Episode: 485 Total reward: 8.7 Training loss: 0.0284 Epsilon: 0.1329 Life: 209\n",
      "Episode Finish  [7. 7. 2.]\n",
      "Episode: 486 Total reward: 10.5 Training loss: 0.0180 Epsilon: 0.1321 Life: 209\n",
      "Episode Finish  [ 9.  2. 24.]\n",
      "Episode: 487 Total reward: 14.3 Training loss: 0.0236 Epsilon: 0.1313 Life: 209\n",
      "Episode Finish  [ 1. 14.  4.]\n",
      "Episode: 488 Total reward: -0.7000000000000001 Training loss: 0.0292 Epsilon: 0.1308 Life: 209\n",
      "Episode Finish  [ 7. 11. 28.]\n",
      "Episode: 489 Total reward: 11.200000000000001 Training loss: 0.0205 Epsilon: 0.1301 Life: 209\n",
      "Episode Finish  [ 5. 10.  4.]\n",
      "Episode: 490 Total reward: 6.8 Training loss: 0.0259 Epsilon: 0.1295 Life: 209\n",
      "Episode Finish  [10.  0. 24.]\n",
      "Episode: 491 Total reward: 16.1 Training loss: 0.0274 Epsilon: 0.1283 Life: 209\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode Finish  [ 1. 12.  8.]\n",
      "Episode: 492 Total reward: -0.7 Training loss: 0.0283 Epsilon: 0.1279 Life: 209\n",
      "Episode Finish  [5. 6. 4.]\n",
      "Episode: 493 Total reward: 6.4 Training loss: 0.0169 Epsilon: 0.1272 Life: 209\n",
      "Episode Finish  [4. 4. 8.]\n",
      "Episode: 494 Total reward: 4.199999999999999 Training loss: 0.0213 Epsilon: 0.1266 Life: 209\n",
      "Episode Finish  [ 1. 18. 24.]\n",
      "Episode: 495 Total reward: -0.09999999999999998 Training loss: 0.0238 Epsilon: 0.1263 Life: 209\n",
      "Episode Finish  [ 6. 12.  8.]\n",
      "Episode: 496 Total reward: 9.1 Training loss: 0.0199 Epsilon: 0.1256 Life: 209\n",
      "Episode Finish  [ 2. 12. 24.]\n",
      "Episode: 497 Total reward: 1.2999999999999998 Training loss: 0.0250 Epsilon: 0.1251 Life: 209\n",
      "Episode Finish  [4. 9. 8.]\n",
      "Episode: 498 Total reward: 4.7 Training loss: 0.0183 Epsilon: 0.1245 Life: 209\n",
      "Episode Finish  [10.  5.  4.]\n",
      "Episode: 499 Total reward: 16.0 Training loss: 0.0187 Epsilon: 0.1236 Life: 209\n",
      "Episode Finish  [11.  1.  8.]\n",
      "Episode: 500 Total reward: 18.200000000000003 Training loss: 0.0242 Epsilon: 0.1227 Life: 209\n",
      "Stats Saved at 500\n",
      "Episode Finish  [ 4.  8. 10.]\n",
      "Episode: 501 Total reward: 4.800000000000001 Training loss: 0.0177 Epsilon: 0.1222 Life: 209\n",
      "Episode Finish  [ 9.  2. 24.]\n",
      "Episode: 502 Total reward: 14.3 Training loss: 0.0251 Epsilon: 0.1213 Life: 209\n",
      "Episode Finish  [10.  0.  4.]\n",
      "Episode: 503 Total reward: 15.7 Training loss: 0.0191 Epsilon: 0.1200 Life: 238\n",
      "Episode Finish  [ 3. 13. 24.]\n",
      "Episode: 504 Total reward: 3.3999999999999995 Training loss: 0.0151 Epsilon: 0.1196 Life: 238\n",
      "Episode Finish  [ 7.  6. 24.]\n",
      "Episode: 505 Total reward: 10.8 Training loss: 0.0365 Epsilon: 0.1189 Life: 238\n",
      "Episode Finish  [8. 8. 8.]\n",
      "Episode: 506 Total reward: 12.599999999999998 Training loss: 0.0209 Epsilon: 0.1182 Life: 238\n",
      "Episode Finish  [ 2. 14. 24.]\n",
      "Episode: 507 Total reward: 1.5 Training loss: 0.0177 Epsilon: 0.1178 Life: 238\n",
      "Episode Finish  [10.  1. 26.]\n",
      "Episode: 508 Total reward: 15.799999999999999 Training loss: 0.0244 Epsilon: 0.1169 Life: 238\n",
      "Episode Finish  [ 6.  5. 12.]\n",
      "Episode: 509 Total reward: 8.3 Training loss: 0.0234 Epsilon: 0.1163 Life: 238\n",
      "Episode Finish  [ 5. 11.  4.]\n",
      "Episode: 510 Total reward: 6.799999999999999 Training loss: 0.0237 Epsilon: 0.1156 Life: 238\n",
      "Episode Finish  [ 5. 10.  8.]\n",
      "Episode: 511 Total reward: 6.800000000000001 Training loss: 0.0333 Epsilon: 0.1150 Life: 238\n",
      "Episode Finish  [ 6.  6. 22.]\n",
      "Episode: 512 Total reward: 8.299999999999999 Training loss: 0.0168 Epsilon: 0.1144 Life: 238\n",
      "Episode Finish  [4. 8. 4.]\n",
      "Episode: 513 Total reward: 4.700000000000001 Training loss: 0.0204 Epsilon: 0.1138 Life: 238\n",
      "Episode Finish  [ 7.  7. 14.]\n",
      "Episode: 514 Total reward: 10.399999999999999 Training loss: 0.0261 Epsilon: 0.1132 Life: 238\n",
      "Episode Finish  [5. 7. 4.]\n",
      "Episode: 515 Total reward: 6.5 Training loss: 0.0255 Epsilon: 0.1126 Life: 238\n",
      "Episode Finish  [10.  2.  2.]\n",
      "Episode: 516 Total reward: 15.999999999999998 Training loss: 0.0249 Epsilon: 0.1118 Life: 238\n",
      "Episode Finish  [4. 8. 6.]\n",
      "Episode: 517 Total reward: 4.7 Training loss: 0.0343 Epsilon: 0.1112 Life: 238\n",
      "Episode Finish  [ 9.  0. 24.]\n",
      "Episode: 518 Total reward: 14.099999999999998 Training loss: 0.0210 Epsilon: 0.1104 Life: 238\n",
      "Episode Finish  [ 6.  8. 24.]\n",
      "Episode: 519 Total reward: 8.9 Training loss: 0.0238 Epsilon: 0.1098 Life: 238\n",
      "Episode Finish  [ 8.  0. 18.]\n",
      "Episode: 520 Total reward: 11.8 Training loss: 0.0194 Epsilon: 0.1090 Life: 238\n",
      "Episode Finish  [ 3. 14. 24.]\n",
      "Episode: 521 Total reward: 3.5 Training loss: 0.0192 Epsilon: 0.1087 Life: 238\n",
      "Episode Finish  [ 5.  7. 12.]\n",
      "Episode: 522 Total reward: 6.600000000000001 Training loss: 0.0256 Epsilon: 0.1081 Life: 238\n",
      "Episode Finish  [8. 0. 8.]\n",
      "Episode: 523 Total reward: 12.0 Training loss: 0.0348 Epsilon: 0.1072 Life: 238\n",
      "Episode Finish  [ 8. 11. 24.]\n",
      "Episode: 524 Total reward: 13.2 Training loss: 0.0273 Epsilon: 0.1066 Life: 238\n",
      "Episode Finish  [ 2. 16. 24.]\n",
      "Episode: 525 Total reward: 1.6999999999999997 Training loss: 0.0194 Epsilon: 0.1062 Life: 238\n",
      "Episode Finish  [ 5.  9. 12.]\n",
      "Episode: 526 Total reward: 6.6000000000000005 Training loss: 0.0219 Epsilon: 0.1057 Life: 238\n",
      "Episode Finish  [ 5. 10. 12.]\n",
      "Episode: 527 Total reward: 7.0 Training loss: 0.0204 Epsilon: 0.1052 Life: 238\n",
      "Episode Finish  [ 7.  5. 34.]\n",
      "Episode: 528 Total reward: 10.200000000000003 Training loss: 0.0236 Epsilon: 0.1046 Life: 238\n",
      "Episode Finish  [8. 0. 2.]\n",
      "Episode: 529 Total reward: 11.700000000000001 Training loss: 0.0424 Epsilon: 0.1039 Life: 238\n",
      "Episode Finish  [ 3. 10. 10.]\n",
      "Episode: 530 Total reward: 3.000000000000001 Training loss: 0.0291 Epsilon: 0.1034 Life: 238\n",
      "Episode Finish  [ 2. 12. 28.]\n",
      "Episode: 531 Total reward: 1.3000000000000003 Training loss: 0.0248 Epsilon: 0.1031 Life: 238\n",
      "Episode Finish  [ 3. 10. 24.]\n",
      "Episode: 532 Total reward: 3.1000000000000005 Training loss: 0.0304 Epsilon: 0.1027 Life: 238\n",
      "Episode Finish  [5. 3. 6.]\n",
      "Episode: 533 Total reward: 6.200000000000001 Training loss: 0.0302 Epsilon: 0.1022 Life: 238\n",
      "Episode Finish  [5. 8. 4.]\n",
      "Episode: 534 Total reward: 6.600000000000001 Training loss: 0.0313 Epsilon: 0.1017 Life: 238\n",
      "Episode Finish  [ 3. 11. 28.]\n",
      "Episode: 535 Total reward: 3.2 Training loss: 0.0231 Epsilon: 0.1013 Life: 238\n",
      "Episode Finish  [ 5.  6. 14.]\n",
      "Episode: 536 Total reward: 6.3 Training loss: 0.0179 Epsilon: 0.1007 Life: 238\n",
      "Episode Finish  [5. 6. 8.]\n",
      "Episode: 537 Total reward: 6.4 Training loss: 0.0152 Epsilon: 0.1002 Life: 238\n",
      "Episode Finish  [ 5. 11.  4.]\n",
      "Episode: 538 Total reward: 7.000000000000002 Training loss: 0.0172 Epsilon: 0.0998 Life: 238\n",
      "Episode Finish  [5. 6. 8.]\n",
      "Episode: 539 Total reward: 6.400000000000001 Training loss: 0.0263 Epsilon: 0.0992 Life: 238\n",
      "Episode Finish  [10.  0. 10.]\n",
      "Episode: 540 Total reward: 15.7 Training loss: 0.0183 Epsilon: 0.0984 Life: 238\n",
      "Episode Finish  [5. 4. 8.]\n",
      "Episode: 541 Total reward: 6.199999999999999 Training loss: 0.0199 Epsilon: 0.0979 Life: 238\n",
      "Episode Finish  [ 8.  4. 22.]\n",
      "Episode: 542 Total reward: 12.2 Training loss: 0.0208 Epsilon: 0.0972 Life: 238\n",
      "Episode Finish  [5. 9. 8.]\n",
      "Episode: 543 Total reward: 6.7 Training loss: 0.0131 Epsilon: 0.0967 Life: 238\n",
      "Episode Finish  [ 5. 10.  8.]\n",
      "Episode: 544 Total reward: 6.800000000000001 Training loss: 0.0287 Epsilon: 0.0963 Life: 238\n",
      "Episode Finish  [ 1. 16. 36.]\n",
      "Episode: 545 Total reward: -0.30000000000000004 Training loss: 0.0235 Epsilon: 0.0961 Life: 238\n",
      "Episode Finish  [6. 5. 4.]\n",
      "Episode: 546 Total reward: 8.299999999999999 Training loss: 0.0225 Epsilon: 0.0955 Life: 238\n",
      "Episode Finish  [5. 5. 8.]\n",
      "Episode: 547 Total reward: 6.299999999999999 Training loss: 0.0181 Epsilon: 0.0950 Life: 238\n",
      "Episode Finish  [ 3. 12. 24.]\n",
      "Episode: 548 Total reward: 3.3000000000000007 Training loss: 0.0296 Epsilon: 0.0947 Life: 238\n",
      "Episode Finish  [ 4. 15. 24.]\n",
      "Episode: 549 Total reward: 5.6000000000000005 Training loss: 0.0265 Epsilon: 0.0943 Life: 238\n",
      "Episode Finish  [5. 7. 8.]\n",
      "Episode: 550 Total reward: 6.5 Training loss: 0.0302 Epsilon: 0.0939 Life: 238\n",
      "Stats Saved at 550\n",
      "Episode Finish  [ 4.  8. 10.]\n",
      "Episode: 551 Total reward: 4.800000000000001 Training loss: 0.0231 Epsilon: 0.0935 Life: 238\n",
      "Episode Finish  [ 9.  0. 10.]\n",
      "Episode: 552 Total reward: 14.000000000000002 Training loss: 0.0333 Epsilon: 0.0928 Life: 238\n",
      "Episode Finish  [6. 8. 8.]\n",
      "Episode: 553 Total reward: 8.6 Training loss: 0.0152 Epsilon: 0.0923 Life: 238\n",
      "Episode Finish  [5. 2. 4.]\n",
      "Episode: 554 Total reward: 6.0 Training loss: 0.0191 Epsilon: 0.0918 Life: 238\n",
      "Episode Finish  [6. 4. 4.]\n",
      "Episode: 555 Total reward: 8.200000000000001 Training loss: 0.0213 Epsilon: 0.0913 Life: 238\n",
      "Model updated\n",
      "Episode Finish  [ 9.  0. 12.]\n",
      "Episode: 556 Total reward: 14.0 Training loss: 0.0308 Epsilon: 0.0906 Life: 238\n",
      "Episode Finish  [5. 7. 4.]\n",
      "Episode: 557 Total reward: 6.500000000000001 Training loss: 0.0284 Epsilon: 0.0901 Life: 238\n",
      "Episode Finish  [ 4. 11. 24.]\n",
      "Episode: 558 Total reward: 5.2 Training loss: 0.0284 Epsilon: 0.0898 Life: 238\n",
      "Episode Finish  [ 8.  7. 24.]\n",
      "Episode: 559 Total reward: 12.799999999999999 Training loss: 0.0268 Epsilon: 0.0893 Life: 238\n",
      "Episode Finish  [10.  0.  8.]\n",
      "Episode: 560 Total reward: 15.800000000000002 Training loss: 0.0300 Epsilon: 0.0886 Life: 238\n",
      "Episode Finish  [ 4. 11. 10.]\n",
      "Episode: 561 Total reward: 5.1 Training loss: 0.0161 Epsilon: 0.0882 Life: 238\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode Finish  [7. 5. 4.]\n",
      "Episode: 562 Total reward: 10.200000000000001 Training loss: 0.0165 Epsilon: 0.0877 Life: 238\n",
      "Episode Finish  [ 6. 11. 14.]\n",
      "Episode: 563 Total reward: 8.8 Training loss: 0.0281 Epsilon: 0.0873 Life: 238\n",
      "Episode Finish  [ 3. 14. 24.]\n",
      "Episode: 564 Total reward: 3.5 Training loss: 0.0184 Epsilon: 0.0870 Life: 238\n",
      "Episode Finish  [ 9.  5. 10.]\n",
      "Episode: 565 Total reward: 14.5 Training loss: 0.0263 Epsilon: 0.0863 Life: 238\n",
      "Episode Finish  [10.  4. 12.]\n",
      "Episode: 566 Total reward: 16.4 Training loss: 0.0212 Epsilon: 0.0857 Life: 238\n",
      "Episode Finish  [4. 9. 8.]\n",
      "Episode: 567 Total reward: 4.8 Training loss: 0.0301 Epsilon: 0.0852 Life: 238\n",
      "Episode Finish  [12.  0.  2.]\n",
      "Episode: 568 Total reward: 19.900000000000002 Training loss: 0.0218 Epsilon: 0.0845 Life: 238\n",
      "Episode Finish  [10.  7.  4.]\n",
      "Episode: 569 Total reward: 16.2 Training loss: 0.0295 Epsilon: 0.0838 Life: 238\n",
      "Episode Finish  [7. 4. 2.]\n",
      "Episode: 570 Total reward: 11.100000000000001 Training loss: 0.0370 Epsilon: 0.0833 Life: 238\n",
      "Episode Finish  [6. 4. 8.]\n",
      "Episode: 571 Total reward: 8.1 Training loss: 0.0197 Epsilon: 0.0829 Life: 238\n",
      "Episode Finish  [13.  0. 20.]\n",
      "Episode: 572 Total reward: 22.1 Training loss: 0.0189 Epsilon: 0.0821 Life: 238\n",
      "Episode Finish  [6. 8. 6.]\n",
      "Episode: 573 Total reward: 8.5 Training loss: 0.0212 Epsilon: 0.0817 Life: 238\n",
      "Episode Finish  [7. 0. 4.]\n",
      "Episode: 574 Total reward: 9.8 Training loss: 0.0223 Epsilon: 0.0810 Life: 238\n",
      "Episode Finish  [ 2. 13. 24.]\n",
      "Episode: 575 Total reward: 1.4000000000000004 Training loss: 0.0379 Epsilon: 0.0808 Life: 238\n",
      "Episode Finish  [ 5. 10. 46.]\n",
      "Episode: 576 Total reward: 7.0 Training loss: 0.0309 Epsilon: 0.0803 Life: 238\n",
      "Episode Finish  [10.  0. 24.]\n",
      "Episode: 577 Total reward: 16.1 Training loss: 0.0266 Epsilon: 0.0796 Life: 238\n",
      "Episode Finish  [ 6.  9. 24.]\n",
      "Episode: 578 Total reward: 8.7 Training loss: 0.0249 Epsilon: 0.0793 Life: 238\n",
      "Episode Finish  [5. 8. 4.]\n",
      "Episode: 579 Total reward: 6.6000000000000005 Training loss: 0.0303 Epsilon: 0.0789 Life: 238\n",
      "Episode Finish  [ 7.  4. 24.]\n",
      "Episode: 580 Total reward: 10.499999999999998 Training loss: 0.0202 Epsilon: 0.0784 Life: 238\n",
      "Episode Finish  [11.  8. 24.]\n",
      "Episode: 581 Total reward: 18.9 Training loss: 0.0177 Epsilon: 0.0779 Life: 238\n",
      "Episode Finish  [8. 6. 8.]\n",
      "Episode: 582 Total reward: 12.399999999999999 Training loss: 0.0354 Epsilon: 0.0773 Life: 238\n",
      "Episode Finish  [ 6.  6. 10.]\n",
      "Episode: 583 Total reward: 8.1 Training loss: 0.0250 Epsilon: 0.0769 Life: 238\n",
      "Episode Finish  [12.  0.  2.]\n",
      "Episode: 584 Total reward: 19.4 Training loss: 0.0239 Epsilon: 0.0763 Life: 238\n",
      "Episode Finish  [8. 0. 8.]\n",
      "Episode: 585 Total reward: 12.0 Training loss: 0.0248 Epsilon: 0.0757 Life: 238\n",
      "Episode Finish  [9. 1. 2.]\n",
      "Episode: 586 Total reward: 14.000000000000002 Training loss: 0.0236 Epsilon: 0.0752 Life: 238\n",
      "Episode Finish  [10.  1. 24.]\n",
      "Episode: 587 Total reward: 16.2 Training loss: 0.0213 Epsilon: 0.0747 Life: 238\n",
      "Episode Finish  [5. 4. 4.]\n",
      "Episode: 588 Total reward: 6.200000000000001 Training loss: 0.0231 Epsilon: 0.0743 Life: 238\n",
      "Episode Finish  [ 2. 17. 24.]\n",
      "Episode: 589 Total reward: 1.8000000000000003 Training loss: 0.0284 Epsilon: 0.0741 Life: 238\n",
      "Episode Finish  [5. 8. 8.]\n",
      "Episode: 590 Total reward: 6.6 Training loss: 0.0329 Epsilon: 0.0737 Life: 238\n",
      "Episode Finish  [ 5. 10.  4.]\n",
      "Episode: 591 Total reward: 6.800000000000001 Training loss: 0.0221 Epsilon: 0.0733 Life: 238\n",
      "Episode Finish  [ 8.  6. 14.]\n",
      "Episode: 592 Total reward: 12.600000000000001 Training loss: 0.0287 Epsilon: 0.0729 Life: 238\n",
      "Episode Finish  [6. 5. 8.]\n",
      "Episode: 593 Total reward: 8.4 Training loss: 0.0227 Epsilon: 0.0725 Life: 238\n",
      "Episode Finish  [ 6.  5. 40.]\n",
      "Episode: 594 Total reward: 8.4 Training loss: 0.0308 Epsilon: 0.0721 Life: 238\n",
      "Episode Finish  [4. 9. 8.]\n",
      "Episode: 595 Total reward: 4.8 Training loss: 0.0298 Epsilon: 0.0718 Life: 238\n",
      "Episode Finish  [7. 7. 4.]\n",
      "Episode: 596 Total reward: 10.499999999999998 Training loss: 0.0312 Epsilon: 0.0714 Life: 238\n",
      "Episode Finish  [ 7.  5. 16.]\n",
      "Episode: 597 Total reward: 10.3 Training loss: 0.0262 Epsilon: 0.0711 Life: 238\n",
      "Episode Finish  [5. 6. 8.]\n",
      "Episode: 598 Total reward: 6.3999999999999995 Training loss: 0.0184 Epsilon: 0.0707 Life: 238\n",
      "Episode Finish  [ 7.  6. 16.]\n",
      "Episode: 599 Total reward: 10.7 Training loss: 0.0148 Epsilon: 0.0703 Life: 238\n",
      "Episode Finish  [11.  0.  8.]\n",
      "Episode: 600 Total reward: 18.1 Training loss: 0.0193 Epsilon: 0.0697 Life: 238\n",
      "Model Saved at 600\n",
      "Stats Saved at 600\n",
      "Episode Finish  [ 9.  6. 14.]\n",
      "Episode: 601 Total reward: 14.4 Training loss: 0.0222 Epsilon: 0.0693 Life: 238\n",
      "Episode Finish  [8. 4. 4.]\n",
      "Episode: 602 Total reward: 12.3 Training loss: 0.0196 Epsilon: 0.0689 Life: 238\n",
      "Episode Finish  [ 4.  7. 10.]\n",
      "Episode: 603 Total reward: 4.7 Training loss: 0.0237 Epsilon: 0.0686 Life: 238\n",
      "Episode Finish  [ 9.  4. 10.]\n",
      "Episode: 604 Total reward: 14.399999999999999 Training loss: 0.0295 Epsilon: 0.0682 Life: 238\n",
      "Episode Finish  [5. 3. 8.]\n",
      "Episode: 605 Total reward: 6.1 Training loss: 0.0260 Epsilon: 0.0678 Life: 238\n",
      "Episode Finish  [4. 8. 8.]\n",
      "Episode: 606 Total reward: 4.7 Training loss: 0.0274 Epsilon: 0.0675 Life: 238\n",
      "Episode Finish  [ 8.  8. 22.]\n",
      "Episode: 607 Total reward: 12.5 Training loss: 0.0132 Epsilon: 0.0671 Life: 238\n",
      "Episode Finish  [5. 7. 8.]\n",
      "Episode: 608 Total reward: 6.300000000000001 Training loss: 0.0278 Epsilon: 0.0667 Life: 238\n",
      "Episode Finish  [ 2. 11. 20.]\n",
      "Episode: 609 Total reward: 2.1 Training loss: 0.0176 Epsilon: 0.0665 Life: 238\n",
      "Episode Finish  [ 5. 11.  8.]\n",
      "Episode: 610 Total reward: 7.1 Training loss: 0.0205 Epsilon: 0.0662 Life: 238\n",
      "Episode Finish  [5. 8. 8.]\n",
      "Episode: 611 Total reward: 6.700000000000001 Training loss: 0.0244 Epsilon: 0.0658 Life: 238\n",
      "Episode Finish  [ 4.  9. 36.]\n",
      "Episode: 612 Total reward: 5.000000000000001 Training loss: 0.0162 Epsilon: 0.0656 Life: 238\n",
      "Episode Finish  [ 3. 16.  4.]\n",
      "Episode: 613 Total reward: 3.6000000000000005 Training loss: 0.0193 Epsilon: 0.0654 Life: 238\n",
      "Episode Finish  [ 7.  6. 22.]\n",
      "Episode: 614 Total reward: 10.600000000000001 Training loss: 0.0206 Epsilon: 0.0650 Life: 238\n",
      "Episode Finish  [ 3. 15. 24.]\n",
      "Episode: 615 Total reward: 3.6000000000000014 Training loss: 0.0274 Epsilon: 0.0648 Life: 238\n",
      "Episode Finish  [9. 0. 2.]\n",
      "Episode: 616 Total reward: 13.8 Training loss: 0.0194 Epsilon: 0.0643 Life: 238\n",
      "Episode Finish  [ 6.  4. 10.]\n",
      "Episode: 617 Total reward: 8.3 Training loss: 0.0224 Epsilon: 0.0639 Life: 238\n",
      "Episode Finish  [1. 9. 4.]\n",
      "Episode: 618 Total reward: -1.1 Training loss: 0.0182 Epsilon: 0.0638 Life: 238\n",
      "Episode Finish  [9. 0. 6.]\n",
      "Episode: 619 Total reward: 13.900000000000002 Training loss: 0.0197 Epsilon: 0.0632 Life: 238\n",
      "Episode Finish  [5. 6. 4.]\n",
      "Episode: 620 Total reward: 6.4 Training loss: 0.0235 Epsilon: 0.0630 Life: 238\n",
      "Episode Finish  [ 5. 11.  4.]\n",
      "Episode: 621 Total reward: 6.9 Training loss: 0.0237 Epsilon: 0.0627 Life: 238\n",
      "Episode Finish  [ 7.  0. 14.]\n",
      "Episode: 622 Total reward: 9.5 Training loss: 0.0172 Epsilon: 0.0622 Life: 238\n",
      "Episode Finish  [ 9.  0. 10.]\n",
      "Episode: 623 Total reward: 14.0 Training loss: 0.0221 Epsilon: 0.0618 Life: 238\n",
      "Episode Finish  [ 9.  0. 24.]\n",
      "Episode: 624 Total reward: 14.100000000000001 Training loss: 0.0326 Epsilon: 0.0613 Life: 238\n",
      "Episode Finish  [10.  0. 24.]\n",
      "Episode: 625 Total reward: 16.1 Training loss: 0.0353 Epsilon: 0.0609 Life: 238\n",
      "Episode Finish  [7. 8. 2.]\n",
      "Episode: 626 Total reward: 10.399999999999999 Training loss: 0.0263 Epsilon: 0.0606 Life: 238\n",
      "Episode Finish  [6. 2. 6.]\n",
      "Episode: 627 Total reward: 7.900000000000001 Training loss: 0.0306 Epsilon: 0.0602 Life: 238\n",
      "Episode Finish  [11.  0. 24.]\n",
      "Episode: 628 Total reward: 18.099999999999998 Training loss: 0.0174 Epsilon: 0.0598 Life: 238\n",
      "Episode Finish  [ 4. 10. 16.]\n",
      "Episode: 629 Total reward: 5.0 Training loss: 0.0262 Epsilon: 0.0596 Life: 238\n",
      "Episode Finish  [ 4.  7. 12.]\n",
      "Episode: 630 Total reward: 4.500000000000001 Training loss: 0.0394 Epsilon: 0.0593 Life: 238\n",
      "Model updated\n",
      "Episode Finish  [5. 9. 8.]\n",
      "Episode: 631 Total reward: 6.700000000000001 Training loss: 0.0317 Epsilon: 0.0590 Life: 238\n",
      "Episode Finish  [ 4.  9. 10.]\n",
      "Episode: 632 Total reward: 4.9 Training loss: 0.0237 Epsilon: 0.0588 Life: 238\n",
      "Episode Finish  [ 6. 11.  4.]\n",
      "Episode: 633 Total reward: 8.9 Training loss: 0.0204 Epsilon: 0.0585 Life: 238\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode Finish  [9. 1. 8.]\n",
      "Episode: 634 Total reward: 13.899999999999999 Training loss: 0.0277 Epsilon: 0.0581 Life: 238\n",
      "Episode Finish  [8. 2. 8.]\n",
      "Episode: 635 Total reward: 12.0 Training loss: 0.0262 Epsilon: 0.0578 Life: 238\n",
      "Episode Finish  [ 5. 15. 24.]\n",
      "Episode: 636 Total reward: 7.6 Training loss: 0.0225 Epsilon: 0.0576 Life: 238\n",
      "Episode Finish  [ 8.  7. 10.]\n",
      "Episode: 637 Total reward: 12.7 Training loss: 0.0152 Epsilon: 0.0573 Life: 238\n",
      "Episode Finish  [ 9.  0. 24.]\n",
      "Episode: 638 Total reward: 14.1 Training loss: 0.0179 Epsilon: 0.0569 Life: 238\n",
      "Episode Finish  [ 8.  1. 14.]\n",
      "Episode: 639 Total reward: 11.7 Training loss: 0.0187 Epsilon: 0.0565 Life: 238\n",
      "Episode Finish  [10.  0.  8.]\n",
      "Episode: 640 Total reward: 16.0 Training loss: 0.0243 Epsilon: 0.0561 Life: 238\n",
      "Episode Finish  [ 4. 13. 10.]\n",
      "Episode: 641 Total reward: 5.3 Training loss: 0.0288 Epsilon: 0.0559 Life: 238\n",
      "Episode Finish  [ 4. 17. 24.]\n",
      "Episode: 642 Total reward: 5.9 Training loss: 0.0166 Epsilon: 0.0557 Life: 238\n",
      "Episode Finish  [6. 5. 6.]\n",
      "Episode: 643 Total reward: 8.3 Training loss: 0.0154 Epsilon: 0.0555 Life: 238\n",
      "Episode Finish  [ 7.  4. 10.]\n",
      "Episode: 644 Total reward: 10.4 Training loss: 0.0316 Epsilon: 0.0552 Life: 238\n",
      "Episode Finish  [ 8.  5. 24.]\n",
      "Episode: 645 Total reward: 12.6 Training loss: 0.0140 Epsilon: 0.0549 Life: 238\n",
      "Episode Finish  [ 8.  5. 24.]\n",
      "Episode: 646 Total reward: 12.600000000000001 Training loss: 0.0277 Epsilon: 0.0546 Life: 238\n",
      "Episode Finish  [ 2. 19. 32.]\n",
      "Episode: 647 Total reward: 1.7999999999999998 Training loss: 0.0215 Epsilon: 0.0545 Life: 238\n",
      "Episode Finish  [ 4. 13.  8.]\n",
      "Episode: 648 Total reward: 6.199999999999999 Training loss: 0.0226 Epsilon: 0.0542 Life: 238\n",
      "Episode Finish  [5. 8. 4.]\n",
      "Episode: 649 Total reward: 6.600000000000001 Training loss: 0.0329 Epsilon: 0.0540 Life: 238\n",
      "Episode Finish  [6. 6. 2.]\n",
      "Episode: 650 Total reward: 8.5 Training loss: 0.0270 Epsilon: 0.0538 Life: 238\n",
      "Stats Saved at 650\n",
      "Episode Finish  [9. 0. 8.]\n",
      "Episode: 651 Total reward: 13.8 Training loss: 0.0262 Epsilon: 0.0534 Life: 238\n",
      "Episode Finish  [ 5. 16. 24.]\n",
      "Episode: 652 Total reward: 7.700000000000001 Training loss: 0.0250 Epsilon: 0.0533 Life: 238\n",
      "Episode Finish  [ 3. 14. 24.]\n",
      "Episode: 653 Total reward: 3.5 Training loss: 0.0345 Epsilon: 0.0531 Life: 238\n",
      "Episode Finish  [11.  0. 24.]\n",
      "Episode: 654 Total reward: 18.1 Training loss: 0.0331 Epsilon: 0.0527 Life: 238\n",
      "Episode Finish  [13.  0.  8.]\n",
      "Episode: 655 Total reward: 21.8 Training loss: 0.0266 Epsilon: 0.0523 Life: 238\n",
      "Episode Finish  [ 3. 14. 12.]\n",
      "Episode: 656 Total reward: 3.4000000000000004 Training loss: 0.0184 Epsilon: 0.0521 Life: 238\n",
      "Episode Finish  [ 7.  9. 24.]\n",
      "Episode: 657 Total reward: 11.0 Training loss: 0.0189 Epsilon: 0.0518 Life: 238\n",
      "Episode Finish  [7. 5. 4.]\n",
      "Episode: 658 Total reward: 10.299999999999999 Training loss: 0.0223 Epsilon: 0.0516 Life: 238\n",
      "Episode Finish  [ 2. 17. 24.]\n",
      "Episode: 659 Total reward: 1.7999999999999998 Training loss: 0.0281 Epsilon: 0.0515 Life: 238\n",
      "Episode Finish  [ 5.  8. 16.]\n",
      "Episode: 660 Total reward: 6.8999999999999995 Training loss: 0.0440 Epsilon: 0.0513 Life: 238\n",
      "Episode Finish  [ 8.  3. 14.]\n",
      "Episode: 661 Total reward: 12.200000000000001 Training loss: 0.0259 Epsilon: 0.0510 Life: 238\n",
      "Episode Finish  [ 9.  0. 36.]\n",
      "Episode: 662 Total reward: 14.100000000000001 Training loss: 0.0265 Epsilon: 0.0506 Life: 238\n",
      "Episode Finish  [11.  0.  6.]\n",
      "Episode: 663 Total reward: 17.9 Training loss: 0.0257 Epsilon: 0.0502 Life: 238\n",
      "Episode Finish  [5. 8. 4.]\n",
      "Episode: 664 Total reward: 6.6000000000000005 Training loss: 0.0241 Epsilon: 0.0500 Life: 238\n",
      "Episode Finish  [ 2. 17. 32.]\n",
      "Episode: 665 Total reward: 1.7000000000000002 Training loss: 0.0238 Epsilon: 0.0499 Life: 238\n",
      "Episode Finish  [ 6.  4. 12.]\n",
      "Episode: 666 Total reward: 8.499999999999998 Training loss: 0.0179 Epsilon: 0.0497 Life: 238\n",
      "Episode Finish  [8. 0. 4.]\n",
      "Episode: 667 Total reward: 11.8 Training loss: 0.0204 Epsilon: 0.0494 Life: 238\n",
      "Episode Finish  [ 5.  6. 16.]\n",
      "Episode: 668 Total reward: 6.400000000000001 Training loss: 0.0198 Epsilon: 0.0492 Life: 238\n",
      "Episode Finish  [10.  0. 12.]\n",
      "Episode: 669 Total reward: 16.0 Training loss: 0.0447 Epsilon: 0.0488 Life: 238\n",
      "Episode Finish  [ 5. 14.  8.]\n",
      "Episode: 670 Total reward: 7.500000000000002 Training loss: 0.0292 Epsilon: 0.0487 Life: 238\n",
      "Episode Finish  [ 9.  0. 16.]\n",
      "Episode: 671 Total reward: 14.0 Training loss: 0.0188 Epsilon: 0.0483 Life: 238\n",
      "Episode Finish  [ 5.  8. 12.]\n",
      "Episode: 672 Total reward: 6.9 Training loss: 0.0272 Epsilon: 0.0482 Life: 238\n",
      "Episode Finish  [ 5. 14. 24.]\n",
      "Episode: 673 Total reward: 7.5 Training loss: 0.0190 Epsilon: 0.0479 Life: 238\n",
      "Episode Finish  [ 1. 13. 24.]\n",
      "Episode: 674 Total reward: -0.6 Training loss: 0.0340 Epsilon: 0.0478 Life: 238\n",
      "Episode Finish  [ 2. 11. 32.]\n",
      "Episode: 675 Total reward: 1.2000000000000002 Training loss: 0.0199 Epsilon: 0.0477 Life: 238\n",
      "Episode Finish  [ 2. 14. 24.]\n",
      "Episode: 676 Total reward: 1.5 Training loss: 0.0245 Epsilon: 0.0475 Life: 238\n",
      "Episode Finish  [11.  0. 32.]\n",
      "Episode: 677 Total reward: 17.9 Training loss: 0.0132 Epsilon: 0.0472 Life: 238\n",
      "Episode Finish  [ 3. 15. 12.]\n",
      "Episode: 678 Total reward: 3.5999999999999996 Training loss: 0.0222 Epsilon: 0.0471 Life: 238\n",
      "Episode Finish  [ 3. 15. 24.]\n",
      "Episode: 679 Total reward: 3.6 Training loss: 0.0205 Epsilon: 0.0469 Life: 238\n",
      "Episode Finish  [12.  0. 24.]\n",
      "Episode: 680 Total reward: 20.1 Training loss: 0.0254 Epsilon: 0.0466 Life: 238\n",
      "Episode Finish  [11.  0. 22.]\n",
      "Episode: 681 Total reward: 17.700000000000003 Training loss: 0.0215 Epsilon: 0.0462 Life: 238\n",
      "Episode Finish  [4. 8. 4.]\n",
      "Episode: 682 Total reward: 4.6 Training loss: 0.0140 Epsilon: 0.0460 Life: 238\n",
      "Episode Finish  [ 7.  5. 16.]\n",
      "Episode: 683 Total reward: 10.5 Training loss: 0.0240 Epsilon: 0.0458 Life: 238\n",
      "Episode Finish  [ 7.  4. 24.]\n",
      "Episode: 684 Total reward: 10.500000000000002 Training loss: 0.0303 Epsilon: 0.0456 Life: 238\n",
      "Episode Finish  [ 5. 16. 24.]\n",
      "Episode: 685 Total reward: 7.699999999999999 Training loss: 0.0207 Epsilon: 0.0455 Life: 238\n",
      "Episode Finish  [4. 8. 2.]\n",
      "Episode: 686 Total reward: 4.600000000000001 Training loss: 0.0134 Epsilon: 0.0453 Life: 238\n",
      "Episode Finish  [ 9.  5. 24.]\n",
      "Episode: 687 Total reward: 14.6 Training loss: 0.0233 Epsilon: 0.0450 Life: 238\n",
      "Episode Finish  [12.  5.  8.]\n",
      "Episode: 688 Total reward: 20.4 Training loss: 0.0214 Epsilon: 0.0447 Life: 238\n",
      "Episode Finish  [ 6.  8. 18.]\n",
      "Episode: 689 Total reward: 8.8 Training loss: 0.0270 Epsilon: 0.0444 Life: 238\n",
      "Episode Finish  [ 8.  0. 22.]\n",
      "Episode: 690 Total reward: 11.600000000000001 Training loss: 0.0216 Epsilon: 0.0441 Life: 238\n",
      "Episode Finish  [ 9.  1. 12.]\n",
      "Episode: 691 Total reward: 13.700000000000001 Training loss: 0.0365 Epsilon: 0.0439 Life: 238\n",
      "Episode Finish  [ 8.  8. 24.]\n",
      "Episode: 692 Total reward: 12.9 Training loss: 0.0219 Epsilon: 0.0437 Life: 238\n",
      "Episode Finish  [14.  0.  4.]\n",
      "Episode: 693 Total reward: 23.800000000000004 Training loss: 0.0183 Epsilon: 0.0433 Life: 238\n",
      "Episode Finish  [9. 0. 2.]\n",
      "Episode: 694 Total reward: 13.5 Training loss: 0.0200 Epsilon: 0.0430 Life: 238\n",
      "Episode Finish  [ 4. 15.  8.]\n",
      "Episode: 695 Total reward: 5.399999999999999 Training loss: 0.0262 Epsilon: 0.0429 Life: 238\n",
      "Episode Finish  [ 7.  9. 18.]\n",
      "Episode: 696 Total reward: 10.6 Training loss: 0.0271 Epsilon: 0.0427 Life: 238\n",
      "Episode Finish  [ 4. 14.  4.]\n",
      "Episode: 697 Total reward: 5.3999999999999995 Training loss: 0.0281 Epsilon: 0.0425 Life: 238\n",
      "Episode Finish  [ 5. 11. 24.]\n",
      "Episode: 698 Total reward: 7.200000000000001 Training loss: 0.0242 Epsilon: 0.0424 Life: 238\n",
      "Episode Finish  [13.  0.  4.]\n",
      "Episode: 699 Total reward: 21.8 Training loss: 0.0160 Epsilon: 0.0420 Life: 238\n",
      "Episode Finish  [10.  0. 24.]\n",
      "Episode: 700 Total reward: 16.099999999999998 Training loss: 0.0254 Epsilon: 0.0418 Life: 238\n",
      "Stats Saved at 700\n",
      "Episode Finish  [ 6. 13.  2.]\n",
      "Episode: 701 Total reward: 9.3 Training loss: 0.0318 Epsilon: 0.0416 Life: 238\n",
      "Episode Finish  [5. 4. 4.]\n",
      "Episode: 702 Total reward: 6.199999999999999 Training loss: 0.0190 Epsilon: 0.0414 Life: 238\n",
      "Episode Finish  [9. 0. 4.]\n",
      "Episode: 703 Total reward: 13.600000000000001 Training loss: 0.0237 Epsilon: 0.0411 Life: 238\n",
      "Episode Finish  [ 3. 14. 24.]\n",
      "Episode: 704 Total reward: 3.5 Training loss: 0.0166 Epsilon: 0.0410 Life: 238\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode Finish  [9. 0. 6.]\n",
      "Episode: 705 Total reward: 13.6 Training loss: 0.0165 Epsilon: 0.0407 Life: 238\n",
      "Episode Finish  [14.  0. 14.]\n",
      "Episode: 706 Total reward: 24.0 Training loss: 0.0169 Epsilon: 0.0404 Life: 238\n",
      "Episode Finish  [9. 0. 4.]\n",
      "Episode: 707 Total reward: 13.899999999999999 Training loss: 0.0329 Epsilon: 0.0401 Life: 238\n",
      "Episode Finish  [ 8. 12. 12.]\n",
      "Episode: 708 Total reward: 13.200000000000001 Training loss: 0.0226 Epsilon: 0.0399 Life: 238\n",
      "Model updated\n",
      "Episode Finish  [ 6. 10.  6.]\n",
      "Episode: 709 Total reward: 8.9 Training loss: 0.0255 Epsilon: 0.0398 Life: 238\n",
      "Episode Finish  [ 3. 17. 24.]\n",
      "Episode: 710 Total reward: 4.800000000000001 Training loss: 0.0323 Epsilon: 0.0397 Life: 238\n",
      "Episode Finish  [ 5. 16.  4.]\n",
      "Episode: 711 Total reward: 7.500000000000002 Training loss: 0.0266 Epsilon: 0.0395 Life: 238\n",
      "Episode Finish  [10.  2.  4.]\n",
      "Episode: 712 Total reward: 15.9 Training loss: 0.0308 Epsilon: 0.0393 Life: 238\n",
      "Episode Finish  [ 5.  9. 10.]\n",
      "Episode: 713 Total reward: 6.9 Training loss: 0.0212 Epsilon: 0.0391 Life: 238\n",
      "Episode Finish  [ 4. 11.  4.]\n",
      "Episode: 714 Total reward: 4.8999999999999995 Training loss: 0.0196 Epsilon: 0.0390 Life: 238\n",
      "Episode Finish  [ 9.  0. 24.]\n",
      "Episode: 715 Total reward: 14.1 Training loss: 0.0181 Epsilon: 0.0387 Life: 238\n",
      "Episode Finish  [ 6. 10. 10.]\n",
      "Episode: 716 Total reward: 8.8 Training loss: 0.0213 Epsilon: 0.0385 Life: 238\n",
      "Episode Finish  [ 4. 14. 10.]\n",
      "Episode: 717 Total reward: 5.4 Training loss: 0.0257 Epsilon: 0.0384 Life: 238\n",
      "Episode Finish  [ 5. 13. 24.]\n",
      "Episode: 718 Total reward: 7.4 Training loss: 0.0206 Epsilon: 0.0383 Life: 238\n",
      "Episode Finish  [10.  8. 28.]\n",
      "Episode: 719 Total reward: 16.8 Training loss: 0.0197 Epsilon: 0.0380 Life: 238\n",
      "Episode Finish  [ 8.  0. 24.]\n",
      "Episode: 720 Total reward: 12.1 Training loss: 0.0243 Epsilon: 0.0378 Life: 238\n",
      "Episode Finish  [ 2. 15.  4.]\n",
      "Episode: 721 Total reward: 1.6 Training loss: 0.0155 Epsilon: 0.0377 Life: 238\n",
      "Episode Finish  [5. 6. 8.]\n",
      "Episode: 722 Total reward: 6.4 Training loss: 0.0376 Epsilon: 0.0375 Life: 238\n",
      "Episode Finish  [10.  3. 10.]\n",
      "Episode: 723 Total reward: 16.300000000000004 Training loss: 0.0149 Epsilon: 0.0373 Life: 238\n",
      "Episode Finish  [12.  0. 24.]\n",
      "Episode: 724 Total reward: 20.1 Training loss: 0.0213 Epsilon: 0.0370 Life: 238\n",
      "Episode Finish  [ 7.  9. 24.]\n",
      "Episode: 725 Total reward: 11.0 Training loss: 0.0162 Epsilon: 0.0369 Life: 238\n",
      "Episode Finish  [5. 3. 4.]\n",
      "Episode: 726 Total reward: 5.8 Training loss: 0.0274 Epsilon: 0.0367 Life: 238\n",
      "Episode Finish  [ 8.  6. 32.]\n",
      "Episode: 727 Total reward: 12.400000000000002 Training loss: 0.0304 Epsilon: 0.0365 Life: 238\n",
      "Episode Finish  [12.  0. 16.]\n",
      "Episode: 728 Total reward: 20.1 Training loss: 0.0232 Epsilon: 0.0363 Life: 238\n",
      "Episode Finish  [ 9.  4. 28.]\n",
      "Episode: 729 Total reward: 14.6 Training loss: 0.0221 Epsilon: 0.0361 Life: 238\n",
      "Episode Finish  [7. 4. 8.]\n",
      "Episode: 730 Total reward: 10.4 Training loss: 0.0208 Epsilon: 0.0360 Life: 238\n",
      "Episode Finish  [ 7.  8. 12.]\n",
      "Episode: 731 Total reward: 10.8 Training loss: 0.0201 Epsilon: 0.0358 Life: 238\n",
      "Episode Finish  [12.  0. 14.]\n",
      "Episode: 732 Total reward: 20.0 Training loss: 0.0185 Epsilon: 0.0355 Life: 238\n",
      "Episode Finish  [13.  0.  4.]\n",
      "Episode: 733 Total reward: 21.799999999999997 Training loss: 0.0303 Epsilon: 0.0353 Life: 238\n",
      "Episode Finish  [9. 5. 8.]\n",
      "Episode: 734 Total reward: 14.3 Training loss: 0.0176 Epsilon: 0.0351 Life: 238\n",
      "Episode Finish  [12.  1.  4.]\n",
      "Episode: 735 Total reward: 19.900000000000002 Training loss: 0.0193 Epsilon: 0.0349 Life: 238\n",
      "Episode Finish  [10.  0. 10.]\n",
      "Episode: 736 Total reward: 16.0 Training loss: 0.0157 Epsilon: 0.0346 Life: 238\n",
      "Episode Finish  [12.  0.  4.]\n",
      "Episode: 737 Total reward: 19.4 Training loss: 0.0210 Epsilon: 0.0344 Life: 238\n",
      "Episode Finish  [ 3. 20. 12.]\n",
      "Episode: 738 Total reward: 4.1 Training loss: 0.0301 Epsilon: 0.0343 Life: 238\n",
      "Episode Finish  [ 6.  6. 14.]\n",
      "Episode: 739 Total reward: 8.5 Training loss: 0.0207 Epsilon: 0.0342 Life: 238\n",
      "Episode Finish  [11.  0.  2.]\n",
      "Episode: 740 Total reward: 17.8 Training loss: 0.0269 Epsilon: 0.0339 Life: 238\n",
      "Episode Finish  [11.  1. 32.]\n",
      "Episode: 741 Total reward: 18.0 Training loss: 0.0250 Epsilon: 0.0337 Life: 238\n",
      "Episode Finish  [9. 6. 8.]\n",
      "Episode: 742 Total reward: 14.3 Training loss: 0.0150 Epsilon: 0.0336 Life: 238\n",
      "Episode Finish  [ 9.  0. 24.]\n",
      "Episode: 743 Total reward: 14.1 Training loss: 0.0275 Epsilon: 0.0334 Life: 238\n",
      "Episode Finish  [ 5. 11.  8.]\n",
      "Episode: 744 Total reward: 6.9 Training loss: 0.0325 Epsilon: 0.0332 Life: 238\n",
      "Episode Finish  [ 4. 16.  8.]\n",
      "Episode: 745 Total reward: 5.5 Training loss: 0.0281 Epsilon: 0.0331 Life: 238\n",
      "Episode Finish  [13.  0. 24.]\n",
      "Episode: 746 Total reward: 22.1 Training loss: 0.0185 Epsilon: 0.0329 Life: 238\n",
      "Episode Finish  [12.  1.  6.]\n",
      "Episode: 747 Total reward: 20.0 Training loss: 0.0316 Epsilon: 0.0327 Life: 238\n",
      "Episode Finish  [13.  0.  8.]\n",
      "Episode: 748 Total reward: 21.8 Training loss: 0.0222 Epsilon: 0.0325 Life: 238\n",
      "Episode Finish  [ 4. 16. 10.]\n",
      "Episode: 749 Total reward: 5.6000000000000005 Training loss: 0.0233 Epsilon: 0.0324 Life: 238\n",
      "Episode Finish  [12.  0.  6.]\n",
      "Episode: 750 Total reward: 19.6 Training loss: 0.0172 Epsilon: 0.0321 Life: 238\n",
      "Stats Saved at 750\n",
      "Episode Finish  [ 5. 14.  6.]\n",
      "Episode: 751 Total reward: 7.4 Training loss: 0.0264 Epsilon: 0.0320 Life: 238\n",
      "Episode Finish  [8. 0. 6.]\n",
      "Episode: 752 Total reward: 12.0 Training loss: 0.0251 Epsilon: 0.0318 Life: 238\n",
      "Episode Finish  [13.  0.  6.]\n",
      "Episode: 753 Total reward: 21.299999999999997 Training loss: 0.0212 Epsilon: 0.0315 Life: 238\n",
      "Episode Finish  [11.  0. 20.]\n",
      "Episode: 754 Total reward: 17.8 Training loss: 0.0279 Epsilon: 0.0313 Life: 238\n",
      "Episode Finish  [8. 4. 8.]\n",
      "Episode: 755 Total reward: 12.2 Training loss: 0.0282 Epsilon: 0.0312 Life: 238\n",
      "Episode Finish  [ 5. 15.  6.]\n",
      "Episode: 756 Total reward: 7.3 Training loss: 0.0178 Epsilon: 0.0311 Life: 238\n",
      "Episode Finish  [ 3. 19. 16.]\n",
      "Episode: 757 Total reward: 3.9000000000000004 Training loss: 0.0235 Epsilon: 0.0310 Life: 238\n",
      "Episode Finish  [10.  5. 24.]\n",
      "Episode: 758 Total reward: 16.6 Training loss: 0.0250 Epsilon: 0.0309 Life: 238\n",
      "Episode Finish  [11.  0. 10.]\n",
      "Episode: 759 Total reward: 18.0 Training loss: 0.0208 Epsilon: 0.0306 Life: 238\n",
      "Episode Finish  [10.  0.  4.]\n",
      "Episode: 760 Total reward: 15.6 Training loss: 0.0290 Epsilon: 0.0304 Life: 238\n",
      "Episode Finish  [ 6.  8. 14.]\n",
      "Episode: 761 Total reward: 8.299999999999999 Training loss: 0.0262 Epsilon: 0.0303 Life: 238\n",
      "Episode Finish  [ 6.  4. 14.]\n",
      "Episode: 762 Total reward: 8.4 Training loss: 0.0444 Epsilon: 0.0302 Life: 238\n",
      "Episode Finish  [11.  0.  4.]\n",
      "Episode: 763 Total reward: 18.1 Training loss: 0.0230 Epsilon: 0.0300 Life: 238\n",
      "Episode Finish  [10.  0. 10.]\n",
      "Episode: 764 Total reward: 16.0 Training loss: 0.0188 Epsilon: 0.0298 Life: 238\n",
      "Episode Finish  [10.  0.  4.]\n",
      "Episode: 765 Total reward: 15.6 Training loss: 0.0237 Epsilon: 0.0296 Life: 238\n",
      "Episode Finish  [5. 8. 4.]\n",
      "Episode: 766 Total reward: 6.600000000000001 Training loss: 0.0128 Epsilon: 0.0295 Life: 238\n",
      "Episode Finish  [5. 4. 2.]\n",
      "Episode: 767 Total reward: 6.0 Training loss: 0.0234 Epsilon: 0.0294 Life: 238\n",
      "Episode Finish  [10.  0.  2.]\n",
      "Episode: 768 Total reward: 15.7 Training loss: 0.0141 Epsilon: 0.0293 Life: 238\n",
      "Episode Finish  [13.  0. 14.]\n",
      "Episode: 769 Total reward: 21.700000000000003 Training loss: 0.0226 Epsilon: 0.0290 Life: 238\n",
      "Episode Finish  [ 9.  6. 24.]\n",
      "Episode: 770 Total reward: 14.700000000000001 Training loss: 0.0264 Epsilon: 0.0289 Life: 238\n",
      "Episode Finish  [ 4. 16.  4.]\n",
      "Episode: 771 Total reward: 5.4 Training loss: 0.0197 Epsilon: 0.0288 Life: 238\n",
      "Episode Finish  [ 2. 20. 32.]\n",
      "Episode: 772 Total reward: 2.1999999999999997 Training loss: 0.0178 Epsilon: 0.0288 Life: 238\n",
      "Episode Finish  [7. 4. 8.]\n",
      "Episode: 773 Total reward: 10.4 Training loss: 0.0415 Epsilon: 0.0287 Life: 238\n",
      "Episode Finish  [11.  0.  8.]\n",
      "Episode: 774 Total reward: 17.8 Training loss: 0.0275 Epsilon: 0.0285 Life: 238\n",
      "Episode Finish  [10.  0. 10.]\n",
      "Episode: 775 Total reward: 16.0 Training loss: 0.0162 Epsilon: 0.0283 Life: 238\n",
      "Model updated\n",
      "Episode Finish  [13.  0.  4.]\n",
      "Episode: 776 Total reward: 21.799999999999997 Training loss: 0.0244 Epsilon: 0.0281 Life: 238\n",
      "Episode Finish  [13.  0. 24.]\n",
      "Episode: 777 Total reward: 22.1 Training loss: 0.0180 Epsilon: 0.0279 Life: 238\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode Finish  [10.  3. 16.]\n",
      "Episode: 778 Total reward: 16.4 Training loss: 0.0282 Epsilon: 0.0278 Life: 238\n",
      "Episode Finish  [13.  0.  8.]\n",
      "Episode: 779 Total reward: 21.7 Training loss: 0.0198 Epsilon: 0.0276 Life: 238\n",
      "Episode Finish  [11.  0. 24.]\n",
      "Episode: 780 Total reward: 18.1 Training loss: 0.0201 Epsilon: 0.0275 Life: 238\n",
      "Episode Finish  [ 5. 10. 24.]\n",
      "Episode: 781 Total reward: 7.000000000000002 Training loss: 0.0238 Epsilon: 0.0274 Life: 238\n",
      "Episode Finish  [11.  0.  4.]\n",
      "Episode: 782 Total reward: 17.799999999999997 Training loss: 0.0202 Epsilon: 0.0272 Life: 238\n",
      "Episode Finish  [ 7.  8. 20.]\n",
      "Episode: 783 Total reward: 10.9 Training loss: 0.0270 Epsilon: 0.0271 Life: 238\n",
      "Episode Finish  [8. 0. 2.]\n",
      "Episode: 784 Total reward: 11.700000000000003 Training loss: 0.0171 Epsilon: 0.0270 Life: 238\n",
      "Episode Finish  [14.  0. 24.]\n",
      "Episode: 785 Total reward: 24.1 Training loss: 0.0297 Epsilon: 0.0268 Life: 238\n",
      "Episode Finish  [12.  0.  2.]\n",
      "Episode: 786 Total reward: 19.7 Training loss: 0.0267 Epsilon: 0.0266 Life: 238\n",
      "Episode Finish  [ 6.  7. 40.]\n",
      "Episode: 787 Total reward: 8.9 Training loss: 0.0398 Epsilon: 0.0265 Life: 238\n",
      "Episode Finish  [6. 7. 8.]\n",
      "Episode: 788 Total reward: 8.5 Training loss: 0.0340 Epsilon: 0.0264 Life: 238\n",
      "Episode Finish  [10.  0. 16.]\n",
      "Episode: 789 Total reward: 16.0 Training loss: 0.0362 Epsilon: 0.0263 Life: 238\n",
      "Episode Finish  [15.  0. 10.]\n",
      "Episode: 790 Total reward: 26.1 Training loss: 0.0268 Epsilon: 0.0261 Life: 238\n",
      "Episode Finish  [ 8.  5. 24.]\n",
      "Episode: 791 Total reward: 12.6 Training loss: 0.0299 Epsilon: 0.0260 Life: 238\n",
      "Episode Finish  [ 8.  6. 24.]\n",
      "Episode: 792 Total reward: 12.7 Training loss: 0.0235 Epsilon: 0.0259 Life: 238\n",
      "Episode Finish  [10.  0. 24.]\n",
      "Episode: 793 Total reward: 16.0 Training loss: 0.0265 Epsilon: 0.0258 Life: 238\n",
      "Episode Finish  [11.  3. 24.]\n",
      "Episode: 794 Total reward: 18.1 Training loss: 0.0253 Epsilon: 0.0256 Life: 238\n",
      "Episode Finish  [10.  1. 24.]\n",
      "Episode: 795 Total reward: 16.2 Training loss: 0.0311 Epsilon: 0.0255 Life: 238\n",
      "Episode Finish  [ 9.  5. 16.]\n",
      "Episode: 796 Total reward: 14.4 Training loss: 0.0244 Epsilon: 0.0254 Life: 238\n",
      "Episode Finish  [11.  0. 10.]\n",
      "Episode: 797 Total reward: 17.9 Training loss: 0.0172 Epsilon: 0.0253 Life: 238\n",
      "Episode Finish  [14.  0. 12.]\n",
      "Episode: 798 Total reward: 23.5 Training loss: 0.0205 Epsilon: 0.0251 Life: 269\n",
      "Episode Finish  [10.  0.  8.]\n",
      "Episode: 799 Total reward: 16.0 Training loss: 0.0239 Epsilon: 0.0249 Life: 269\n",
      "Episode Finish  [ 5. 14. 24.]\n",
      "Episode: 800 Total reward: 7.5 Training loss: 0.0229 Epsilon: 0.0248 Life: 269\n",
      "Model Saved at 800\n",
      "Stats Saved at 800\n",
      "Episode Finish  [ 4. 12.  8.]\n",
      "Episode: 801 Total reward: 5.1 Training loss: 0.0193 Epsilon: 0.0248 Life: 269\n",
      "Episode Finish  [ 9.  7. 24.]\n",
      "Episode: 802 Total reward: 14.799999999999999 Training loss: 0.0241 Epsilon: 0.0247 Life: 269\n",
      "Episode Finish  [ 4. 10. 28.]\n",
      "Episode: 803 Total reward: 4.9 Training loss: 0.0308 Epsilon: 0.0246 Life: 269\n",
      "Episode Finish  [ 8.  3. 24.]\n",
      "Episode: 804 Total reward: 12.399999999999999 Training loss: 0.0201 Epsilon: 0.0245 Life: 269\n",
      "Episode Finish  [11.  0. 12.]\n",
      "Episode: 805 Total reward: 18.0 Training loss: 0.0330 Epsilon: 0.0243 Life: 269\n",
      "Episode Finish  [9. 4. 6.]\n",
      "Episode: 806 Total reward: 14.4 Training loss: 0.0340 Epsilon: 0.0243 Life: 269\n",
      "Episode Finish  [ 8.  1. 12.]\n",
      "Episode: 807 Total reward: 12.099999999999998 Training loss: 0.0321 Epsilon: 0.0241 Life: 269\n",
      "Episode Finish  [ 9.  0. 10.]\n",
      "Episode: 808 Total reward: 13.6 Training loss: 0.0228 Epsilon: 0.0240 Life: 269\n",
      "Episode Finish  [12.  0. 12.]\n",
      "Episode: 809 Total reward: 19.8 Training loss: 0.0197 Epsilon: 0.0239 Life: 269\n",
      "Episode Finish  [ 7.  4. 18.]\n",
      "Episode: 810 Total reward: 10.0 Training loss: 0.0394 Epsilon: 0.0238 Life: 269\n",
      "Episode Finish  [7. 2. 6.]\n",
      "Episode: 811 Total reward: 9.9 Training loss: 0.0356 Epsilon: 0.0237 Life: 269\n",
      "Episode Finish  [ 5. 16. 24.]\n",
      "Episode: 812 Total reward: 7.7 Training loss: 0.0276 Epsilon: 0.0236 Life: 269\n",
      "Episode Finish  [10.  7. 12.]\n",
      "Episode: 813 Total reward: 16.6 Training loss: 0.0183 Epsilon: 0.0235 Life: 269\n",
      "Episode Finish  [15.  0. 24.]\n",
      "Episode: 814 Total reward: 26.099999999999998 Training loss: 0.0289 Epsilon: 0.0234 Life: 269\n",
      "Episode Finish  [10.  0. 16.]\n",
      "Episode: 815 Total reward: 16.0 Training loss: 0.0210 Epsilon: 0.0232 Life: 269\n",
      "Episode Finish  [ 4. 15. 12.]\n",
      "Episode: 816 Total reward: 5.6 Training loss: 0.0320 Epsilon: 0.0232 Life: 269\n",
      "Episode Finish  [5. 7. 8.]\n",
      "Episode: 817 Total reward: 6.5 Training loss: 0.0228 Epsilon: 0.0231 Life: 269\n",
      "Episode Finish  [ 7.  4. 14.]\n",
      "Episode: 818 Total reward: 10.399999999999999 Training loss: 0.0333 Epsilon: 0.0230 Life: 269\n",
      "Episode Finish  [ 4. 13. 12.]\n",
      "Episode: 819 Total reward: 5.200000000000001 Training loss: 0.0328 Epsilon: 0.0230 Life: 269\n",
      "Episode Finish  [ 8.  5. 14.]\n",
      "Episode: 820 Total reward: 12.5 Training loss: 0.0264 Epsilon: 0.0229 Life: 269\n",
      "Episode Finish  [10.  0.  6.]\n",
      "Episode: 821 Total reward: 15.900000000000002 Training loss: 0.0238 Epsilon: 0.0228 Life: 269\n",
      "Episode Finish  [11.  1. 24.]\n",
      "Episode: 822 Total reward: 18.2 Training loss: 0.0167 Epsilon: 0.0227 Life: 269\n",
      "Episode Finish  [8. 7. 4.]\n",
      "Episode: 823 Total reward: 12.700000000000001 Training loss: 0.0234 Epsilon: 0.0226 Life: 269\n",
      "Episode Finish  [10.  5. 24.]\n",
      "Episode: 824 Total reward: 16.6 Training loss: 0.0313 Epsilon: 0.0225 Life: 269\n",
      "Episode Finish  [ 3. 20.  4.]\n",
      "Episode: 825 Total reward: 3.8 Training loss: 0.0351 Epsilon: 0.0225 Life: 269\n",
      "Episode Finish  [10.  6.  4.]\n",
      "Episode: 826 Total reward: 16.6 Training loss: 0.0231 Epsilon: 0.0224 Life: 269\n",
      "Episode Finish  [ 9.  2. 24.]\n",
      "Episode: 827 Total reward: 14.3 Training loss: 0.0337 Epsilon: 0.0223 Life: 269\n",
      "Episode Finish  [ 5. 10.  8.]\n",
      "Episode: 828 Total reward: 6.8 Training loss: 0.0288 Epsilon: 0.0222 Life: 269\n",
      "Episode Finish  [16.  0.  4.]\n",
      "Episode: 829 Total reward: 27.799999999999997 Training loss: 0.0261 Epsilon: 0.0221 Life: 269\n",
      "Episode Finish  [10.  0.  8.]\n",
      "Episode: 830 Total reward: 15.8 Training loss: 0.0292 Epsilon: 0.0219 Life: 269\n",
      "Episode Finish  [13.  0. 10.]\n",
      "Episode: 831 Total reward: 22.1 Training loss: 0.0253 Epsilon: 0.0218 Life: 269\n",
      "Episode Finish  [11.  0.  4.]\n",
      "Episode: 832 Total reward: 17.8 Training loss: 0.0310 Epsilon: 0.0217 Life: 269\n",
      "Episode Finish  [11.  0.  8.]\n",
      "Episode: 833 Total reward: 17.799999999999997 Training loss: 0.0349 Epsilon: 0.0216 Life: 269\n",
      "Episode Finish  [ 7. 10. 14.]\n",
      "Episode: 834 Total reward: 11.0 Training loss: 0.0210 Epsilon: 0.0215 Life: 269\n",
      "Episode Finish  [ 7.  8. 14.]\n",
      "Episode: 835 Total reward: 10.500000000000002 Training loss: 0.0190 Epsilon: 0.0215 Life: 269\n",
      "Episode Finish  [13.  0. 30.]\n",
      "Episode: 836 Total reward: 22.1 Training loss: 0.0226 Epsilon: 0.0213 Life: 269\n",
      "Episode Finish  [5. 9. 8.]\n",
      "Episode: 837 Total reward: 6.7 Training loss: 0.0147 Epsilon: 0.0213 Life: 269\n",
      "Episode Finish  [10.  4. 24.]\n",
      "Episode: 838 Total reward: 16.5 Training loss: 0.0200 Epsilon: 0.0212 Life: 269\n",
      "Episode Finish  [ 7.  5. 14.]\n",
      "Episode: 839 Total reward: 10.200000000000001 Training loss: 0.0134 Epsilon: 0.0211 Life: 269\n",
      "Episode Finish  [ 9.  4. 24.]\n",
      "Episode: 840 Total reward: 14.5 Training loss: 0.0259 Epsilon: 0.0211 Life: 269\n",
      "Model updated\n",
      "Episode Finish  [ 8.  7. 28.]\n",
      "Episode: 841 Total reward: 12.8 Training loss: 0.0253 Epsilon: 0.0210 Life: 269\n",
      "Episode Finish  [ 7.  8. 20.]\n",
      "Episode: 842 Total reward: 10.899999999999999 Training loss: 0.0201 Epsilon: 0.0209 Life: 269\n",
      "Episode Finish  [11.  0.  2.]\n",
      "Episode: 843 Total reward: 17.799999999999997 Training loss: 0.0182 Epsilon: 0.0208 Life: 269\n",
      "Episode Finish  [11.  8.  8.]\n",
      "Episode: 844 Total reward: 18.9 Training loss: 0.0223 Epsilon: 0.0207 Life: 269\n",
      "Episode Finish  [8. 0. 6.]\n",
      "Episode: 845 Total reward: 11.600000000000001 Training loss: 0.0211 Epsilon: 0.0206 Life: 269\n",
      "Episode Finish  [ 8.  7. 10.]\n",
      "Episode: 846 Total reward: 12.700000000000001 Training loss: 0.0328 Epsilon: 0.0206 Life: 269\n",
      "Episode Finish  [ 9. 10. 24.]\n",
      "Episode: 847 Total reward: 15.099999999999998 Training loss: 0.0303 Epsilon: 0.0205 Life: 269\n",
      "Episode Finish  [6. 5. 2.]\n",
      "Episode: 848 Total reward: 8.3 Training loss: 0.0153 Epsilon: 0.0204 Life: 269\n",
      "Episode Finish  [ 8.  7. 40.]\n",
      "Episode: 849 Total reward: 12.9 Training loss: 0.0211 Epsilon: 0.0204 Life: 269\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode Finish  [ 4. 10.  8.]\n",
      "Episode: 850 Total reward: 4.800000000000001 Training loss: 0.0323 Epsilon: 0.0203 Life: 269\n",
      "Stats Saved at 850\n",
      "Episode Finish  [ 8.  0. 12.]\n",
      "Episode: 851 Total reward: 11.600000000000001 Training loss: 0.0235 Epsilon: 0.0202 Life: 269\n",
      "Episode Finish  [ 6. 13. 24.]\n",
      "Episode: 852 Total reward: 9.4 Training loss: 0.0255 Epsilon: 0.0202 Life: 269\n",
      "Episode Finish  [ 8.  0. 24.]\n",
      "Episode: 853 Total reward: 12.100000000000001 Training loss: 0.0262 Epsilon: 0.0201 Life: 269\n",
      "Episode Finish  [11.  0. 16.]\n",
      "Episode: 854 Total reward: 18.0 Training loss: 0.0246 Epsilon: 0.0200 Life: 269\n",
      "Episode Finish  [13.  0. 24.]\n",
      "Episode: 855 Total reward: 22.1 Training loss: 0.0217 Epsilon: 0.0200 Life: 269\n",
      "Episode Finish  [ 4. 14. 24.]\n",
      "Episode: 856 Total reward: 5.5 Training loss: 0.0315 Epsilon: 0.0199 Life: 269\n",
      "Episode Finish  [ 7. 10. 24.]\n",
      "Episode: 857 Total reward: 11.1 Training loss: 0.0253 Epsilon: 0.0199 Life: 269\n",
      "Episode Finish  [ 8.  4. 20.]\n",
      "Episode: 858 Total reward: 12.299999999999999 Training loss: 0.0149 Epsilon: 0.0198 Life: 269\n",
      "Episode Finish  [11.  6. 14.]\n",
      "Episode: 859 Total reward: 18.4 Training loss: 0.0185 Epsilon: 0.0197 Life: 269\n",
      "Episode Finish  [ 5. 15. 24.]\n",
      "Episode: 860 Total reward: 7.6 Training loss: 0.0356 Epsilon: 0.0197 Life: 269\n",
      "Episode Finish  [ 8.  0. 20.]\n",
      "Episode: 861 Total reward: 11.6 Training loss: 0.0269 Epsilon: 0.0196 Life: 269\n",
      "Episode Finish  [9. 0. 4.]\n",
      "Episode: 862 Total reward: 13.5 Training loss: 0.0317 Epsilon: 0.0195 Life: 269\n",
      "Episode Finish  [10.  0. 24.]\n",
      "Episode: 863 Total reward: 16.1 Training loss: 0.0254 Epsilon: 0.0195 Life: 269\n",
      "Episode Finish  [ 8.  3. 24.]\n",
      "Episode: 864 Total reward: 12.399999999999999 Training loss: 0.0152 Epsilon: 0.0194 Life: 269\n",
      "Episode Finish  [4. 7. 8.]\n",
      "Episode: 865 Total reward: 4.5 Training loss: 0.0189 Epsilon: 0.0193 Life: 269\n",
      "Episode Finish  [13.  0.  2.]\n",
      "Episode: 866 Total reward: 21.5 Training loss: 0.0288 Epsilon: 0.0192 Life: 269\n",
      "Episode Finish  [10.  1. 24.]\n",
      "Episode: 867 Total reward: 16.199999999999996 Training loss: 0.0261 Epsilon: 0.0192 Life: 269\n",
      "Episode Finish  [8. 9. 8.]\n",
      "Episode: 868 Total reward: 12.600000000000001 Training loss: 0.0280 Epsilon: 0.0191 Life: 269\n",
      "Episode Finish  [7. 3. 8.]\n",
      "Episode: 869 Total reward: 10.100000000000001 Training loss: 0.0331 Epsilon: 0.0190 Life: 269\n",
      "Episode Finish  [ 9.  0. 24.]\n",
      "Episode: 870 Total reward: 14.1 Training loss: 0.0206 Epsilon: 0.0190 Life: 269\n",
      "Episode Finish  [12.  0.  8.]\n",
      "Episode: 871 Total reward: 19.8 Training loss: 0.0305 Epsilon: 0.0189 Life: 269\n",
      "Episode Finish  [ 9.  6. 24.]\n",
      "Episode: 872 Total reward: 14.700000000000001 Training loss: 0.0145 Epsilon: 0.0188 Life: 269\n",
      "Episode Finish  [ 7. 10. 12.]\n",
      "Episode: 873 Total reward: 11.799999999999999 Training loss: 0.0221 Epsilon: 0.0188 Life: 269\n",
      "Episode Finish  [10.  0.  4.]\n",
      "Episode: 874 Total reward: 16.0 Training loss: 0.0200 Epsilon: 0.0187 Life: 269\n",
      "Episode Finish  [8. 5. 8.]\n",
      "Episode: 875 Total reward: 12.5 Training loss: 0.0305 Epsilon: 0.0186 Life: 269\n",
      "Episode Finish  [7. 0. 8.]\n",
      "Episode: 876 Total reward: 9.7 Training loss: 0.0295 Epsilon: 0.0186 Life: 269\n",
      "Episode Finish  [7. 0. 8.]\n",
      "Episode: 877 Total reward: 9.600000000000001 Training loss: 0.0279 Epsilon: 0.0185 Life: 269\n",
      "Episode Finish  [12.  0.  4.]\n",
      "Episode: 878 Total reward: 19.799999999999997 Training loss: 0.0230 Epsilon: 0.0184 Life: 269\n",
      "Episode Finish  [11.  0. 16.]\n",
      "Episode: 879 Total reward: 18.0 Training loss: 0.0313 Epsilon: 0.0183 Life: 269\n",
      "Episode Finish  [10.  0. 24.]\n",
      "Episode: 880 Total reward: 16.1 Training loss: 0.0220 Epsilon: 0.0183 Life: 269\n",
      "Episode Finish  [11.  0. 10.]\n",
      "Episode: 881 Total reward: 18.0 Training loss: 0.0229 Epsilon: 0.0182 Life: 269\n",
      "Episode Finish  [12.  0.  6.]\n",
      "Episode: 882 Total reward: 19.6 Training loss: 0.0273 Epsilon: 0.0181 Life: 269\n",
      "Episode Finish  [13.  0.  4.]\n",
      "Episode: 883 Total reward: 21.8 Training loss: 0.0236 Epsilon: 0.0180 Life: 269\n",
      "Episode Finish  [13.  0.  8.]\n",
      "Episode: 884 Total reward: 21.6 Training loss: 0.0234 Epsilon: 0.0179 Life: 269\n",
      "Episode Finish  [13.  0. 16.]\n",
      "Episode: 885 Total reward: 22.1 Training loss: 0.0202 Epsilon: 0.0179 Life: 269\n",
      "Episode Finish  [11.  0. 24.]\n",
      "Episode: 886 Total reward: 18.1 Training loss: 0.0254 Epsilon: 0.0178 Life: 269\n",
      "Episode Finish  [10.  0. 14.]\n",
      "Episode: 887 Total reward: 15.9 Training loss: 0.0165 Epsilon: 0.0177 Life: 269\n",
      "Episode Finish  [ 7.  7. 26.]\n",
      "Episode: 888 Total reward: 10.6 Training loss: 0.0192 Epsilon: 0.0177 Life: 269\n",
      "Episode Finish  [8. 6. 8.]\n",
      "Episode: 889 Total reward: 12.4 Training loss: 0.0263 Epsilon: 0.0176 Life: 269\n",
      "Episode Finish  [12.  0. 24.]\n",
      "Episode: 890 Total reward: 20.099999999999998 Training loss: 0.0171 Epsilon: 0.0175 Life: 269\n",
      "Episode Finish  [10.  0.  4.]\n",
      "Episode: 891 Total reward: 15.799999999999997 Training loss: 0.0169 Epsilon: 0.0175 Life: 269\n",
      "Episode Finish  [ 4. 10.  8.]\n",
      "Episode: 892 Total reward: 4.899999999999999 Training loss: 0.0177 Epsilon: 0.0174 Life: 269\n",
      "Episode Finish  [10.  0. 16.]\n",
      "Episode: 893 Total reward: 15.999999999999998 Training loss: 0.0375 Epsilon: 0.0174 Life: 269\n",
      "Episode Finish  [ 8.  0. 16.]\n",
      "Episode: 894 Total reward: 11.600000000000001 Training loss: 0.0212 Epsilon: 0.0173 Life: 269\n",
      "Episode Finish  [ 4. 17. 24.]\n",
      "Episode: 895 Total reward: 5.800000000000001 Training loss: 0.0231 Epsilon: 0.0173 Life: 269\n",
      "Episode Finish  [ 9.  0. 12.]\n",
      "Episode: 896 Total reward: 14.0 Training loss: 0.0226 Epsilon: 0.0172 Life: 269\n",
      "Episode Finish  [10.  0.  8.]\n",
      "Episode: 897 Total reward: 15.799999999999999 Training loss: 0.0506 Epsilon: 0.0172 Life: 269\n",
      "Episode Finish  [12.  0.  8.]\n",
      "Episode: 898 Total reward: 20.1 Training loss: 0.0280 Epsilon: 0.0171 Life: 269\n",
      "Episode Finish  [ 8.  7. 24.]\n",
      "Episode: 899 Total reward: 12.799999999999999 Training loss: 0.0214 Epsilon: 0.0171 Life: 269\n",
      "Episode Finish  [ 9.  0. 24.]\n",
      "Episode: 900 Total reward: 14.1 Training loss: 0.0253 Epsilon: 0.0170 Life: 269\n",
      "Stats Saved at 900\n",
      "Episode Finish  [8. 6. 8.]\n",
      "Episode: 901 Total reward: 12.700000000000001 Training loss: 0.0133 Epsilon: 0.0170 Life: 269\n",
      "Episode Finish  [ 9.  0. 24.]\n",
      "Episode: 902 Total reward: 14.1 Training loss: 0.0241 Epsilon: 0.0169 Life: 269\n",
      "Episode Finish  [8. 1. 4.]\n",
      "Episode: 903 Total reward: 11.8 Training loss: 0.0253 Epsilon: 0.0168 Life: 269\n",
      "Episode Finish  [10.  3. 16.]\n",
      "Episode: 904 Total reward: 16.3 Training loss: 0.0268 Epsilon: 0.0168 Life: 269\n",
      "Episode Finish  [10.  0. 24.]\n",
      "Episode: 905 Total reward: 16.1 Training loss: 0.0145 Epsilon: 0.0167 Life: 269\n",
      "Episode Finish  [ 6.  5. 56.]\n",
      "Episode: 906 Total reward: 9.6 Training loss: 0.0144 Epsilon: 0.0167 Life: 269\n",
      "Model updated\n",
      "Episode Finish  [11.  0. 24.]\n",
      "Episode: 907 Total reward: 18.1 Training loss: 0.0170 Epsilon: 0.0166 Life: 269\n",
      "Episode Finish  [9. 0. 8.]\n",
      "Episode: 908 Total reward: 13.799999999999999 Training loss: 0.0296 Epsilon: 0.0166 Life: 269\n",
      "Episode Finish  [10.  0.  4.]\n",
      "Episode: 909 Total reward: 15.899999999999999 Training loss: 0.0244 Epsilon: 0.0165 Life: 269\n",
      "Episode Finish  [ 8.  0. 24.]\n",
      "Episode: 910 Total reward: 12.100000000000001 Training loss: 0.0200 Epsilon: 0.0165 Life: 269\n",
      "Episode Finish  [10.  4. 24.]\n",
      "Episode: 911 Total reward: 16.5 Training loss: 0.0209 Epsilon: 0.0164 Life: 269\n",
      "Episode Finish  [ 9.  0. 16.]\n",
      "Episode: 912 Total reward: 13.900000000000002 Training loss: 0.0245 Epsilon: 0.0164 Life: 269\n",
      "Episode Finish  [13.  0. 24.]\n",
      "Episode: 913 Total reward: 22.099999999999998 Training loss: 0.0248 Epsilon: 0.0163 Life: 269\n",
      "Episode Finish  [10.  0. 24.]\n",
      "Episode: 914 Total reward: 16.1 Training loss: 0.0229 Epsilon: 0.0163 Life: 269\n",
      "Episode Finish  [ 9.  0. 28.]\n",
      "Episode: 915 Total reward: 14.0 Training loss: 0.0227 Epsilon: 0.0162 Life: 269\n",
      "Episode Finish  [ 9.  7. 24.]\n",
      "Episode: 916 Total reward: 14.799999999999999 Training loss: 0.0269 Epsilon: 0.0162 Life: 269\n",
      "Episode Finish  [ 8.  2. 24.]\n",
      "Episode: 917 Total reward: 12.3 Training loss: 0.0305 Epsilon: 0.0161 Life: 269\n",
      "Episode Finish  [13.  0. 24.]\n",
      "Episode: 918 Total reward: 22.099999999999998 Training loss: 0.0358 Epsilon: 0.0161 Life: 269\n",
      "Episode Finish  [6. 0. 6.]\n",
      "Episode: 919 Total reward: 7.9 Training loss: 0.0331 Epsilon: 0.0161 Life: 269\n",
      "Episode Finish  [8. 3. 8.]\n",
      "Episode: 920 Total reward: 12.100000000000001 Training loss: 0.0236 Epsilon: 0.0160 Life: 269\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode Finish  [10.  0.  8.]\n",
      "Episode: 921 Total reward: 15.600000000000001 Training loss: 0.0334 Epsilon: 0.0160 Life: 269\n",
      "Episode Finish  [ 9.  3. 10.]\n",
      "Episode: 922 Total reward: 14.100000000000001 Training loss: 0.0232 Epsilon: 0.0159 Life: 269\n",
      "Episode Finish  [6. 0. 8.]\n",
      "Episode: 923 Total reward: 7.800000000000001 Training loss: 0.0266 Epsilon: 0.0159 Life: 269\n",
      "Episode Finish  [14.  0. 24.]\n",
      "Episode: 924 Total reward: 24.0 Training loss: 0.0265 Epsilon: 0.0158 Life: 269\n",
      "Episode Finish  [13.  0. 16.]\n",
      "Episode: 925 Total reward: 22.0 Training loss: 0.0304 Epsilon: 0.0157 Life: 269\n",
      "Episode Finish  [10.  0. 48.]\n",
      "Episode: 926 Total reward: 15.899999999999999 Training loss: 0.0230 Epsilon: 0.0157 Life: 269\n",
      "Episode Finish  [13.  0. 34.]\n",
      "Episode: 927 Total reward: 22.0 Training loss: 0.0265 Epsilon: 0.0156 Life: 269\n",
      "Episode Finish  [10.  0. 24.]\n",
      "Episode: 928 Total reward: 15.9 Training loss: 0.0187 Epsilon: 0.0156 Life: 269\n",
      "Episode Finish  [ 9.  4. 10.]\n",
      "Episode: 929 Total reward: 13.899999999999999 Training loss: 0.0238 Epsilon: 0.0155 Life: 269\n",
      "Episode Finish  [10.  1.  8.]\n",
      "Episode: 930 Total reward: 15.899999999999999 Training loss: 0.0368 Epsilon: 0.0155 Life: 269\n",
      "Episode Finish  [10.  0. 32.]\n",
      "Episode: 931 Total reward: 15.8 Training loss: 0.0246 Epsilon: 0.0155 Life: 269\n",
      "Episode Finish  [8. 0. 8.]\n",
      "Episode: 932 Total reward: 11.700000000000001 Training loss: 0.0287 Epsilon: 0.0154 Life: 269\n",
      "Episode Finish  [10.  0.  8.]\n",
      "Episode: 933 Total reward: 16.0 Training loss: 0.0288 Epsilon: 0.0154 Life: 269\n",
      "Episode Finish  [10.  0. 24.]\n",
      "Episode: 934 Total reward: 16.1 Training loss: 0.0205 Epsilon: 0.0153 Life: 269\n",
      "Episode Finish  [15.  0. 16.]\n",
      "Episode: 935 Total reward: 26.1 Training loss: 0.0281 Epsilon: 0.0153 Life: 269\n",
      "Episode Finish  [10.  0. 10.]\n",
      "Episode: 936 Total reward: 16.0 Training loss: 0.0188 Epsilon: 0.0152 Life: 269\n",
      "Episode Finish  [10.  0. 24.]\n",
      "Episode: 937 Total reward: 16.1 Training loss: 0.0246 Epsilon: 0.0152 Life: 269\n",
      "Episode Finish  [11.  0. 24.]\n",
      "Episode: 938 Total reward: 18.1 Training loss: 0.0248 Epsilon: 0.0151 Life: 269\n",
      "Episode Finish  [12.  0. 18.]\n",
      "Episode: 939 Total reward: 19.8 Training loss: 0.0209 Epsilon: 0.0151 Life: 269\n",
      "Episode Finish  [ 8.  4. 24.]\n",
      "Episode: 940 Total reward: 12.5 Training loss: 0.0213 Epsilon: 0.0150 Life: 269\n",
      "Episode Finish  [15.  0.  4.]\n",
      "Episode: 941 Total reward: 25.799999999999997 Training loss: 0.0246 Epsilon: 0.0150 Life: 269\n",
      "Episode Finish  [10.  0. 12.]\n",
      "Episode: 942 Total reward: 15.9 Training loss: 0.0361 Epsilon: 0.0149 Life: 269\n",
      "Episode Finish  [10.  0.  2.]\n",
      "Episode: 943 Total reward: 16.0 Training loss: 0.0231 Epsilon: 0.0149 Life: 269\n",
      "Episode Finish  [10.  0.  2.]\n",
      "Episode: 944 Total reward: 15.7 Training loss: 0.0256 Epsilon: 0.0149 Life: 269\n",
      "Episode Finish  [12.  0. 10.]\n",
      "Episode: 945 Total reward: 20.0 Training loss: 0.0280 Epsilon: 0.0148 Life: 269\n",
      "Episode Finish  [14.  0.  8.]\n",
      "Episode: 946 Total reward: 24.0 Training loss: 0.0261 Epsilon: 0.0148 Life: 269\n",
      "Episode Finish  [10.  0.  6.]\n",
      "Episode: 947 Total reward: 15.899999999999999 Training loss: 0.0304 Epsilon: 0.0147 Life: 269\n",
      "Episode Finish  [10.  1.  8.]\n",
      "Episode: 948 Total reward: 16.1 Training loss: 0.0232 Epsilon: 0.0147 Life: 269\n",
      "Episode Finish  [15.  0. 24.]\n",
      "Episode: 949 Total reward: 26.099999999999998 Training loss: 0.0236 Epsilon: 0.0146 Life: 269\n",
      "Episode Finish  [12.  4. 24.]\n",
      "Episode: 950 Total reward: 20.5 Training loss: 0.0193 Epsilon: 0.0146 Life: 269\n",
      "Stats Saved at 950\n",
      "Episode Finish  [13.  0. 24.]\n",
      "Episode: 951 Total reward: 22.099999999999998 Training loss: 0.0279 Epsilon: 0.0146 Life: 269\n",
      "Episode Finish  [10.  3. 24.]\n",
      "Episode: 952 Total reward: 16.400000000000002 Training loss: 0.0201 Epsilon: 0.0145 Life: 269\n",
      "Episode Finish  [13.  2. 24.]\n",
      "Episode: 953 Total reward: 22.3 Training loss: 0.0181 Epsilon: 0.0145 Life: 269\n",
      "Episode Finish  [ 8.  7. 14.]\n",
      "Episode: 954 Total reward: 12.699999999999998 Training loss: 0.0144 Epsilon: 0.0145 Life: 269\n",
      "Episode Finish  [9. 0. 8.]\n",
      "Episode: 955 Total reward: 13.5 Training loss: 0.0257 Epsilon: 0.0144 Life: 269\n",
      "Episode Finish  [ 9.  7. 24.]\n",
      "Episode: 956 Total reward: 14.8 Training loss: 0.0270 Epsilon: 0.0144 Life: 269\n",
      "Episode Finish  [11.  0. 12.]\n",
      "Episode: 957 Total reward: 17.8 Training loss: 0.0287 Epsilon: 0.0144 Life: 269\n",
      "Episode Finish  [9. 0. 4.]\n",
      "Episode: 958 Total reward: 13.5 Training loss: 0.0295 Epsilon: 0.0143 Life: 269\n",
      "Episode Finish  [11.  0. 10.]\n",
      "Episode: 959 Total reward: 18.0 Training loss: 0.0198 Epsilon: 0.0143 Life: 269\n",
      "Episode Finish  [7. 0. 8.]\n",
      "Episode: 960 Total reward: 9.8 Training loss: 0.0229 Epsilon: 0.0143 Life: 269\n",
      "Episode Finish  [7. 0. 6.]\n",
      "Episode: 961 Total reward: 9.8 Training loss: 0.0247 Epsilon: 0.0142 Life: 269\n",
      "Episode Finish  [6. 7. 8.]\n",
      "Episode: 962 Total reward: 8.600000000000001 Training loss: 0.0265 Epsilon: 0.0142 Life: 269\n",
      "Episode Finish  [10.  0.  2.]\n",
      "Episode: 963 Total reward: 15.6 Training loss: 0.0235 Epsilon: 0.0142 Life: 269\n",
      "Episode Finish  [11.  0. 14.]\n",
      "Episode: 964 Total reward: 17.999999999999996 Training loss: 0.0281 Epsilon: 0.0141 Life: 269\n",
      "Episode Finish  [10.  0. 28.]\n",
      "Episode: 965 Total reward: 16.0 Training loss: 0.0222 Epsilon: 0.0141 Life: 269\n",
      "Episode Finish  [15.  0. 24.]\n",
      "Episode: 966 Total reward: 26.099999999999998 Training loss: 0.0266 Epsilon: 0.0141 Life: 269\n",
      "Model updated\n",
      "Episode Finish  [12.  0.  6.]\n",
      "Episode: 967 Total reward: 19.599999999999998 Training loss: 0.0203 Epsilon: 0.0140 Life: 269\n",
      "Episode Finish  [11.  0. 14.]\n",
      "Episode: 968 Total reward: 18.0 Training loss: 0.0230 Epsilon: 0.0140 Life: 269\n",
      "Episode Finish  [16.  0. 24.]\n",
      "Episode: 969 Total reward: 28.099999999999998 Training loss: 0.0240 Epsilon: 0.0139 Life: 269\n",
      "Episode Finish  [14.  0. 24.]\n",
      "Episode: 970 Total reward: 24.099999999999998 Training loss: 0.0263 Epsilon: 0.0139 Life: 269\n",
      "Episode Finish  [10.  0. 24.]\n",
      "Episode: 971 Total reward: 16.1 Training loss: 0.0206 Epsilon: 0.0139 Life: 269\n",
      "Episode Finish  [ 8.  1. 20.]\n",
      "Episode: 972 Total reward: 12.100000000000001 Training loss: 0.0291 Epsilon: 0.0138 Life: 269\n",
      "Episode Finish  [11.  0. 10.]\n",
      "Episode: 973 Total reward: 18.0 Training loss: 0.0362 Epsilon: 0.0138 Life: 269\n",
      "Episode Finish  [10.  5.  4.]\n",
      "Episode: 974 Total reward: 16.0 Training loss: 0.0225 Epsilon: 0.0138 Life: 269\n",
      "Episode Finish  [ 5. 12.  2.]\n",
      "Episode: 975 Total reward: 6.9 Training loss: 0.0337 Epsilon: 0.0138 Life: 269\n",
      "Episode Finish  [6. 9. 8.]\n",
      "Episode: 976 Total reward: 8.8 Training loss: 0.0204 Epsilon: 0.0137 Life: 269\n",
      "Episode Finish  [15.  0.  4.]\n",
      "Episode: 977 Total reward: 25.9 Training loss: 0.0305 Epsilon: 0.0137 Life: 269\n",
      "Episode Finish  [10.  2. 24.]\n",
      "Episode: 978 Total reward: 16.3 Training loss: 0.0199 Epsilon: 0.0137 Life: 269\n",
      "Episode Finish  [11.  2.  4.]\n",
      "Episode: 979 Total reward: 18.8 Training loss: 0.0296 Epsilon: 0.0136 Life: 269\n",
      "Episode Finish  [13.  0.  2.]\n",
      "Episode: 980 Total reward: 21.699999999999996 Training loss: 0.0313 Epsilon: 0.0136 Life: 269\n",
      "Episode Finish  [11.  0. 18.]\n",
      "Episode: 981 Total reward: 17.9 Training loss: 0.0185 Epsilon: 0.0135 Life: 269\n",
      "Episode Finish  [11.  0. 10.]\n",
      "Episode: 982 Total reward: 18.0 Training loss: 0.0254 Epsilon: 0.0135 Life: 269\n",
      "Episode Finish  [9. 7. 4.]\n",
      "Episode: 983 Total reward: 14.2 Training loss: 0.0410 Epsilon: 0.0135 Life: 269\n",
      "Episode Finish  [ 6. 11. 10.]\n",
      "Episode: 984 Total reward: 10.099999999999998 Training loss: 0.0249 Epsilon: 0.0135 Life: 269\n",
      "Episode Finish  [12.  0. 14.]\n",
      "Episode: 985 Total reward: 20.0 Training loss: 0.0247 Epsilon: 0.0134 Life: 269\n",
      "Episode Finish  [11.  0. 24.]\n",
      "Episode: 986 Total reward: 18.1 Training loss: 0.0272 Epsilon: 0.0134 Life: 269\n",
      "Episode Finish  [ 7.  9. 32.]\n",
      "Episode: 987 Total reward: 10.7 Training loss: 0.0221 Epsilon: 0.0134 Life: 269\n",
      "Episode Finish  [8. 6. 8.]\n",
      "Episode: 988 Total reward: 12.4 Training loss: 0.0233 Epsilon: 0.0133 Life: 269\n",
      "Episode Finish  [11.  0. 10.]\n",
      "Episode: 989 Total reward: 18.0 Training loss: 0.0224 Epsilon: 0.0133 Life: 269\n",
      "Episode Finish  [6. 7. 6.]\n",
      "Episode: 990 Total reward: 8.4 Training loss: 0.0163 Epsilon: 0.0133 Life: 269\n",
      "Episode Finish  [12.  0. 10.]\n",
      "Episode: 991 Total reward: 20.0 Training loss: 0.0297 Epsilon: 0.0133 Life: 269\n",
      "Episode Finish  [ 7.  4. 16.]\n",
      "Episode: 992 Total reward: 10.5 Training loss: 0.0223 Epsilon: 0.0132 Life: 269\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode Finish  [12.  0. 24.]\n",
      "Episode: 993 Total reward: 20.099999999999998 Training loss: 0.0190 Epsilon: 0.0132 Life: 269\n",
      "Episode Finish  [ 5. 12.  8.]\n",
      "Episode: 994 Total reward: 7.1 Training loss: 0.0310 Epsilon: 0.0132 Life: 269\n",
      "Episode Finish  [14.  0. 24.]\n",
      "Episode: 995 Total reward: 24.1 Training loss: 0.0236 Epsilon: 0.0132 Life: 269\n",
      "Episode Finish  [14.  0.  4.]\n",
      "Episode: 996 Total reward: 23.799999999999997 Training loss: 0.0226 Epsilon: 0.0131 Life: 269\n",
      "Episode Finish  [10.  2. 24.]\n",
      "Episode: 997 Total reward: 16.3 Training loss: 0.0263 Epsilon: 0.0131 Life: 269\n",
      "Episode Finish  [12.  3. 24.]\n",
      "Episode: 998 Total reward: 20.4 Training loss: 0.0231 Epsilon: 0.0131 Life: 269\n",
      "Episode Finish  [10.  0. 10.]\n",
      "Episode: 999 Total reward: 15.899999999999999 Training loss: 0.0319 Epsilon: 0.0131 Life: 269\n",
      "Episode Finish  [5. 6. 8.]\n",
      "Episode: 1000 Total reward: 6.399999999999999 Training loss: 0.0227 Epsilon: 0.0130 Life: 269\n",
      "Model Saved at 1000\n",
      "Stats Saved at 1000\n",
      "Episode Finish  [ 8.  9. 24.]\n",
      "Episode: 1001 Total reward: 12.999999999999998 Training loss: 0.0244 Epsilon: 0.0130 Life: 269\n",
      "Episode Finish  [13.  3. 10.]\n",
      "Episode: 1002 Total reward: 22.3 Training loss: 0.0274 Epsilon: 0.0130 Life: 269\n",
      "Episode Finish  [ 8.  7. 24.]\n",
      "Episode: 1003 Total reward: 12.8 Training loss: 0.0338 Epsilon: 0.0130 Life: 269\n",
      "Episode Finish  [ 8. 10. 12.]\n",
      "Episode: 1004 Total reward: 13.1 Training loss: 0.0221 Epsilon: 0.0130 Life: 269\n",
      "Episode Finish  [ 8.  2. 24.]\n",
      "Episode: 1005 Total reward: 13.299999999999999 Training loss: 0.0236 Epsilon: 0.0129 Life: 269\n",
      "Episode Finish  [10.  0. 24.]\n",
      "Episode: 1006 Total reward: 15.6 Training loss: 0.0158 Epsilon: 0.0129 Life: 269\n",
      "Episode Finish  [10.  7.  4.]\n",
      "Episode: 1007 Total reward: 16.5 Training loss: 0.0250 Epsilon: 0.0129 Life: 269\n",
      "Episode Finish  [11.  0.  4.]\n",
      "Episode: 1008 Total reward: 17.8 Training loss: 0.0157 Epsilon: 0.0129 Life: 269\n",
      "Episode Finish  [13.  0. 10.]\n",
      "Episode: 1009 Total reward: 22.0 Training loss: 0.0207 Epsilon: 0.0128 Life: 269\n",
      "Episode Finish  [ 7.  5. 32.]\n",
      "Episode: 1010 Total reward: 10.3 Training loss: 0.0202 Epsilon: 0.0128 Life: 269\n",
      "Episode Finish  [12.  0.  2.]\n",
      "Episode: 1011 Total reward: 19.5 Training loss: 0.0270 Epsilon: 0.0128 Life: 269\n",
      "Episode Finish  [8. 4. 4.]\n",
      "Episode: 1012 Total reward: 12.4 Training loss: 0.0222 Epsilon: 0.0128 Life: 269\n",
      "Episode Finish  [14.  0.  6.]\n",
      "Episode: 1013 Total reward: 23.700000000000003 Training loss: 0.0225 Epsilon: 0.0128 Life: 269\n",
      "Episode Finish  [ 5. 11.  8.]\n",
      "Episode: 1014 Total reward: 6.8999999999999995 Training loss: 0.0240 Epsilon: 0.0127 Life: 269\n",
      "Episode Finish  [11.  1. 20.]\n",
      "Episode: 1015 Total reward: 19.2 Training loss: 0.0236 Epsilon: 0.0127 Life: 269\n",
      "Episode Finish  [ 6. 10.  8.]\n",
      "Episode: 1016 Total reward: 8.8 Training loss: 0.0184 Epsilon: 0.0127 Life: 269\n",
      "Episode Finish  [10.  0. 32.]\n",
      "Episode: 1017 Total reward: 16.1 Training loss: 0.0242 Epsilon: 0.0127 Life: 269\n",
      "Episode Finish  [16.  0.  4.]\n",
      "Episode: 1018 Total reward: 27.799999999999997 Training loss: 0.0194 Epsilon: 0.0126 Life: 269\n",
      "Episode Finish  [10.  1.  8.]\n",
      "Episode: 1019 Total reward: 15.9 Training loss: 0.0245 Epsilon: 0.0126 Life: 269\n",
      "Episode Finish  [10.  0.  4.]\n",
      "Episode: 1020 Total reward: 15.899999999999999 Training loss: 0.0181 Epsilon: 0.0126 Life: 269\n",
      "Episode Finish  [8. 6. 4.]\n",
      "Episode: 1021 Total reward: 13.5 Training loss: 0.0237 Epsilon: 0.0126 Life: 269\n",
      "Episode Finish  [9. 7. 4.]\n",
      "Episode: 1022 Total reward: 14.5 Training loss: 0.0251 Epsilon: 0.0126 Life: 269\n",
      "Episode Finish  [11.  8. 24.]\n",
      "Episode: 1023 Total reward: 18.9 Training loss: 0.0248 Epsilon: 0.0125 Life: 269\n",
      "Episode Finish  [ 8.  3. 16.]\n",
      "Episode: 1024 Total reward: 13.0 Training loss: 0.0268 Epsilon: 0.0125 Life: 269\n",
      "Episode Finish  [12.  1. 24.]\n",
      "Episode: 1025 Total reward: 20.199999999999996 Training loss: 0.0243 Epsilon: 0.0125 Life: 269\n",
      "Episode Finish  [10.  6.  8.]\n",
      "Episode: 1026 Total reward: 16.2 Training loss: 0.0211 Epsilon: 0.0125 Life: 269\n",
      "Episode Finish  [11.  2. 26.]\n",
      "Episode: 1027 Total reward: 17.9 Training loss: 0.0245 Epsilon: 0.0125 Life: 269\n",
      "Model updated\n",
      "Episode Finish  [12.  0. 10.]\n",
      "Episode: 1028 Total reward: 20.0 Training loss: 0.0268 Epsilon: 0.0124 Life: 269\n",
      "Episode Finish  [14.  0. 10.]\n",
      "Episode: 1029 Total reward: 23.700000000000003 Training loss: 0.0205 Epsilon: 0.0124 Life: 269\n",
      "Episode Finish  [15.  0. 20.]\n",
      "Episode: 1030 Total reward: 25.9 Training loss: 0.0477 Epsilon: 0.0124 Life: 269\n",
      "Episode Finish  [15.  0.  4.]\n",
      "Episode: 1031 Total reward: 25.900000000000002 Training loss: 0.0341 Epsilon: 0.0124 Life: 269\n",
      "Episode Finish  [11.  0.  4.]\n",
      "Episode: 1032 Total reward: 17.8 Training loss: 0.0303 Epsilon: 0.0123 Life: 269\n",
      "Episode Finish  [ 8.  9. 24.]\n",
      "Episode: 1033 Total reward: 13.0 Training loss: 0.0302 Epsilon: 0.0123 Life: 269\n",
      "Episode Finish  [10.  0. 18.]\n",
      "Episode: 1034 Total reward: 15.9 Training loss: 0.0194 Epsilon: 0.0123 Life: 269\n",
      "Episode Finish  [10.  0.  4.]\n",
      "Episode: 1035 Total reward: 15.5 Training loss: 0.0229 Epsilon: 0.0123 Life: 269\n",
      "Episode Finish  [12.  1. 10.]\n",
      "Episode: 1036 Total reward: 20.7 Training loss: 0.0257 Epsilon: 0.0123 Life: 269\n",
      "Episode Finish  [ 5. 19. 24.]\n",
      "Episode: 1037 Total reward: 8.0 Training loss: 0.0244 Epsilon: 0.0123 Life: 269\n",
      "Episode Finish  [9. 6. 2.]\n",
      "Episode: 1038 Total reward: 14.299999999999999 Training loss: 0.0396 Epsilon: 0.0122 Life: 269\n",
      "Episode Finish  [14.  0. 24.]\n",
      "Episode: 1039 Total reward: 24.1 Training loss: 0.0242 Epsilon: 0.0122 Life: 269\n",
      "Episode Finish  [14.  0. 24.]\n",
      "Episode: 1040 Total reward: 24.1 Training loss: 0.0275 Epsilon: 0.0122 Life: 269\n",
      "Episode Finish  [10.  0. 14.]\n",
      "Episode: 1041 Total reward: 15.999999999999998 Training loss: 0.0210 Epsilon: 0.0122 Life: 269\n",
      "Episode Finish  [14.  0. 24.]\n",
      "Episode: 1042 Total reward: 24.1 Training loss: 0.0243 Epsilon: 0.0122 Life: 269\n",
      "Episode Finish  [12.  0.  6.]\n",
      "Episode: 1043 Total reward: 19.799999999999997 Training loss: 0.0259 Epsilon: 0.0121 Life: 269\n",
      "Episode Finish  [11.  0. 24.]\n",
      "Episode: 1044 Total reward: 18.1 Training loss: 0.0501 Epsilon: 0.0121 Life: 269\n",
      "Episode Finish  [12.  1. 10.]\n",
      "Episode: 1045 Total reward: 20.1 Training loss: 0.0262 Epsilon: 0.0121 Life: 269\n",
      "Episode Finish  [ 8.  5. 20.]\n",
      "Episode: 1046 Total reward: 12.6 Training loss: 0.0278 Epsilon: 0.0121 Life: 269\n",
      "Episode Finish  [ 9.  7. 24.]\n",
      "Episode: 1047 Total reward: 14.799999999999999 Training loss: 0.0252 Epsilon: 0.0121 Life: 269\n",
      "Episode Finish  [8. 8. 8.]\n",
      "Episode: 1048 Total reward: 12.600000000000001 Training loss: 0.0253 Epsilon: 0.0121 Life: 269\n",
      "Episode Finish  [10.  0.  4.]\n",
      "Episode: 1049 Total reward: 15.999999999999998 Training loss: 0.0202 Epsilon: 0.0120 Life: 269\n",
      "Episode Finish  [11.  0. 24.]\n",
      "Episode: 1050 Total reward: 18.1 Training loss: 0.0324 Epsilon: 0.0120 Life: 269\n",
      "Stats Saved at 1050\n",
      "Episode Finish  [10.  0. 10.]\n",
      "Episode: 1051 Total reward: 16.0 Training loss: 0.0216 Epsilon: 0.0120 Life: 269\n",
      "Episode Finish  [11.  0.  8.]\n",
      "Episode: 1052 Total reward: 17.8 Training loss: 0.0193 Epsilon: 0.0120 Life: 269\n",
      "Episode Finish  [10.  0. 10.]\n",
      "Episode: 1053 Total reward: 16.0 Training loss: 0.0440 Epsilon: 0.0120 Life: 269\n",
      "Episode Finish  [10.  0. 14.]\n",
      "Episode: 1054 Total reward: 16.0 Training loss: 0.0215 Epsilon: 0.0119 Life: 269\n",
      "Episode Finish  [15.  0.  4.]\n",
      "Episode: 1055 Total reward: 25.799999999999997 Training loss: 0.0302 Epsilon: 0.0119 Life: 269\n",
      "Episode Finish  [11.  3.  6.]\n",
      "Episode: 1056 Total reward: 17.9 Training loss: 0.0263 Epsilon: 0.0119 Life: 269\n",
      "Episode Finish  [10.  0. 24.]\n",
      "Episode: 1057 Total reward: 15.8 Training loss: 0.0264 Epsilon: 0.0119 Life: 269\n",
      "Episode Finish  [ 8.  2. 10.]\n",
      "Episode: 1058 Total reward: 11.8 Training loss: 0.0273 Epsilon: 0.0119 Life: 269\n",
      "Episode Finish  [ 9.  3. 20.]\n",
      "Episode: 1059 Total reward: 14.399999999999999 Training loss: 0.0372 Epsilon: 0.0119 Life: 269\n",
      "Episode Finish  [8. 4. 6.]\n",
      "Episode: 1060 Total reward: 12.099999999999998 Training loss: 0.0323 Epsilon: 0.0119 Life: 269\n",
      "Episode Finish  [11.  0. 40.]\n",
      "Episode: 1061 Total reward: 17.9 Training loss: 0.0241 Epsilon: 0.0118 Life: 269\n",
      "Episode Finish  [13.  0. 32.]\n",
      "Episode: 1062 Total reward: 22.0 Training loss: 0.0284 Epsilon: 0.0118 Life: 269\n",
      "Episode Finish  [12.  0. 10.]\n",
      "Episode: 1063 Total reward: 20.0 Training loss: 0.0261 Epsilon: 0.0118 Life: 269\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode Finish  [9. 4. 4.]\n",
      "Episode: 1064 Total reward: 14.2 Training loss: 0.0199 Epsilon: 0.0118 Life: 269\n",
      "Episode Finish  [10.  4. 24.]\n",
      "Episode: 1065 Total reward: 16.4 Training loss: 0.0264 Epsilon: 0.0118 Life: 269\n",
      "Episode Finish  [ 9. 13. 24.]\n",
      "Episode: 1066 Total reward: 16.4 Training loss: 0.0232 Epsilon: 0.0118 Life: 269\n",
      "Episode Finish  [11.  4. 24.]\n",
      "Episode: 1067 Total reward: 18.5 Training loss: 0.0154 Epsilon: 0.0118 Life: 269\n",
      "Episode Finish  [8. 6. 8.]\n",
      "Episode: 1068 Total reward: 12.399999999999999 Training loss: 0.0260 Epsilon: 0.0117 Life: 269\n",
      "Episode Finish  [ 4. 17. 24.]\n",
      "Episode: 1069 Total reward: 5.8 Training loss: 0.0300 Epsilon: 0.0117 Life: 269\n",
      "Episode Finish  [ 7. 10. 24.]\n",
      "Episode: 1070 Total reward: 11.1 Training loss: 0.0261 Epsilon: 0.0117 Life: 269\n",
      "Episode Finish  [ 5. 11.  8.]\n",
      "Episode: 1071 Total reward: 6.700000000000001 Training loss: 0.0255 Epsilon: 0.0117 Life: 269\n",
      "Episode Finish  [10.  0. 14.]\n",
      "Episode: 1072 Total reward: 16.0 Training loss: 0.0242 Epsilon: 0.0117 Life: 269\n",
      "Episode Finish  [ 7. 13.  8.]\n",
      "Episode: 1073 Total reward: 11.299999999999999 Training loss: 0.0224 Epsilon: 0.0117 Life: 269\n",
      "Episode Finish  [12.  3. 24.]\n",
      "Episode: 1074 Total reward: 20.300000000000004 Training loss: 0.0192 Epsilon: 0.0117 Life: 269\n",
      "Episode Finish  [11.  0. 14.]\n",
      "Episode: 1075 Total reward: 18.0 Training loss: 0.0248 Epsilon: 0.0117 Life: 269\n",
      "Episode Finish  [11.  0. 12.]\n",
      "Episode: 1076 Total reward: 17.7 Training loss: 0.0298 Epsilon: 0.0117 Life: 269\n",
      "Episode Finish  [10.  1. 24.]\n",
      "Episode: 1077 Total reward: 15.600000000000001 Training loss: 0.0344 Epsilon: 0.0116 Life: 269\n",
      "Episode Finish  [11.  0. 20.]\n",
      "Episode: 1078 Total reward: 17.9 Training loss: 0.0341 Epsilon: 0.0116 Life: 269\n",
      "Episode Finish  [ 7.  8. 24.]\n",
      "Episode: 1079 Total reward: 10.9 Training loss: 0.0267 Epsilon: 0.0116 Life: 269\n",
      "Episode Finish  [11.  0.  4.]\n",
      "Episode: 1080 Total reward: 17.800000000000004 Training loss: 0.0321 Epsilon: 0.0116 Life: 269\n",
      "Episode Finish  [11.  5.  4.]\n",
      "Episode: 1081 Total reward: 18.3 Training loss: 0.0354 Epsilon: 0.0116 Life: 269\n",
      "Episode Finish  [ 8.  6. 18.]\n",
      "Episode: 1082 Total reward: 12.5 Training loss: 0.0268 Epsilon: 0.0116 Life: 269\n",
      "Episode Finish  [ 9.  8. 24.]\n",
      "Episode: 1083 Total reward: 14.9 Training loss: 0.0313 Epsilon: 0.0116 Life: 269\n",
      "Episode Finish  [ 8.  5. 24.]\n",
      "Episode: 1084 Total reward: 12.6 Training loss: 0.0337 Epsilon: 0.0116 Life: 269\n",
      "Episode Finish  [11.  0. 10.]\n",
      "Episode: 1085 Total reward: 18.0 Training loss: 0.0203 Epsilon: 0.0115 Life: 269\n",
      "Episode Finish  [ 7.  4. 24.]\n",
      "Episode: 1086 Total reward: 10.5 Training loss: 0.0209 Epsilon: 0.0115 Life: 269\n",
      "Episode Finish  [ 9.  8. 24.]\n",
      "Episode: 1087 Total reward: 14.9 Training loss: 0.0206 Epsilon: 0.0115 Life: 269\n",
      "Episode Finish  [12.  0. 12.]\n",
      "Episode: 1088 Total reward: 19.799999999999997 Training loss: 0.0395 Epsilon: 0.0115 Life: 269\n",
      "Episode Finish  [6. 8. 4.]\n",
      "Episode: 1089 Total reward: 8.3 Training loss: 0.0240 Epsilon: 0.0115 Life: 269\n",
      "Episode Finish  [ 5. 12.  4.]\n",
      "Episode: 1090 Total reward: 7.1 Training loss: 0.0244 Epsilon: 0.0115 Life: 269\n",
      "Model updated\n",
      "Episode Finish  [12.  0. 24.]\n",
      "Episode: 1091 Total reward: 20.1 Training loss: 0.0260 Epsilon: 0.0115 Life: 269\n",
      "Episode Finish  [15.  5.  8.]\n",
      "Episode: 1092 Total reward: 26.3 Training loss: 0.0226 Epsilon: 0.0115 Life: 269\n",
      "Episode Finish  [ 6. 10. 12.]\n",
      "Episode: 1093 Total reward: 8.799999999999999 Training loss: 0.0341 Epsilon: 0.0115 Life: 269\n",
      "Episode Finish  [11.  3. 24.]\n",
      "Episode: 1094 Total reward: 17.9 Training loss: 0.0192 Epsilon: 0.0114 Life: 269\n",
      "Episode Finish  [10.  0.  2.]\n",
      "Episode: 1095 Total reward: 15.899999999999999 Training loss: 0.0207 Epsilon: 0.0114 Life: 269\n",
      "Episode Finish  [8. 2. 8.]\n",
      "Episode: 1096 Total reward: 11.8 Training loss: 0.0317 Epsilon: 0.0114 Life: 269\n",
      "Episode Finish  [ 5. 17. 24.]\n",
      "Episode: 1097 Total reward: 7.800000000000001 Training loss: 0.0262 Epsilon: 0.0114 Life: 269\n",
      "Episode Finish  [10.  0. 32.]\n",
      "Episode: 1098 Total reward: 15.799999999999999 Training loss: 0.0182 Epsilon: 0.0114 Life: 269\n",
      "Episode Finish  [12.  0. 24.]\n",
      "Episode: 1099 Total reward: 20.099999999999998 Training loss: 0.0277 Epsilon: 0.0114 Life: 269\n",
      "Episode Finish  [ 3. 20. 24.]\n",
      "Episode: 1100 Total reward: 4.1000000000000005 Training loss: 0.0388 Epsilon: 0.0114 Life: 269\n",
      "Stats Saved at 1100\n",
      "Episode Finish  [14.  0.  4.]\n",
      "Episode: 1101 Total reward: 23.9 Training loss: 0.0227 Epsilon: 0.0114 Life: 269\n",
      "Episode Finish  [11.  0. 28.]\n",
      "Episode: 1102 Total reward: 17.7 Training loss: 0.0193 Epsilon: 0.0114 Life: 269\n",
      "Episode Finish  [12.  3.  4.]\n",
      "Episode: 1103 Total reward: 19.8 Training loss: 0.0147 Epsilon: 0.0113 Life: 269\n",
      "Episode Finish  [ 9.  4. 12.]\n",
      "Episode: 1104 Total reward: 13.700000000000001 Training loss: 0.0243 Epsilon: 0.0113 Life: 269\n",
      "Episode Finish  [9. 9. 4.]\n",
      "Episode: 1105 Total reward: 14.700000000000001 Training loss: 0.0183 Epsilon: 0.0113 Life: 269\n",
      "Episode Finish  [12.  0. 18.]\n",
      "Episode: 1106 Total reward: 19.500000000000004 Training loss: 0.0235 Epsilon: 0.0113 Life: 269\n",
      "Episode Finish  [9. 2. 8.]\n",
      "Episode: 1107 Total reward: 13.999999999999998 Training loss: 0.0208 Epsilon: 0.0113 Life: 269\n",
      "Episode Finish  [ 7. 10. 24.]\n",
      "Episode: 1108 Total reward: 11.100000000000001 Training loss: 0.0396 Epsilon: 0.0113 Life: 269\n",
      "Episode Finish  [ 9.  8. 10.]\n",
      "Episode: 1109 Total reward: 14.9 Training loss: 0.0318 Epsilon: 0.0113 Life: 269\n",
      "Episode Finish  [12.  4. 24.]\n",
      "Episode: 1110 Total reward: 20.5 Training loss: 0.0271 Epsilon: 0.0113 Life: 269\n",
      "Episode Finish  [13.  2. 24.]\n",
      "Episode: 1111 Total reward: 22.299999999999997 Training loss: 0.0298 Epsilon: 0.0113 Life: 269\n",
      "Episode Finish  [15.  0.  8.]\n",
      "Episode: 1112 Total reward: 26.0 Training loss: 0.0263 Epsilon: 0.0112 Life: 269\n",
      "Episode Finish  [14.  0.  8.]\n",
      "Episode: 1113 Total reward: 23.900000000000002 Training loss: 0.0217 Epsilon: 0.0112 Life: 269\n",
      "Episode Finish  [14.  2. 14.]\n",
      "Episode: 1114 Total reward: 24.200000000000003 Training loss: 0.0228 Epsilon: 0.0112 Life: 269\n",
      "Episode Finish  [11.  9.  6.]\n",
      "Episode: 1115 Total reward: 18.4 Training loss: 0.0218 Epsilon: 0.0112 Life: 269\n",
      "Episode Finish  [ 5. 15. 24.]\n",
      "Episode: 1116 Total reward: 7.5 Training loss: 0.0288 Epsilon: 0.0112 Life: 269\n",
      "Episode Finish  [ 6.  4. 24.]\n",
      "Episode: 1117 Total reward: 8.5 Training loss: 0.0398 Epsilon: 0.0112 Life: 269\n",
      "Episode Finish  [15.  0. 30.]\n",
      "Episode: 1118 Total reward: 26.1 Training loss: 0.0248 Epsilon: 0.0112 Life: 269\n",
      "Episode Finish  [ 9.  0. 10.]\n",
      "Episode: 1119 Total reward: 14.0 Training loss: 0.0237 Epsilon: 0.0112 Life: 269\n",
      "Episode Finish  [10.  6. 24.]\n",
      "Episode: 1120 Total reward: 17.7 Training loss: 0.0248 Epsilon: 0.0112 Life: 269\n",
      "Episode Finish  [11.  1. 14.]\n",
      "Episode: 1121 Total reward: 18.1 Training loss: 0.0212 Epsilon: 0.0112 Life: 269\n",
      "Episode Finish  [14.  0.  4.]\n",
      "Episode: 1122 Total reward: 23.5 Training loss: 0.0211 Epsilon: 0.0111 Life: 269\n",
      "Episode Finish  [11.  5. 10.]\n",
      "Episode: 1123 Total reward: 18.5 Training loss: 0.0290 Epsilon: 0.0111 Life: 269\n",
      "Episode Finish  [13.  0. 12.]\n",
      "Episode: 1124 Total reward: 21.9 Training loss: 0.0199 Epsilon: 0.0111 Life: 269\n",
      "Episode Finish  [13.  0. 24.]\n",
      "Episode: 1125 Total reward: 22.1 Training loss: 0.0214 Epsilon: 0.0111 Life: 269\n",
      "Episode Finish  [13.  0. 12.]\n",
      "Episode: 1126 Total reward: 22.0 Training loss: 0.0177 Epsilon: 0.0111 Life: 269\n",
      "Episode Finish  [11.  0. 24.]\n",
      "Episode: 1127 Total reward: 18.1 Training loss: 0.0194 Epsilon: 0.0111 Life: 269\n",
      "Episode Finish  [9. 0. 8.]\n",
      "Episode: 1128 Total reward: 14.0 Training loss: 0.0307 Epsilon: 0.0111 Life: 269\n",
      "Episode Finish  [8. 0. 2.]\n",
      "Episode: 1129 Total reward: 11.399999999999999 Training loss: 0.0271 Epsilon: 0.0111 Life: 269\n",
      "Episode Finish  [12.  6. 10.]\n",
      "Episode: 1130 Total reward: 20.599999999999998 Training loss: 0.0241 Epsilon: 0.0111 Life: 269\n",
      "Episode Finish  [12.  0. 24.]\n",
      "Episode: 1131 Total reward: 20.099999999999998 Training loss: 0.0220 Epsilon: 0.0111 Life: 269\n",
      "Episode Finish  [12.  0. 12.]\n",
      "Episode: 1132 Total reward: 19.5 Training loss: 0.0198 Epsilon: 0.0111 Life: 269\n",
      "Episode Finish  [ 3. 17.  4.]\n",
      "Episode: 1133 Total reward: 3.8 Training loss: 0.0300 Epsilon: 0.0110 Life: 269\n",
      "Episode Finish  [10.  0.  8.]\n",
      "Episode: 1134 Total reward: 15.8 Training loss: 0.0204 Epsilon: 0.0110 Life: 269\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode Finish  [11.  1. 24.]\n",
      "Episode: 1135 Total reward: 18.2 Training loss: 0.0268 Epsilon: 0.0110 Life: 269\n",
      "Episode Finish  [12.  2. 22.]\n",
      "Episode: 1136 Total reward: 20.0 Training loss: 0.0205 Epsilon: 0.0110 Life: 269\n",
      "Episode Finish  [12.  1.  6.]\n",
      "Episode: 1137 Total reward: 20.0 Training loss: 0.0380 Epsilon: 0.0110 Life: 269\n",
      "Episode Finish  [12.  4.  8.]\n",
      "Episode: 1138 Total reward: 20.200000000000003 Training loss: 0.0264 Epsilon: 0.0110 Life: 269\n",
      "Episode Finish  [14.  0. 28.]\n",
      "Episode: 1139 Total reward: 24.0 Training loss: 0.0238 Epsilon: 0.0110 Life: 269\n",
      "Episode Finish  [ 6. 18. 24.]\n",
      "Episode: 1140 Total reward: 9.9 Training loss: 0.0151 Epsilon: 0.0110 Life: 269\n",
      "Episode Finish  [ 8. 13. 32.]\n",
      "Episode: 1141 Total reward: 13.5 Training loss: 0.0238 Epsilon: 0.0110 Life: 269\n",
      "Episode Finish  [ 9.  4. 16.]\n",
      "Episode: 1142 Total reward: 14.5 Training loss: 0.0402 Epsilon: 0.0110 Life: 269\n",
      "Episode Finish  [ 8.  6. 16.]\n",
      "Episode: 1143 Total reward: 12.7 Training loss: 0.0202 Epsilon: 0.0110 Life: 269\n",
      "Episode Finish  [ 6. 11. 24.]\n",
      "Episode: 1144 Total reward: 9.2 Training loss: 0.0277 Epsilon: 0.0110 Life: 269\n",
      "Episode Finish  [ 8. 10. 24.]\n",
      "Episode: 1145 Total reward: 13.100000000000001 Training loss: 0.0263 Epsilon: 0.0110 Life: 269\n",
      "Episode Finish  [15.  0.  4.]\n",
      "Episode: 1146 Total reward: 25.8 Training loss: 0.0200 Epsilon: 0.0109 Life: 269\n",
      "Episode Finish  [11.  0.  8.]\n",
      "Episode: 1147 Total reward: 17.9 Training loss: 0.0328 Epsilon: 0.0109 Life: 269\n",
      "Episode Finish  [10.  4.  4.]\n",
      "Episode: 1148 Total reward: 16.200000000000003 Training loss: 0.0292 Epsilon: 0.0109 Life: 269\n",
      "Episode Finish  [ 5. 13. 24.]\n",
      "Episode: 1149 Total reward: 7.399999999999999 Training loss: 0.0284 Epsilon: 0.0109 Life: 269\n",
      "Episode Finish  [10.  6.  4.]\n",
      "Episode: 1150 Total reward: 16.1 Training loss: 0.0373 Epsilon: 0.0109 Life: 269\n",
      "Stats Saved at 1150\n",
      "Episode Finish  [11.  2. 24.]\n",
      "Episode: 1151 Total reward: 18.3 Training loss: 0.0269 Epsilon: 0.0109 Life: 269\n",
      "Episode Finish  [ 6. 17. 20.]\n",
      "Episode: 1152 Total reward: 9.6 Training loss: 0.0253 Epsilon: 0.0109 Life: 269\n",
      "Model updated\n",
      "Episode Finish  [ 7.  9. 24.]\n",
      "Episode: 1153 Total reward: 10.999999999999998 Training loss: 0.0222 Epsilon: 0.0109 Life: 269\n",
      "Episode Finish  [13.  0.  4.]\n",
      "Episode: 1154 Total reward: 21.9 Training loss: 0.0280 Epsilon: 0.0109 Life: 269\n",
      "Episode Finish  [ 5. 12.  4.]\n",
      "Episode: 1155 Total reward: 7.000000000000002 Training loss: 0.0253 Epsilon: 0.0109 Life: 269\n",
      "Episode Finish  [15.  0. 12.]\n",
      "Episode: 1156 Total reward: 26.1 Training loss: 0.0241 Epsilon: 0.0109 Life: 269\n",
      "Episode Finish  [ 7. 14. 24.]\n",
      "Episode: 1157 Total reward: 11.5 Training loss: 0.0231 Epsilon: 0.0109 Life: 269\n",
      "Episode Finish  [13.  0.  4.]\n",
      "Episode: 1158 Total reward: 21.8 Training loss: 0.0291 Epsilon: 0.0109 Life: 269\n",
      "Episode Finish  [9. 0. 8.]\n",
      "Episode: 1159 Total reward: 13.899999999999999 Training loss: 0.0242 Epsilon: 0.0109 Life: 269\n",
      "Episode Finish  [16.  0.  4.]\n",
      "Episode: 1160 Total reward: 27.8 Training loss: 0.0334 Epsilon: 0.0108 Life: 269\n",
      "Episode Finish  [13.  0. 10.]\n",
      "Episode: 1161 Total reward: 22.0 Training loss: 0.0250 Epsilon: 0.0108 Life: 269\n",
      "Episode Finish  [7. 0. 8.]\n",
      "Episode: 1162 Total reward: 10.099999999999998 Training loss: 0.0208 Epsilon: 0.0108 Life: 269\n",
      "Episode Finish  [11.  0. 12.]\n",
      "Episode: 1163 Total reward: 17.9 Training loss: 0.0257 Epsilon: 0.0108 Life: 269\n",
      "Episode Finish  [ 5. 15.  4.]\n",
      "Episode: 1164 Total reward: 7.5 Training loss: 0.0243 Epsilon: 0.0108 Life: 269\n",
      "Episode Finish  [14.  0. 24.]\n",
      "Episode: 1165 Total reward: 24.1 Training loss: 0.0176 Epsilon: 0.0108 Life: 269\n",
      "Episode Finish  [10.  0. 14.]\n",
      "Episode: 1166 Total reward: 16.0 Training loss: 0.0237 Epsilon: 0.0108 Life: 269\n",
      "Episode Finish  [12.  0. 22.]\n",
      "Episode: 1167 Total reward: 20.0 Training loss: 0.0250 Epsilon: 0.0108 Life: 269\n",
      "Episode Finish  [13.  0.  8.]\n",
      "Episode: 1168 Total reward: 21.8 Training loss: 0.0272 Epsilon: 0.0108 Life: 269\n",
      "Episode Finish  [11.  4. 24.]\n",
      "Episode: 1169 Total reward: 19.5 Training loss: 0.0238 Epsilon: 0.0108 Life: 269\n",
      "Episode Finish  [12.  0.  2.]\n",
      "Episode: 1170 Total reward: 19.6 Training loss: 0.0396 Epsilon: 0.0108 Life: 269\n",
      "Episode Finish  [12.  0.  4.]\n",
      "Episode: 1171 Total reward: 19.9 Training loss: 0.0212 Epsilon: 0.0108 Life: 269\n",
      "Episode Finish  [15.  0.  4.]\n",
      "Episode: 1172 Total reward: 25.8 Training loss: 0.0423 Epsilon: 0.0108 Life: 269\n",
      "Episode Finish  [13.  0. 10.]\n",
      "Episode: 1173 Total reward: 22.0 Training loss: 0.0353 Epsilon: 0.0108 Life: 269\n",
      "Episode Finish  [10.  6. 10.]\n",
      "Episode: 1174 Total reward: 16.599999999999998 Training loss: 0.0309 Epsilon: 0.0107 Life: 269\n",
      "Episode Finish  [14.  0. 12.]\n",
      "Episode: 1175 Total reward: 24.0 Training loss: 0.0185 Epsilon: 0.0107 Life: 269\n",
      "Episode Finish  [ 8. 10.  8.]\n",
      "Episode: 1176 Total reward: 12.799999999999999 Training loss: 0.0292 Epsilon: 0.0107 Life: 269\n",
      "Episode Finish  [10.  2.  4.]\n",
      "Episode: 1177 Total reward: 16.0 Training loss: 0.0264 Epsilon: 0.0107 Life: 269\n",
      "Episode Finish  [10.  0. 20.]\n",
      "Episode: 1178 Total reward: 15.999999999999998 Training loss: 0.0309 Epsilon: 0.0107 Life: 269\n",
      "Episode Finish  [15.  0. 14.]\n",
      "Episode: 1179 Total reward: 26.0 Training loss: 0.0290 Epsilon: 0.0107 Life: 269\n",
      "Episode Finish  [13.  0.  4.]\n",
      "Episode: 1180 Total reward: 21.799999999999997 Training loss: 0.0295 Epsilon: 0.0107 Life: 269\n",
      "Episode Finish  [11.  0.  4.]\n",
      "Episode: 1181 Total reward: 18.099999999999998 Training loss: 0.0257 Epsilon: 0.0107 Life: 269\n",
      "Episode Finish  [ 9.  1. 24.]\n",
      "Episode: 1182 Total reward: 14.2 Training loss: 0.0199 Epsilon: 0.0107 Life: 269\n",
      "Episode Finish  [ 4. 16. 24.]\n",
      "Episode: 1183 Total reward: 6.700000000000001 Training loss: 0.0321 Epsilon: 0.0107 Life: 269\n",
      "Episode Finish  [ 9. 12.  4.]\n",
      "Episode: 1184 Total reward: 15.000000000000002 Training loss: 0.0235 Epsilon: 0.0107 Life: 269\n",
      "Episode Finish  [ 6. 11. 14.]\n",
      "Episode: 1185 Total reward: 8.900000000000002 Training loss: 0.0311 Epsilon: 0.0107 Life: 269\n",
      "Episode Finish  [18.  0. 18.]\n",
      "Episode: 1186 Total reward: 32.0 Training loss: 0.0234 Epsilon: 0.0107 Life: 300\n",
      "Episode Finish  [ 7. 11. 24.]\n",
      "Episode: 1187 Total reward: 11.2 Training loss: 0.0259 Epsilon: 0.0107 Life: 300\n",
      "Episode Finish  [12.  6. 16.]\n",
      "Episode: 1188 Total reward: 20.6 Training loss: 0.0208 Epsilon: 0.0107 Life: 300\n",
      "Episode Finish  [12.  0.  2.]\n",
      "Episode: 1189 Total reward: 20.0 Training loss: 0.0263 Epsilon: 0.0107 Life: 300\n",
      "Episode Finish  [15.  0.  2.]\n",
      "Episode: 1190 Total reward: 25.9 Training loss: 0.0219 Epsilon: 0.0107 Life: 300\n",
      "Episode Finish  [14.  0.  2.]\n",
      "Episode: 1191 Total reward: 23.6 Training loss: 0.0274 Epsilon: 0.0106 Life: 300\n",
      "Episode Finish  [10.  4. 10.]\n",
      "Episode: 1192 Total reward: 16.4 Training loss: 0.0220 Epsilon: 0.0106 Life: 300\n",
      "Episode Finish  [10.  7. 24.]\n",
      "Episode: 1193 Total reward: 16.799999999999997 Training loss: 0.0236 Epsilon: 0.0106 Life: 300\n",
      "Episode Finish  [ 7. 13. 24.]\n",
      "Episode: 1194 Total reward: 11.4 Training loss: 0.0184 Epsilon: 0.0106 Life: 300\n",
      "Episode Finish  [ 9. 10. 14.]\n",
      "Episode: 1195 Total reward: 15.0 Training loss: 0.0253 Epsilon: 0.0106 Life: 300\n",
      "Episode Finish  [ 5. 13. 24.]\n",
      "Episode: 1196 Total reward: 7.4 Training loss: 0.0244 Epsilon: 0.0106 Life: 300\n",
      "Episode Finish  [ 7.  8. 24.]\n",
      "Episode: 1197 Total reward: 10.9 Training loss: 0.0263 Epsilon: 0.0106 Life: 300\n",
      "Episode Finish  [10.  4.  8.]\n",
      "Episode: 1198 Total reward: 16.2 Training loss: 0.0250 Epsilon: 0.0106 Life: 300\n",
      "Episode Finish  [10.  0.  4.]\n",
      "Episode: 1199 Total reward: 15.600000000000001 Training loss: 0.0189 Epsilon: 0.0106 Life: 300\n",
      "Episode Finish  [11.  0.  4.]\n",
      "Episode: 1200 Total reward: 17.5 Training loss: 0.0278 Epsilon: 0.0106 Life: 300\n",
      "Model Saved at 1200\n",
      "Stats Saved at 1200\n",
      "Episode Finish  [10.  6. 28.]\n",
      "Episode: 1201 Total reward: 17.299999999999997 Training loss: 0.0223 Epsilon: 0.0106 Life: 300\n",
      "Episode Finish  [ 5. 15. 24.]\n",
      "Episode: 1202 Total reward: 7.600000000000001 Training loss: 0.0257 Epsilon: 0.0106 Life: 300\n",
      "Episode Finish  [12.  1.  4.]\n",
      "Episode: 1203 Total reward: 19.9 Training loss: 0.0216 Epsilon: 0.0106 Life: 300\n",
      "Episode Finish  [ 8.  7. 24.]\n",
      "Episode: 1204 Total reward: 12.799999999999999 Training loss: 0.0220 Epsilon: 0.0106 Life: 300\n",
      "Episode Finish  [ 8. 11. 12.]\n",
      "Episode: 1205 Total reward: 13.1 Training loss: 0.0236 Epsilon: 0.0106 Life: 300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode Finish  [ 8.  5. 16.]\n",
      "Episode: 1206 Total reward: 12.3 Training loss: 0.0295 Epsilon: 0.0106 Life: 300\n",
      "Episode Finish  [10.  7.  4.]\n",
      "Episode: 1207 Total reward: 16.5 Training loss: 0.0298 Epsilon: 0.0106 Life: 300\n",
      "Episode Finish  [7. 9. 4.]\n",
      "Episode: 1208 Total reward: 10.7 Training loss: 0.0198 Epsilon: 0.0106 Life: 300\n",
      "Episode Finish  [11.  3. 10.]\n",
      "Episode: 1209 Total reward: 18.299999999999997 Training loss: 0.0296 Epsilon: 0.0106 Life: 300\n",
      "Episode Finish  [ 6. 10. 12.]\n",
      "Episode: 1210 Total reward: 8.7 Training loss: 0.0250 Epsilon: 0.0106 Life: 300\n",
      "Episode Finish  [ 8. 10.  6.]\n",
      "Episode: 1211 Total reward: 12.6 Training loss: 0.0223 Epsilon: 0.0106 Life: 300\n",
      "Episode Finish  [ 5. 11.  8.]\n",
      "Episode: 1212 Total reward: 6.800000000000001 Training loss: 0.0201 Epsilon: 0.0106 Life: 300\n",
      "Episode Finish  [ 9.  0. 24.]\n",
      "Episode: 1213 Total reward: 14.100000000000001 Training loss: 0.0225 Epsilon: 0.0106 Life: 300\n",
      "Episode Finish  [15.  0. 24.]\n",
      "Episode: 1214 Total reward: 26.1 Training loss: 0.0302 Epsilon: 0.0105 Life: 300\n",
      "Model updated\n",
      "Episode Finish  [10.  4.  2.]\n",
      "Episode: 1215 Total reward: 16.0 Training loss: 0.0240 Epsilon: 0.0105 Life: 300\n",
      "Episode Finish  [8. 3. 4.]\n",
      "Episode: 1216 Total reward: 11.8 Training loss: 0.0241 Epsilon: 0.0105 Life: 300\n",
      "Episode Finish  [8. 8. 8.]\n",
      "Episode: 1217 Total reward: 13.3 Training loss: 0.0247 Epsilon: 0.0105 Life: 300\n",
      "Episode Finish  [ 9.  5. 16.]\n",
      "Episode: 1218 Total reward: 14.599999999999998 Training loss: 0.0230 Epsilon: 0.0105 Life: 300\n",
      "Episode Finish  [10.  8. 36.]\n",
      "Episode: 1219 Total reward: 16.9 Training loss: 0.0362 Epsilon: 0.0105 Life: 300\n",
      "Episode Finish  [13.  0. 16.]\n",
      "Episode: 1220 Total reward: 21.799999999999997 Training loss: 0.0237 Epsilon: 0.0105 Life: 300\n",
      "Episode Finish  [11.  0. 24.]\n",
      "Episode: 1221 Total reward: 18.200000000000003 Training loss: 0.0251 Epsilon: 0.0105 Life: 300\n",
      "Episode Finish  [8. 6. 4.]\n",
      "Episode: 1222 Total reward: 13.399999999999999 Training loss: 0.0490 Epsilon: 0.0105 Life: 300\n",
      "Episode Finish  [10.  6.  6.]\n",
      "Episode: 1223 Total reward: 16.5 Training loss: 0.0347 Epsilon: 0.0105 Life: 300\n",
      "Episode Finish  [ 9.  3. 24.]\n",
      "Episode: 1224 Total reward: 14.400000000000002 Training loss: 0.0278 Epsilon: 0.0105 Life: 300\n",
      "Episode Finish  [11.  0. 24.]\n",
      "Episode: 1225 Total reward: 18.1 Training loss: 0.0495 Epsilon: 0.0105 Life: 300\n",
      "Episode Finish  [11.  0. 12.]\n",
      "Episode: 1226 Total reward: 17.9 Training loss: 0.0287 Epsilon: 0.0105 Life: 300\n",
      "Episode Finish  [11.  1.  4.]\n",
      "Episode: 1227 Total reward: 18.0 Training loss: 0.0186 Epsilon: 0.0105 Life: 300\n",
      "Episode Finish  [12.  0. 24.]\n",
      "Episode: 1228 Total reward: 20.099999999999998 Training loss: 0.0280 Epsilon: 0.0105 Life: 300\n",
      "Episode Finish  [7. 5. 8.]\n",
      "Episode: 1229 Total reward: 9.9 Training loss: 0.0218 Epsilon: 0.0105 Life: 300\n",
      "Episode Finish  [13.  0.  6.]\n",
      "Episode: 1230 Total reward: 21.9 Training loss: 0.0272 Epsilon: 0.0105 Life: 300\n",
      "Episode Finish  [ 7.  3. 24.]\n",
      "Episode: 1231 Total reward: 10.4 Training loss: 0.0204 Epsilon: 0.0105 Life: 300\n",
      "Episode Finish  [ 4. 17. 24.]\n",
      "Episode: 1232 Total reward: 5.800000000000001 Training loss: 0.0363 Epsilon: 0.0105 Life: 300\n",
      "Episode Finish  [7. 6. 8.]\n",
      "Episode: 1233 Total reward: 10.4 Training loss: 0.0359 Epsilon: 0.0105 Life: 300\n",
      "Episode Finish  [13.  0. 16.]\n",
      "Episode: 1234 Total reward: 21.700000000000003 Training loss: 0.0334 Epsilon: 0.0105 Life: 300\n",
      "Episode Finish  [12.  0. 12.]\n",
      "Episode: 1235 Total reward: 19.599999999999998 Training loss: 0.0212 Epsilon: 0.0105 Life: 300\n",
      "Episode Finish  [12.  0. 32.]\n",
      "Episode: 1236 Total reward: 19.799999999999997 Training loss: 0.0266 Epsilon: 0.0105 Life: 300\n",
      "Episode Finish  [13.  0.  8.]\n",
      "Episode: 1237 Total reward: 21.8 Training loss: 0.0215 Epsilon: 0.0105 Life: 300\n",
      "Episode Finish  [ 8.  0. 32.]\n",
      "Episode: 1238 Total reward: 11.899999999999999 Training loss: 0.0290 Epsilon: 0.0105 Life: 300\n",
      "Episode Finish  [12.  0.  4.]\n",
      "Episode: 1239 Total reward: 19.8 Training loss: 0.0340 Epsilon: 0.0105 Life: 300\n",
      "Episode Finish  [10.  0.  4.]\n",
      "Episode: 1240 Total reward: 15.8 Training loss: 0.0403 Epsilon: 0.0104 Life: 300\n",
      "Episode Finish  [12.  0.  4.]\n",
      "Episode: 1241 Total reward: 19.5 Training loss: 0.0330 Epsilon: 0.0104 Life: 300\n",
      "Episode Finish  [ 8.  6. 24.]\n",
      "Episode: 1242 Total reward: 12.700000000000001 Training loss: 0.0213 Epsilon: 0.0104 Life: 300\n",
      "Episode Finish  [ 4. 18. 24.]\n",
      "Episode: 1243 Total reward: 5.8999999999999995 Training loss: 0.0201 Epsilon: 0.0104 Life: 300\n",
      "Episode Finish  [ 3. 17. 24.]\n",
      "Episode: 1244 Total reward: 4.8 Training loss: 0.0361 Epsilon: 0.0104 Life: 300\n",
      "Episode Finish  [10.  3. 24.]\n",
      "Episode: 1245 Total reward: 16.4 Training loss: 0.0302 Epsilon: 0.0104 Life: 300\n",
      "Episode Finish  [11.  0. 32.]\n",
      "Episode: 1246 Total reward: 18.200000000000003 Training loss: 0.0193 Epsilon: 0.0104 Life: 300\n",
      "Episode Finish  [ 8.  4. 10.]\n",
      "Episode: 1247 Total reward: 12.399999999999999 Training loss: 0.0244 Epsilon: 0.0104 Life: 300\n",
      "Episode Finish  [12.  1.  6.]\n",
      "Episode: 1248 Total reward: 20.0 Training loss: 0.0425 Epsilon: 0.0104 Life: 300\n",
      "Episode Finish  [12.  0. 16.]\n",
      "Episode: 1249 Total reward: 19.5 Training loss: 0.0260 Epsilon: 0.0104 Life: 300\n",
      "Episode Finish  [ 8.  4. 24.]\n",
      "Episode: 1250 Total reward: 12.5 Training loss: 0.0277 Epsilon: 0.0104 Life: 300\n",
      "Stats Saved at 1250\n",
      "Episode Finish  [ 7. 13. 24.]\n",
      "Episode: 1251 Total reward: 11.4 Training loss: 0.0249 Epsilon: 0.0104 Life: 300\n",
      "Episode Finish  [8. 6. 4.]\n",
      "Episode: 1252 Total reward: 12.400000000000002 Training loss: 0.0380 Epsilon: 0.0104 Life: 300\n",
      "Episode Finish  [ 9.  0. 14.]\n",
      "Episode: 1253 Total reward: 13.5 Training loss: 0.0215 Epsilon: 0.0104 Life: 300\n",
      "Episode Finish  [10.  0.  8.]\n",
      "Episode: 1254 Total reward: 16.0 Training loss: 0.0336 Epsilon: 0.0104 Life: 300\n",
      "Episode Finish  [10.  0. 36.]\n",
      "Episode: 1255 Total reward: 16.0 Training loss: 0.0207 Epsilon: 0.0104 Life: 300\n",
      "Episode Finish  [11.  0. 36.]\n",
      "Episode: 1256 Total reward: 18.200000000000003 Training loss: 0.0271 Epsilon: 0.0104 Life: 300\n",
      "Episode Finish  [10.  3. 24.]\n",
      "Episode: 1257 Total reward: 16.4 Training loss: 0.0250 Epsilon: 0.0104 Life: 300\n",
      "Episode Finish  [ 8.  9. 16.]\n",
      "Episode: 1258 Total reward: 12.9 Training loss: 0.0203 Epsilon: 0.0104 Life: 300\n",
      "Episode Finish  [ 7.  6. 12.]\n",
      "Episode: 1259 Total reward: 10.799999999999999 Training loss: 0.0304 Epsilon: 0.0104 Life: 300\n",
      "Episode Finish  [ 5. 15. 24.]\n",
      "Episode: 1260 Total reward: 7.600000000000001 Training loss: 0.0235 Epsilon: 0.0104 Life: 300\n",
      "Episode Finish  [11.  0. 10.]\n",
      "Episode: 1261 Total reward: 18.0 Training loss: 0.0268 Epsilon: 0.0104 Life: 300\n",
      "Episode Finish  [12.  0. 20.]\n",
      "Episode: 1262 Total reward: 20.0 Training loss: 0.0288 Epsilon: 0.0104 Life: 300\n",
      "Episode Finish  [15.  0. 24.]\n",
      "Episode: 1263 Total reward: 26.099999999999998 Training loss: 0.0462 Epsilon: 0.0104 Life: 300\n",
      "Episode Finish  [ 7.  1. 10.]\n",
      "Episode: 1264 Total reward: 10.0 Training loss: 0.0210 Epsilon: 0.0104 Life: 300\n",
      "Episode Finish  [10.  4. 24.]\n",
      "Episode: 1265 Total reward: 16.5 Training loss: 0.0250 Epsilon: 0.0104 Life: 300\n",
      "Episode Finish  [12.  0. 20.]\n",
      "Episode: 1266 Total reward: 20.2 Training loss: 0.0275 Epsilon: 0.0104 Life: 300\n",
      "Episode Finish  [13.  0. 14.]\n",
      "Episode: 1267 Total reward: 22.0 Training loss: 0.0274 Epsilon: 0.0104 Life: 300\n",
      "Episode Finish  [12.  0.  4.]\n",
      "Episode: 1268 Total reward: 19.7 Training loss: 0.0240 Epsilon: 0.0104 Life: 300\n",
      "Episode Finish  [ 5. 15. 24.]\n",
      "Episode: 1269 Total reward: 7.600000000000001 Training loss: 0.0218 Epsilon: 0.0104 Life: 300\n",
      "Episode Finish  [7. 0. 2.]\n",
      "Episode: 1270 Total reward: 9.3 Training loss: 0.0247 Epsilon: 0.0104 Life: 300\n",
      "Episode Finish  [8. 0. 2.]\n",
      "Episode: 1271 Total reward: 12.0 Training loss: 0.0307 Epsilon: 0.0104 Life: 300\n",
      "Episode Finish  [10.  6.  4.]\n",
      "Episode: 1272 Total reward: 16.4 Training loss: 0.0186 Epsilon: 0.0104 Life: 300\n",
      "Episode Finish  [12.  0. 24.]\n",
      "Episode: 1273 Total reward: 20.1 Training loss: 0.0241 Epsilon: 0.0103 Life: 300\n",
      "Episode Finish  [13.  0. 12.]\n",
      "Episode: 1274 Total reward: 21.9 Training loss: 0.0174 Epsilon: 0.0103 Life: 300\n",
      "Episode Finish  [15.  0. 24.]\n",
      "Episode: 1275 Total reward: 26.1 Training loss: 0.0328 Epsilon: 0.0103 Life: 300\n",
      "Episode Finish  [10.  4.  2.]\n",
      "Episode: 1276 Total reward: 16.0 Training loss: 0.0257 Epsilon: 0.0103 Life: 300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode Finish  [10.  0. 20.]\n",
      "Episode: 1277 Total reward: 16.099999999999998 Training loss: 0.0401 Epsilon: 0.0103 Life: 300\n",
      "Episode Finish  [11.  0. 16.]\n",
      "Episode: 1278 Total reward: 17.7 Training loss: 0.0185 Epsilon: 0.0103 Life: 300\n",
      "Model updated\n",
      "Episode Finish  [10.  0.  4.]\n",
      "Episode: 1279 Total reward: 15.800000000000002 Training loss: 0.0246 Epsilon: 0.0103 Life: 300\n",
      "Episode Finish  [10.  0. 24.]\n",
      "Episode: 1280 Total reward: 16.1 Training loss: 0.0263 Epsilon: 0.0103 Life: 300\n",
      "Episode Finish  [11.  0. 32.]\n",
      "Episode: 1281 Total reward: 17.8 Training loss: 0.0200 Epsilon: 0.0103 Life: 300\n",
      "Episode Finish  [11.  1. 24.]\n",
      "Episode: 1282 Total reward: 18.200000000000003 Training loss: 0.0384 Epsilon: 0.0103 Life: 300\n",
      "Episode Finish  [10.  2.  4.]\n",
      "Episode: 1283 Total reward: 16.0 Training loss: 0.0206 Epsilon: 0.0103 Life: 300\n",
      "Episode Finish  [12.  0. 24.]\n",
      "Episode: 1284 Total reward: 20.1 Training loss: 0.0335 Epsilon: 0.0103 Life: 300\n",
      "Episode Finish  [14.  0. 24.]\n",
      "Episode: 1285 Total reward: 24.099999999999998 Training loss: 0.0187 Epsilon: 0.0103 Life: 300\n",
      "Episode Finish  [12.  0. 24.]\n",
      "Episode: 1286 Total reward: 20.1 Training loss: 0.0364 Epsilon: 0.0103 Life: 300\n",
      "Episode Finish  [ 6. 10. 24.]\n",
      "Episode: 1287 Total reward: 9.1 Training loss: 0.0380 Epsilon: 0.0103 Life: 300\n",
      "Episode Finish  [15.  0. 10.]\n",
      "Episode: 1288 Total reward: 26.0 Training loss: 0.0259 Epsilon: 0.0103 Life: 300\n",
      "Episode Finish  [11.  2. 10.]\n",
      "Episode: 1289 Total reward: 18.200000000000003 Training loss: 0.0292 Epsilon: 0.0103 Life: 300\n",
      "Episode Finish  [ 4. 13. 24.]\n",
      "Episode: 1290 Total reward: 5.4 Training loss: 0.0162 Epsilon: 0.0103 Life: 300\n",
      "Episode Finish  [10.  4. 16.]\n",
      "Episode: 1291 Total reward: 16.3 Training loss: 0.0318 Epsilon: 0.0103 Life: 300\n",
      "Episode Finish  [14.  0. 24.]\n",
      "Episode: 1292 Total reward: 24.1 Training loss: 0.0216 Epsilon: 0.0103 Life: 300\n",
      "Episode Finish  [ 7. 11. 28.]\n",
      "Episode: 1293 Total reward: 11.200000000000001 Training loss: 0.0258 Epsilon: 0.0103 Life: 300\n",
      "Episode Finish  [ 8.  5. 10.]\n",
      "Episode: 1294 Total reward: 12.0 Training loss: 0.0302 Epsilon: 0.0103 Life: 300\n",
      "Episode Finish  [10.  0. 10.]\n",
      "Episode: 1295 Total reward: 15.8 Training loss: 0.0318 Epsilon: 0.0103 Life: 300\n",
      "Episode Finish  [13.  0.  4.]\n",
      "Episode: 1296 Total reward: 21.800000000000004 Training loss: 0.0200 Epsilon: 0.0103 Life: 300\n",
      "Episode Finish  [ 6.  9. 24.]\n",
      "Episode: 1297 Total reward: 9.0 Training loss: 0.0241 Epsilon: 0.0103 Life: 300\n",
      "Episode Finish  [ 8.  1. 12.]\n",
      "Episode: 1298 Total reward: 11.899999999999999 Training loss: 0.0295 Epsilon: 0.0103 Life: 300\n",
      "Episode Finish  [9. 3. 6.]\n",
      "Episode: 1299 Total reward: 14.000000000000002 Training loss: 0.0171 Epsilon: 0.0103 Life: 300\n",
      "Episode Finish  [ 7.  4. 24.]\n",
      "Episode: 1300 Total reward: 10.499999999999998 Training loss: 0.0315 Epsilon: 0.0103 Life: 300\n",
      "Stats Saved at 1300\n",
      "Episode Finish  [12.  4. 24.]\n",
      "Episode: 1301 Total reward: 20.499999999999996 Training loss: 0.0236 Epsilon: 0.0103 Life: 300\n",
      "Episode Finish  [12.  0. 10.]\n",
      "Episode: 1302 Total reward: 19.800000000000004 Training loss: 0.0256 Epsilon: 0.0103 Life: 300\n",
      "Episode Finish  [ 6. 15. 14.]\n",
      "Episode: 1303 Total reward: 9.3 Training loss: 0.0275 Epsilon: 0.0103 Life: 300\n",
      "Episode Finish  [9. 7. 4.]\n",
      "Episode: 1304 Total reward: 14.500000000000002 Training loss: 0.0272 Epsilon: 0.0103 Life: 300\n",
      "Episode Finish  [9. 0. 4.]\n",
      "Episode: 1305 Total reward: 13.899999999999999 Training loss: 0.0346 Epsilon: 0.0103 Life: 300\n",
      "Episode Finish  [10.  7. 24.]\n",
      "Episode: 1306 Total reward: 16.799999999999997 Training loss: 0.0229 Epsilon: 0.0103 Life: 300\n",
      "Episode Finish  [9. 0. 6.]\n",
      "Episode: 1307 Total reward: 13.5 Training loss: 0.0195 Epsilon: 0.0103 Life: 300\n",
      "Episode Finish  [10.  4. 24.]\n",
      "Episode: 1308 Total reward: 16.5 Training loss: 0.0285 Epsilon: 0.0103 Life: 300\n",
      "Episode Finish  [ 6.  8. 32.]\n",
      "Episode: 1309 Total reward: 8.8 Training loss: 0.0226 Epsilon: 0.0103 Life: 300\n",
      "Episode Finish  [10.  8. 16.]\n",
      "Episode: 1310 Total reward: 16.8 Training loss: 0.0242 Epsilon: 0.0103 Life: 300\n",
      "Episode Finish  [12.  0. 24.]\n",
      "Episode: 1311 Total reward: 20.1 Training loss: 0.0242 Epsilon: 0.0103 Life: 300\n",
      "Episode Finish  [ 8. 10. 16.]\n",
      "Episode: 1312 Total reward: 12.599999999999998 Training loss: 0.0275 Epsilon: 0.0103 Life: 300\n",
      "Episode Finish  [ 7. 11. 24.]\n",
      "Episode: 1313 Total reward: 11.2 Training loss: 0.0284 Epsilon: 0.0103 Life: 300\n",
      "Episode Finish  [ 6. 12. 10.]\n",
      "Episode: 1314 Total reward: 9.200000000000001 Training loss: 0.0243 Epsilon: 0.0103 Life: 300\n",
      "Episode Finish  [ 7. 14. 24.]\n",
      "Episode: 1315 Total reward: 11.5 Training loss: 0.0208 Epsilon: 0.0103 Life: 300\n",
      "Episode Finish  [13.  0. 24.]\n",
      "Episode: 1316 Total reward: 22.1 Training loss: 0.0203 Epsilon: 0.0102 Life: 300\n",
      "Episode Finish  [ 9.  1. 22.]\n",
      "Episode: 1317 Total reward: 14.100000000000001 Training loss: 0.0439 Epsilon: 0.0102 Life: 300\n",
      "Episode Finish  [9. 6. 4.]\n",
      "Episode: 1318 Total reward: 14.6 Training loss: 0.0170 Epsilon: 0.0102 Life: 300\n",
      "Episode Finish  [ 6. 11. 24.]\n",
      "Episode: 1319 Total reward: 9.200000000000001 Training loss: 0.0236 Epsilon: 0.0102 Life: 300\n",
      "Episode Finish  [9. 0. 8.]\n",
      "Episode: 1320 Total reward: 14.0 Training loss: 0.0242 Epsilon: 0.0102 Life: 300\n",
      "Episode Finish  [16.  0.  8.]\n",
      "Episode: 1321 Total reward: 27.8 Training loss: 0.0267 Epsilon: 0.0102 Life: 300\n",
      "Episode Finish  [11.  2. 20.]\n",
      "Episode: 1322 Total reward: 18.1 Training loss: 0.0219 Epsilon: 0.0102 Life: 300\n",
      "Episode Finish  [10.  0. 10.]\n",
      "Episode: 1323 Total reward: 15.999999999999998 Training loss: 0.0331 Epsilon: 0.0102 Life: 300\n",
      "Episode Finish  [9. 0. 8.]\n",
      "Episode: 1324 Total reward: 13.799999999999999 Training loss: 0.0243 Epsilon: 0.0102 Life: 300\n",
      "Episode Finish  [ 5. 14. 24.]\n",
      "Episode: 1325 Total reward: 7.5 Training loss: 0.0240 Epsilon: 0.0102 Life: 300\n",
      "Episode Finish  [10.  7.  8.]\n",
      "Episode: 1326 Total reward: 16.5 Training loss: 0.0395 Epsilon: 0.0102 Life: 300\n",
      "Episode Finish  [ 7. 10.  2.]\n",
      "Episode: 1327 Total reward: 10.9 Training loss: 0.0199 Epsilon: 0.0102 Life: 300\n",
      "Episode Finish  [12.  0. 24.]\n",
      "Episode: 1328 Total reward: 20.1 Training loss: 0.0267 Epsilon: 0.0102 Life: 300\n",
      "Episode Finish  [13.  0. 32.]\n",
      "Episode: 1329 Total reward: 21.800000000000004 Training loss: 0.0257 Epsilon: 0.0102 Life: 300\n",
      "Episode Finish  [11.  0. 40.]\n",
      "Episode: 1330 Total reward: 18.200000000000003 Training loss: 0.0249 Epsilon: 0.0102 Life: 300\n",
      "Episode Finish  [ 6.  8. 24.]\n",
      "Episode: 1331 Total reward: 8.9 Training loss: 0.0226 Epsilon: 0.0102 Life: 300\n",
      "Episode Finish  [ 7. 10.  4.]\n",
      "Episode: 1332 Total reward: 10.8 Training loss: 0.0201 Epsilon: 0.0102 Life: 300\n",
      "Episode Finish  [8. 7. 4.]\n",
      "Episode: 1333 Total reward: 13.5 Training loss: 0.0178 Epsilon: 0.0102 Life: 300\n",
      "Episode Finish  [15.  0. 10.]\n",
      "Episode: 1334 Total reward: 26.0 Training loss: 0.0211 Epsilon: 0.0102 Life: 300\n",
      "Episode Finish  [13.  3. 24.]\n",
      "Episode: 1335 Total reward: 22.4 Training loss: 0.0283 Epsilon: 0.0102 Life: 300\n",
      "Episode Finish  [12.  0. 16.]\n",
      "Episode: 1336 Total reward: 19.9 Training loss: 0.0245 Epsilon: 0.0102 Life: 300\n",
      "Episode Finish  [13.  0. 16.]\n",
      "Episode: 1337 Total reward: 22.0 Training loss: 0.0181 Epsilon: 0.0102 Life: 300\n",
      "Episode Finish  [11.  4.  6.]\n",
      "Episode: 1338 Total reward: 18.400000000000002 Training loss: 0.0262 Epsilon: 0.0102 Life: 300\n",
      "Episode Finish  [12.  0.  2.]\n",
      "Episode: 1339 Total reward: 19.799999999999997 Training loss: 0.0212 Epsilon: 0.0102 Life: 300\n",
      "Episode Finish  [13.  0. 24.]\n",
      "Episode: 1340 Total reward: 22.1 Training loss: 0.0233 Epsilon: 0.0102 Life: 300\n",
      "Episode Finish  [10.  3. 16.]\n",
      "Episode: 1341 Total reward: 15.9 Training loss: 0.0184 Epsilon: 0.0102 Life: 300\n",
      "Episode Finish  [10.  4. 20.]\n",
      "Episode: 1342 Total reward: 16.4 Training loss: 0.0164 Epsilon: 0.0102 Life: 300\n",
      "Model updated\n",
      "Episode Finish  [ 5. 13. 24.]\n",
      "Episode: 1343 Total reward: 7.399999999999999 Training loss: 0.0286 Epsilon: 0.0102 Life: 300\n",
      "Episode Finish  [ 7.  8. 24.]\n",
      "Episode: 1344 Total reward: 10.899999999999999 Training loss: 0.0264 Epsilon: 0.0102 Life: 300\n",
      "Episode Finish  [11.  0. 16.]\n",
      "Episode: 1345 Total reward: 18.1 Training loss: 0.0307 Epsilon: 0.0102 Life: 300\n",
      "Episode Finish  [14.  0. 24.]\n",
      "Episode: 1346 Total reward: 24.1 Training loss: 0.0389 Epsilon: 0.0102 Life: 300\n",
      "Episode Finish  [13.  3. 16.]\n",
      "Episode: 1347 Total reward: 22.4 Training loss: 0.0409 Epsilon: 0.0102 Life: 300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode Finish  [ 8.  7. 32.]\n",
      "Episode: 1348 Total reward: 12.500000000000002 Training loss: 0.0254 Epsilon: 0.0102 Life: 300\n",
      "Episode Finish  [11.  0.  8.]\n",
      "Episode: 1349 Total reward: 18.099999999999998 Training loss: 0.0298 Epsilon: 0.0102 Life: 300\n",
      "Episode Finish  [15.  0. 24.]\n",
      "Episode: 1350 Total reward: 26.1 Training loss: 0.0291 Epsilon: 0.0102 Life: 300\n",
      "Stats Saved at 1350\n",
      "Episode Finish  [14.  0.  4.]\n",
      "Episode: 1351 Total reward: 23.700000000000003 Training loss: 0.0338 Epsilon: 0.0102 Life: 300\n",
      "Episode Finish  [13.  0. 24.]\n",
      "Episode: 1352 Total reward: 22.1 Training loss: 0.0315 Epsilon: 0.0102 Life: 300\n",
      "Episode Finish  [14.  0. 24.]\n",
      "Episode: 1353 Total reward: 24.1 Training loss: 0.0238 Epsilon: 0.0102 Life: 300\n",
      "Episode Finish  [7. 6. 8.]\n",
      "Episode: 1354 Total reward: 10.4 Training loss: 0.0229 Epsilon: 0.0102 Life: 300\n",
      "Episode Finish  [15.  0.  8.]\n",
      "Episode: 1355 Total reward: 25.7 Training loss: 0.0206 Epsilon: 0.0102 Life: 300\n",
      "Episode Finish  [ 6. 13.  2.]\n",
      "Episode: 1356 Total reward: 9.2 Training loss: 0.0244 Epsilon: 0.0102 Life: 300\n",
      "Episode Finish  [12.  0. 24.]\n",
      "Episode: 1357 Total reward: 20.1 Training loss: 0.0242 Epsilon: 0.0102 Life: 300\n",
      "Episode Finish  [14.  1. 24.]\n",
      "Episode: 1358 Total reward: 24.200000000000003 Training loss: 0.0306 Epsilon: 0.0102 Life: 300\n",
      "Episode Finish  [11.  0. 14.]\n",
      "Episode: 1359 Total reward: 18.0 Training loss: 0.0374 Epsilon: 0.0102 Life: 300\n",
      "Episode Finish  [9. 5. 4.]\n",
      "Episode: 1360 Total reward: 14.0 Training loss: 0.0234 Epsilon: 0.0102 Life: 300\n",
      "Episode Finish  [13.  0. 24.]\n",
      "Episode: 1361 Total reward: 22.1 Training loss: 0.0174 Epsilon: 0.0102 Life: 300\n",
      "Episode Finish  [12.  0. 14.]\n",
      "Episode: 1362 Total reward: 20.0 Training loss: 0.0313 Epsilon: 0.0102 Life: 300\n",
      "Episode Finish  [12.  9. 24.]\n",
      "Episode: 1363 Total reward: 21.0 Training loss: 0.0301 Epsilon: 0.0102 Life: 300\n",
      "Episode Finish  [12.  0.  6.]\n",
      "Episode: 1364 Total reward: 19.700000000000003 Training loss: 0.0292 Epsilon: 0.0102 Life: 300\n",
      "Episode Finish  [15.  0. 24.]\n",
      "Episode: 1365 Total reward: 26.1 Training loss: 0.0300 Epsilon: 0.0102 Life: 300\n",
      "Episode Finish  [8. 9. 6.]\n",
      "Episode: 1366 Total reward: 12.7 Training loss: 0.0204 Epsilon: 0.0102 Life: 300\n",
      "Episode Finish  [10.  2. 14.]\n",
      "Episode: 1367 Total reward: 16.1 Training loss: 0.0199 Epsilon: 0.0102 Life: 300\n",
      "Episode Finish  [ 7. 13.  4.]\n",
      "Episode: 1368 Total reward: 11.3 Training loss: 0.0230 Epsilon: 0.0102 Life: 300\n",
      "Episode Finish  [12.  0.  4.]\n",
      "Episode: 1369 Total reward: 19.8 Training loss: 0.0196 Epsilon: 0.0102 Life: 300\n",
      "Episode Finish  [13.  0. 10.]\n",
      "Episode: 1370 Total reward: 22.0 Training loss: 0.0220 Epsilon: 0.0102 Life: 300\n",
      "Episode Finish  [13.  0. 16.]\n",
      "Episode: 1371 Total reward: 21.7 Training loss: 0.0175 Epsilon: 0.0102 Life: 300\n",
      "Episode Finish  [12.  0.  8.]\n",
      "Episode: 1372 Total reward: 19.800000000000004 Training loss: 0.0278 Epsilon: 0.0102 Life: 300\n",
      "Episode Finish  [ 7. 12. 24.]\n",
      "Episode: 1373 Total reward: 11.3 Training loss: 0.0278 Epsilon: 0.0102 Life: 300\n",
      "Episode Finish  [11.  0.  6.]\n",
      "Episode: 1374 Total reward: 17.6 Training loss: 0.0255 Epsilon: 0.0102 Life: 300\n",
      "Episode Finish  [14.  0. 14.]\n",
      "Episode: 1375 Total reward: 24.0 Training loss: 0.0299 Epsilon: 0.0102 Life: 300\n",
      "Episode Finish  [14.  0. 24.]\n",
      "Episode: 1376 Total reward: 24.1 Training loss: 0.0298 Epsilon: 0.0102 Life: 300\n",
      "Episode Finish  [11.  5.  4.]\n",
      "Episode: 1377 Total reward: 18.1 Training loss: 0.0201 Epsilon: 0.0101 Life: 300\n",
      "Episode Finish  [17.  0. 24.]\n",
      "Episode: 1378 Total reward: 30.1 Training loss: 0.0255 Epsilon: 0.0101 Life: 300\n",
      "Episode Finish  [9. 0. 8.]\n",
      "Episode: 1379 Total reward: 13.9 Training loss: 0.0196 Epsilon: 0.0101 Life: 300\n",
      "Episode Finish  [14.  0.  8.]\n",
      "Episode: 1380 Total reward: 23.7 Training loss: 0.0209 Epsilon: 0.0101 Life: 300\n",
      "Episode Finish  [ 9.  0. 10.]\n",
      "Episode: 1381 Total reward: 13.7 Training loss: 0.0295 Epsilon: 0.0101 Life: 300\n",
      "Episode Finish  [13.  0. 24.]\n",
      "Episode: 1382 Total reward: 22.1 Training loss: 0.0234 Epsilon: 0.0101 Life: 300\n",
      "Episode Finish  [ 9.  0. 12.]\n",
      "Episode: 1383 Total reward: 14.0 Training loss: 0.0288 Epsilon: 0.0101 Life: 300\n",
      "Episode Finish  [13.  2. 24.]\n",
      "Episode: 1384 Total reward: 22.299999999999997 Training loss: 0.0170 Epsilon: 0.0101 Life: 300\n",
      "Episode Finish  [15.  0.  8.]\n",
      "Episode: 1385 Total reward: 25.599999999999998 Training loss: 0.0287 Epsilon: 0.0101 Life: 300\n",
      "Episode Finish  [14.  0. 24.]\n",
      "Episode: 1386 Total reward: 24.1 Training loss: 0.0241 Epsilon: 0.0101 Life: 300\n",
      "Episode Finish  [13.  0. 14.]\n",
      "Episode: 1387 Total reward: 21.9 Training loss: 0.0243 Epsilon: 0.0101 Life: 300\n",
      "Episode Finish  [ 7. 13. 24.]\n",
      "Episode: 1388 Total reward: 11.4 Training loss: 0.0227 Epsilon: 0.0101 Life: 300\n",
      "Episode Finish  [11.  0.  4.]\n",
      "Episode: 1389 Total reward: 17.799999999999997 Training loss: 0.0327 Epsilon: 0.0101 Life: 300\n",
      "Episode Finish  [13.  0. 24.]\n",
      "Episode: 1390 Total reward: 22.1 Training loss: 0.0283 Epsilon: 0.0101 Life: 300\n",
      "Episode Finish  [12.  0. 24.]\n",
      "Episode: 1391 Total reward: 20.1 Training loss: 0.0206 Epsilon: 0.0101 Life: 300\n",
      "Episode Finish  [14.  0.  2.]\n",
      "Episode: 1392 Total reward: 23.700000000000003 Training loss: 0.0235 Epsilon: 0.0101 Life: 300\n",
      "Episode Finish  [13.  0.  2.]\n",
      "Episode: 1393 Total reward: 21.599999999999998 Training loss: 0.0275 Epsilon: 0.0101 Life: 300\n",
      "Episode Finish  [16.  0. 24.]\n",
      "Episode: 1394 Total reward: 28.1 Training loss: 0.0202 Epsilon: 0.0101 Life: 300\n",
      "Episode Finish  [ 7. 14. 24.]\n",
      "Episode: 1395 Total reward: 11.5 Training loss: 0.0365 Epsilon: 0.0101 Life: 300\n",
      "Episode Finish  [13.  1. 24.]\n",
      "Episode: 1396 Total reward: 22.200000000000003 Training loss: 0.0235 Epsilon: 0.0101 Life: 300\n",
      "Episode Finish  [9. 4. 4.]\n",
      "Episode: 1397 Total reward: 14.5 Training loss: 0.0287 Epsilon: 0.0101 Life: 300\n",
      "Episode Finish  [14.  0. 16.]\n",
      "Episode: 1398 Total reward: 23.5 Training loss: 0.0255 Epsilon: 0.0101 Life: 300\n",
      "Model updated\n",
      "Episode Finish  [13.  0. 24.]\n",
      "Episode: 1399 Total reward: 22.1 Training loss: 0.0254 Epsilon: 0.0101 Life: 300\n",
      "Episode Finish  [ 9.  3. 24.]\n",
      "Episode: 1400 Total reward: 14.4 Training loss: 0.0265 Epsilon: 0.0101 Life: 300\n",
      "Model Saved at 1400\n",
      "Stats Saved at 1400\n",
      "Episode Finish  [14.  0.  2.]\n",
      "Episode: 1401 Total reward: 23.7 Training loss: 0.0232 Epsilon: 0.0101 Life: 300\n",
      "Episode Finish  [13.  0. 24.]\n",
      "Episode: 1402 Total reward: 22.1 Training loss: 0.0253 Epsilon: 0.0101 Life: 300\n",
      "Episode Finish  [12.  0. 22.]\n",
      "Episode: 1403 Total reward: 20.0 Training loss: 0.0230 Epsilon: 0.0101 Life: 300\n",
      "Episode Finish  [12.  4. 24.]\n",
      "Episode: 1404 Total reward: 20.5 Training loss: 0.0289 Epsilon: 0.0101 Life: 300\n",
      "Episode Finish  [15.  0.  2.]\n",
      "Episode: 1405 Total reward: 25.800000000000004 Training loss: 0.0259 Epsilon: 0.0101 Life: 300\n",
      "Episode Finish  [10.  0.  4.]\n",
      "Episode: 1406 Total reward: 15.8 Training loss: 0.0204 Epsilon: 0.0101 Life: 300\n",
      "Episode Finish  [11.  0. 16.]\n",
      "Episode: 1407 Total reward: 17.9 Training loss: 0.0250 Epsilon: 0.0101 Life: 300\n",
      "Episode Finish  [ 7.  3. 10.]\n",
      "Episode: 1408 Total reward: 9.9 Training loss: 0.0221 Epsilon: 0.0101 Life: 300\n",
      "Episode Finish  [12.  0. 16.]\n",
      "Episode: 1409 Total reward: 20.0 Training loss: 0.0352 Epsilon: 0.0101 Life: 300\n",
      "Episode Finish  [13.  0.  8.]\n",
      "Episode: 1410 Total reward: 21.799999999999997 Training loss: 0.0209 Epsilon: 0.0101 Life: 300\n",
      "Episode Finish  [8. 0. 8.]\n",
      "Episode: 1411 Total reward: 11.7 Training loss: 0.0365 Epsilon: 0.0101 Life: 300\n",
      "Episode Finish  [8. 3. 6.]\n",
      "Episode: 1412 Total reward: 11.700000000000001 Training loss: 0.0203 Epsilon: 0.0101 Life: 300\n",
      "Episode Finish  [13.  0. 32.]\n",
      "Episode: 1413 Total reward: 22.1 Training loss: 0.0187 Epsilon: 0.0101 Life: 300\n",
      "Episode Finish  [14.  0. 24.]\n",
      "Episode: 1414 Total reward: 24.1 Training loss: 0.0210 Epsilon: 0.0101 Life: 300\n",
      "Episode Finish  [9. 0. 4.]\n",
      "Episode: 1415 Total reward: 13.8 Training loss: 0.0223 Epsilon: 0.0101 Life: 300\n",
      "Episode Finish  [12.  0. 24.]\n",
      "Episode: 1416 Total reward: 20.1 Training loss: 0.0182 Epsilon: 0.0101 Life: 300\n",
      "Episode Finish  [12.  0.  8.]\n",
      "Episode: 1417 Total reward: 19.799999999999997 Training loss: 0.0237 Epsilon: 0.0101 Life: 300\n",
      "Episode Finish  [ 8.  0. 28.]\n",
      "Episode: 1418 Total reward: 11.7 Training loss: 0.0222 Epsilon: 0.0101 Life: 300\n",
      "Episode Finish  [12.  1. 16.]\n",
      "Episode: 1419 Total reward: 20.200000000000003 Training loss: 0.0194 Epsilon: 0.0101 Life: 300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode Finish  [7. 8. 8.]\n",
      "Episode: 1420 Total reward: 10.6 Training loss: 0.0247 Epsilon: 0.0101 Life: 300\n",
      "Episode Finish  [13.  0. 24.]\n",
      "Episode: 1421 Total reward: 22.1 Training loss: 0.0281 Epsilon: 0.0101 Life: 300\n",
      "Episode Finish  [13.  0. 24.]\n",
      "Episode: 1422 Total reward: 22.1 Training loss: 0.0234 Epsilon: 0.0101 Life: 300\n",
      "Episode Finish  [ 9.  8. 28.]\n",
      "Episode: 1423 Total reward: 14.7 Training loss: 0.0189 Epsilon: 0.0101 Life: 300\n",
      "Episode Finish  [13.  0. 24.]\n",
      "Episode: 1424 Total reward: 22.2 Training loss: 0.0244 Epsilon: 0.0101 Life: 300\n",
      "Episode Finish  [11.  0. 36.]\n",
      "Episode: 1425 Total reward: 18.099999999999998 Training loss: 0.0224 Epsilon: 0.0101 Life: 300\n",
      "Episode Finish  [ 7.  1. 12.]\n",
      "Episode: 1426 Total reward: 9.7 Training loss: 0.0293 Epsilon: 0.0101 Life: 300\n",
      "Episode Finish  [11.  0. 10.]\n",
      "Episode: 1427 Total reward: 18.0 Training loss: 0.0377 Epsilon: 0.0101 Life: 300\n",
      "Episode Finish  [12.  0. 24.]\n",
      "Episode: 1428 Total reward: 20.1 Training loss: 0.0297 Epsilon: 0.0101 Life: 300\n",
      "Episode Finish  [13.  5. 14.]\n",
      "Episode: 1429 Total reward: 22.5 Training loss: 0.0345 Epsilon: 0.0101 Life: 300\n",
      "Episode Finish  [7. 3. 2.]\n",
      "Episode: 1430 Total reward: 10.1 Training loss: 0.0294 Epsilon: 0.0101 Life: 300\n",
      "Episode Finish  [9. 4. 4.]\n",
      "Episode: 1431 Total reward: 13.899999999999999 Training loss: 0.0236 Epsilon: 0.0101 Life: 300\n",
      "Episode Finish  [14.  0. 24.]\n",
      "Episode: 1432 Total reward: 24.1 Training loss: 0.0265 Epsilon: 0.0101 Life: 300\n",
      "Episode Finish  [ 9.  0. 12.]\n",
      "Episode: 1433 Total reward: 13.899999999999999 Training loss: 0.0251 Epsilon: 0.0101 Life: 300\n",
      "Episode Finish  [10.  3. 24.]\n",
      "Episode: 1434 Total reward: 16.4 Training loss: 0.0212 Epsilon: 0.0101 Life: 300\n",
      "Episode Finish  [14.  0. 22.]\n",
      "Episode: 1435 Total reward: 23.7 Training loss: 0.0181 Epsilon: 0.0101 Life: 300\n",
      "Episode Finish  [12.  0. 24.]\n",
      "Episode: 1436 Total reward: 20.1 Training loss: 0.0262 Epsilon: 0.0101 Life: 300\n",
      "Episode Finish  [12.  0.  4.]\n",
      "Episode: 1437 Total reward: 20.0 Training loss: 0.0252 Epsilon: 0.0101 Life: 300\n",
      "Episode Finish  [13.  0. 24.]\n",
      "Episode: 1438 Total reward: 22.1 Training loss: 0.0232 Epsilon: 0.0101 Life: 300\n",
      "Episode Finish  [ 6.  5. 14.]\n",
      "Episode: 1439 Total reward: 8.299999999999999 Training loss: 0.0254 Epsilon: 0.0101 Life: 300\n",
      "Episode Finish  [14.  0. 24.]\n",
      "Episode: 1440 Total reward: 24.1 Training loss: 0.0282 Epsilon: 0.0101 Life: 300\n",
      "Episode Finish  [14.  0.  2.]\n",
      "Episode: 1441 Total reward: 23.6 Training loss: 0.0212 Epsilon: 0.0101 Life: 300\n",
      "Episode Finish  [16.  0.  8.]\n",
      "Episode: 1442 Total reward: 28.0 Training loss: 0.0178 Epsilon: 0.0101 Life: 300\n",
      "Episode Finish  [11.  0. 10.]\n",
      "Episode: 1443 Total reward: 17.900000000000002 Training loss: 0.0191 Epsilon: 0.0101 Life: 300\n",
      "Episode Finish  [9. 3. 8.]\n",
      "Episode: 1444 Total reward: 15.1 Training loss: 0.0152 Epsilon: 0.0101 Life: 300\n",
      "Episode Finish  [11.  0. 16.]\n",
      "Episode: 1445 Total reward: 17.9 Training loss: 0.0237 Epsilon: 0.0101 Life: 300\n",
      "Episode Finish  [8. 0. 8.]\n",
      "Episode: 1446 Total reward: 11.8 Training loss: 0.0185 Epsilon: 0.0101 Life: 300\n",
      "Episode Finish  [10.  2. 14.]\n",
      "Episode: 1447 Total reward: 16.0 Training loss: 0.0253 Epsilon: 0.0101 Life: 300\n",
      "Episode Finish  [ 8.  0. 40.]\n",
      "Episode: 1448 Total reward: 11.9 Training loss: 0.0180 Epsilon: 0.0101 Life: 300\n",
      "Episode Finish  [12.  0.  4.]\n",
      "Episode: 1449 Total reward: 19.700000000000003 Training loss: 0.0301 Epsilon: 0.0101 Life: 300\n",
      "Episode Finish  [14.  0. 44.]\n",
      "Episode: 1450 Total reward: 24.2 Training loss: 0.0183 Epsilon: 0.0101 Life: 300\n",
      "Stats Saved at 1450\n",
      "Episode Finish  [11.  0. 24.]\n",
      "Episode: 1451 Total reward: 18.099999999999998 Training loss: 0.0171 Epsilon: 0.0101 Life: 300\n",
      "Episode Finish  [16.  0.  4.]\n",
      "Episode: 1452 Total reward: 27.799999999999997 Training loss: 0.0209 Epsilon: 0.0101 Life: 300\n",
      "Episode Finish  [12.  0. 20.]\n",
      "Episode: 1453 Total reward: 19.9 Training loss: 0.0274 Epsilon: 0.0101 Life: 300\n",
      "Episode Finish  [10.  5. 16.]\n",
      "Episode: 1454 Total reward: 16.6 Training loss: 0.0211 Epsilon: 0.0101 Life: 300\n",
      "Episode Finish  [13.  0. 12.]\n",
      "Episode: 1455 Total reward: 21.700000000000003 Training loss: 0.0308 Epsilon: 0.0101 Life: 300\n",
      "Episode Finish  [12.  0.  4.]\n",
      "Episode: 1456 Total reward: 19.5 Training loss: 0.0304 Epsilon: 0.0101 Life: 300\n",
      "Model updated\n",
      "Episode Finish  [12.  2. 16.]\n",
      "Episode: 1457 Total reward: 19.6 Training loss: 0.0282 Epsilon: 0.0101 Life: 300\n",
      "Episode Finish  [15.  0. 10.]\n",
      "Episode: 1458 Total reward: 26.0 Training loss: 0.0216 Epsilon: 0.0101 Life: 300\n",
      "Episode Finish  [15.  0.  8.]\n",
      "Episode: 1459 Total reward: 25.8 Training loss: 0.0228 Epsilon: 0.0101 Life: 300\n",
      "Episode Finish  [15.  0. 24.]\n",
      "Episode: 1460 Total reward: 26.1 Training loss: 0.0185 Epsilon: 0.0101 Life: 300\n",
      "Episode Finish  [14.  0.  4.]\n",
      "Episode: 1461 Total reward: 23.9 Training loss: 0.0239 Epsilon: 0.0101 Life: 300\n",
      "Episode Finish  [13.  0. 18.]\n",
      "Episode: 1462 Total reward: 22.0 Training loss: 0.0229 Epsilon: 0.0101 Life: 300\n",
      "Episode Finish  [12.  6. 10.]\n",
      "Episode: 1463 Total reward: 20.6 Training loss: 0.0269 Epsilon: 0.0101 Life: 300\n",
      "Episode Finish  [10.  1. 16.]\n",
      "Episode: 1464 Total reward: 15.700000000000001 Training loss: 0.0296 Epsilon: 0.0101 Life: 300\n",
      "Episode Finish  [15.  1. 16.]\n",
      "Episode: 1465 Total reward: 25.9 Training loss: 0.0206 Epsilon: 0.0101 Life: 300\n",
      "Episode Finish  [ 5. 11. 24.]\n",
      "Episode: 1466 Total reward: 7.200000000000001 Training loss: 0.0346 Epsilon: 0.0101 Life: 300\n",
      "Episode Finish  [13.  0. 10.]\n",
      "Episode: 1467 Total reward: 22.0 Training loss: 0.0275 Epsilon: 0.0101 Life: 300\n",
      "Episode Finish  [12.  0. 24.]\n",
      "Episode: 1468 Total reward: 20.1 Training loss: 0.0175 Epsilon: 0.0101 Life: 300\n",
      "Episode Finish  [13.  0. 24.]\n",
      "Episode: 1469 Total reward: 22.1 Training loss: 0.0249 Epsilon: 0.0101 Life: 300\n",
      "Episode Finish  [7. 5. 8.]\n",
      "Episode: 1470 Total reward: 10.400000000000002 Training loss: 0.0264 Epsilon: 0.0101 Life: 300\n",
      "Episode Finish  [13.  0. 24.]\n",
      "Episode: 1471 Total reward: 22.1 Training loss: 0.0228 Epsilon: 0.0101 Life: 300\n",
      "Episode Finish  [10.  4. 10.]\n",
      "Episode: 1472 Total reward: 15.8 Training loss: 0.0329 Epsilon: 0.0101 Life: 300\n",
      "Episode Finish  [12.  0. 24.]\n",
      "Episode: 1473 Total reward: 20.1 Training loss: 0.0237 Epsilon: 0.0101 Life: 300\n",
      "Episode Finish  [11.  4.  4.]\n",
      "Episode: 1474 Total reward: 18.3 Training loss: 0.0277 Epsilon: 0.0101 Life: 300\n",
      "Episode Finish  [10.  1.  8.]\n",
      "Episode: 1475 Total reward: 15.900000000000002 Training loss: 0.0209 Epsilon: 0.0101 Life: 300\n",
      "Episode Finish  [12.  0. 10.]\n",
      "Episode: 1476 Total reward: 20.0 Training loss: 0.0306 Epsilon: 0.0101 Life: 300\n",
      "Episode Finish  [11.  0.  4.]\n",
      "Episode: 1477 Total reward: 17.299999999999997 Training loss: 0.0208 Epsilon: 0.0101 Life: 300\n",
      "Episode Finish  [11.  1. 16.]\n",
      "Episode: 1478 Total reward: 17.6 Training loss: 0.0222 Epsilon: 0.0101 Life: 300\n",
      "Episode Finish  [16.  0. 10.]\n",
      "Episode: 1479 Total reward: 28.000000000000004 Training loss: 0.0295 Epsilon: 0.0101 Life: 300\n",
      "Episode Finish  [16.  0.  4.]\n",
      "Episode: 1480 Total reward: 27.800000000000004 Training loss: 0.0161 Epsilon: 0.0101 Life: 300\n",
      "Episode Finish  [ 8. 10.  2.]\n",
      "Episode: 1481 Total reward: 12.8 Training loss: 0.0281 Epsilon: 0.0101 Life: 300\n",
      "Episode Finish  [11.  3. 12.]\n",
      "Episode: 1482 Total reward: 18.3 Training loss: 0.0303 Epsilon: 0.0101 Life: 300\n",
      "Episode Finish  [12.  0. 24.]\n",
      "Episode: 1483 Total reward: 20.1 Training loss: 0.0322 Epsilon: 0.0101 Life: 300\n",
      "Episode Finish  [12.  0.  4.]\n",
      "Episode: 1484 Total reward: 19.7 Training loss: 0.0325 Epsilon: 0.0101 Life: 300\n",
      "Episode Finish  [9. 5. 8.]\n",
      "Episode: 1485 Total reward: 14.500000000000002 Training loss: 0.0278 Epsilon: 0.0101 Life: 300\n",
      "Episode Finish  [15.  0. 10.]\n",
      "Episode: 1486 Total reward: 26.0 Training loss: 0.0248 Epsilon: 0.0101 Life: 300\n",
      "Episode Finish  [ 8.  7. 12.]\n",
      "Episode: 1487 Total reward: 12.700000000000001 Training loss: 0.0307 Epsilon: 0.0101 Life: 300\n",
      "Episode Finish  [14.  0. 24.]\n",
      "Episode: 1488 Total reward: 24.1 Training loss: 0.0222 Epsilon: 0.0101 Life: 300\n",
      "Episode Finish  [12.  0. 24.]\n",
      "Episode: 1489 Total reward: 20.1 Training loss: 0.0237 Epsilon: 0.0101 Life: 300\n",
      "Episode Finish  [14.  0.  8.]\n",
      "Episode: 1490 Total reward: 23.7 Training loss: 0.0235 Epsilon: 0.0101 Life: 300\n",
      "Episode Finish  [13.  0. 24.]\n",
      "Episode: 1491 Total reward: 22.1 Training loss: 0.0321 Epsilon: 0.0101 Life: 300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode Finish  [11.  0. 18.]\n",
      "Episode: 1492 Total reward: 18.0 Training loss: 0.0239 Epsilon: 0.0101 Life: 300\n",
      "Episode Finish  [ 9.  1. 18.]\n",
      "Episode: 1493 Total reward: 14.100000000000001 Training loss: 0.0210 Epsilon: 0.0101 Life: 300\n",
      "Episode Finish  [15.  0. 18.]\n",
      "Episode: 1494 Total reward: 26.1 Training loss: 0.0211 Epsilon: 0.0101 Life: 300\n",
      "Episode Finish  [16.  0. 24.]\n",
      "Episode: 1495 Total reward: 28.1 Training loss: 0.0184 Epsilon: 0.0101 Life: 300\n",
      "Episode Finish  [13.  0. 24.]\n",
      "Episode: 1496 Total reward: 22.099999999999998 Training loss: 0.0267 Epsilon: 0.0101 Life: 300\n",
      "Episode Finish  [9. 0. 2.]\n",
      "Episode: 1497 Total reward: 13.5 Training loss: 0.0202 Epsilon: 0.0101 Life: 300\n",
      "Episode Finish  [13.  2. 12.]\n",
      "Episode: 1498 Total reward: 21.799999999999997 Training loss: 0.0227 Epsilon: 0.0101 Life: 300\n",
      "Episode Finish  [ 8.  5. 10.]\n",
      "Episode: 1499 Total reward: 12.500000000000002 Training loss: 0.0220 Epsilon: 0.0101 Life: 300\n",
      "Episode Finish  [11.  0. 24.]\n",
      "Episode: 1500 Total reward: 18.099999999999998 Training loss: 0.0171 Epsilon: 0.0100 Life: 300\n",
      "Stats Saved at 1500\n",
      "Episode Finish  [7. 7. 8.]\n",
      "Episode: 1501 Total reward: 10.799999999999999 Training loss: 0.0175 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [14.  0. 24.]\n",
      "Episode: 1502 Total reward: 24.1 Training loss: 0.0184 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [12.  0. 22.]\n",
      "Episode: 1503 Total reward: 19.799999999999997 Training loss: 0.0290 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [13.  0. 24.]\n",
      "Episode: 1504 Total reward: 22.1 Training loss: 0.0208 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [12.  0. 24.]\n",
      "Episode: 1505 Total reward: 20.099999999999998 Training loss: 0.0226 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [12.  0.  6.]\n",
      "Episode: 1506 Total reward: 19.9 Training loss: 0.0177 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [16.  0. 24.]\n",
      "Episode: 1507 Total reward: 28.1 Training loss: 0.0271 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [10.  3. 24.]\n",
      "Episode: 1508 Total reward: 16.4 Training loss: 0.0230 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [12.  0. 14.]\n",
      "Episode: 1509 Total reward: 19.4 Training loss: 0.0247 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [15.  0. 18.]\n",
      "Episode: 1510 Total reward: 25.9 Training loss: 0.0166 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [13.  0. 24.]\n",
      "Episode: 1511 Total reward: 22.1 Training loss: 0.0210 Epsilon: 0.0100 Life: 300\n",
      "Model updated\n",
      "Episode Finish  [12.  0. 24.]\n",
      "Episode: 1512 Total reward: 20.1 Training loss: 0.0210 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [10.  0.  6.]\n",
      "Episode: 1513 Total reward: 15.600000000000001 Training loss: 0.0263 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [11.  0.  8.]\n",
      "Episode: 1514 Total reward: 17.2 Training loss: 0.0221 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [9. 0. 6.]\n",
      "Episode: 1515 Total reward: 13.799999999999999 Training loss: 0.0223 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [ 9.  3. 24.]\n",
      "Episode: 1516 Total reward: 14.399999999999999 Training loss: 0.0226 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [12.  0. 24.]\n",
      "Episode: 1517 Total reward: 20.099999999999998 Training loss: 0.0285 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [ 7.  0. 24.]\n",
      "Episode: 1518 Total reward: 10.099999999999998 Training loss: 0.0165 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [13.  0. 24.]\n",
      "Episode: 1519 Total reward: 22.1 Training loss: 0.0262 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [11.  0. 24.]\n",
      "Episode: 1520 Total reward: 18.1 Training loss: 0.0223 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [11.  0.  2.]\n",
      "Episode: 1521 Total reward: 18.0 Training loss: 0.0239 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [11.  0.  4.]\n",
      "Episode: 1522 Total reward: 18.0 Training loss: 0.0322 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [13.  0. 24.]\n",
      "Episode: 1523 Total reward: 22.1 Training loss: 0.0288 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [ 9.  0. 12.]\n",
      "Episode: 1524 Total reward: 13.899999999999999 Training loss: 0.0242 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [8. 2. 4.]\n",
      "Episode: 1525 Total reward: 12.1 Training loss: 0.0216 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [11.  0.  6.]\n",
      "Episode: 1526 Total reward: 18.0 Training loss: 0.0259 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [11.  0.  6.]\n",
      "Episode: 1527 Total reward: 17.9 Training loss: 0.0240 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [ 7.  0. 24.]\n",
      "Episode: 1528 Total reward: 10.100000000000003 Training loss: 0.0173 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [10.  0.  6.]\n",
      "Episode: 1529 Total reward: 15.9 Training loss: 0.0217 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [12.  0. 24.]\n",
      "Episode: 1530 Total reward: 20.1 Training loss: 0.0263 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [13.  0. 24.]\n",
      "Episode: 1531 Total reward: 22.1 Training loss: 0.0182 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [10.  0. 10.]\n",
      "Episode: 1532 Total reward: 15.6 Training loss: 0.0210 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [7. 0. 8.]\n",
      "Episode: 1533 Total reward: 9.799999999999999 Training loss: 0.0216 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [10.  0. 16.]\n",
      "Episode: 1534 Total reward: 15.8 Training loss: 0.0176 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [12.  1. 24.]\n",
      "Episode: 1535 Total reward: 20.200000000000003 Training loss: 0.0324 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [ 9.  0. 14.]\n",
      "Episode: 1536 Total reward: 13.8 Training loss: 0.0249 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [13.  0. 20.]\n",
      "Episode: 1537 Total reward: 21.9 Training loss: 0.0290 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [ 9.  0. 14.]\n",
      "Episode: 1538 Total reward: 13.5 Training loss: 0.0292 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [13.  0. 24.]\n",
      "Episode: 1539 Total reward: 22.1 Training loss: 0.0261 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [11.  0. 10.]\n",
      "Episode: 1540 Total reward: 18.0 Training loss: 0.0278 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [13.  0. 24.]\n",
      "Episode: 1541 Total reward: 22.1 Training loss: 0.0235 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [11.  0. 24.]\n",
      "Episode: 1542 Total reward: 18.1 Training loss: 0.0277 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [10.  0. 22.]\n",
      "Episode: 1543 Total reward: 15.3 Training loss: 0.0204 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [13.  0. 24.]\n",
      "Episode: 1544 Total reward: 22.1 Training loss: 0.0215 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [9. 7. 6.]\n",
      "Episode: 1545 Total reward: 14.600000000000001 Training loss: 0.0247 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [ 7. 10. 24.]\n",
      "Episode: 1546 Total reward: 11.100000000000001 Training loss: 0.0245 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [ 9.  2. 24.]\n",
      "Episode: 1547 Total reward: 14.3 Training loss: 0.0258 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [12.  0. 24.]\n",
      "Episode: 1548 Total reward: 20.1 Training loss: 0.0169 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [12.  0.  6.]\n",
      "Episode: 1549 Total reward: 19.8 Training loss: 0.0215 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [12.  0. 10.]\n",
      "Episode: 1550 Total reward: 20.0 Training loss: 0.0315 Epsilon: 0.0100 Life: 300\n",
      "Stats Saved at 1550\n",
      "Episode Finish  [12.  0. 20.]\n",
      "Episode: 1551 Total reward: 20.2 Training loss: 0.0258 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [12.  0. 12.]\n",
      "Episode: 1552 Total reward: 19.9 Training loss: 0.0172 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [14.  0. 16.]\n",
      "Episode: 1553 Total reward: 24.0 Training loss: 0.0210 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [11.  0. 10.]\n",
      "Episode: 1554 Total reward: 18.0 Training loss: 0.0186 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [ 8.  4. 24.]\n",
      "Episode: 1555 Total reward: 12.5 Training loss: 0.0313 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [11.  0. 20.]\n",
      "Episode: 1556 Total reward: 18.1 Training loss: 0.0223 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [12.  0.  8.]\n",
      "Episode: 1557 Total reward: 19.8 Training loss: 0.0247 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [11.  0. 24.]\n",
      "Episode: 1558 Total reward: 18.1 Training loss: 0.0292 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [8. 0. 8.]\n",
      "Episode: 1559 Total reward: 11.800000000000002 Training loss: 0.0285 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [11.  0. 24.]\n",
      "Episode: 1560 Total reward: 18.099999999999998 Training loss: 0.0251 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [15.  0. 16.]\n",
      "Episode: 1561 Total reward: 25.8 Training loss: 0.0305 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [12.  0. 24.]\n",
      "Episode: 1562 Total reward: 20.099999999999998 Training loss: 0.0237 Epsilon: 0.0100 Life: 300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode Finish  [12.  0. 24.]\n",
      "Episode: 1563 Total reward: 20.1 Training loss: 0.0199 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [11.  0.  8.]\n",
      "Episode: 1564 Total reward: 17.5 Training loss: 0.0234 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [10.  0.  6.]\n",
      "Episode: 1565 Total reward: 15.899999999999999 Training loss: 0.0240 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [14.  0.  4.]\n",
      "Episode: 1566 Total reward: 23.999999999999996 Training loss: 0.0224 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [11.  0. 16.]\n",
      "Episode: 1567 Total reward: 17.9 Training loss: 0.0203 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [11.  0.  4.]\n",
      "Episode: 1568 Total reward: 17.9 Training loss: 0.0253 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [ 8.  0. 10.]\n",
      "Episode: 1569 Total reward: 12.0 Training loss: 0.0210 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [12.  0. 24.]\n",
      "Episode: 1570 Total reward: 20.1 Training loss: 0.0264 Epsilon: 0.0100 Life: 300\n",
      "Model updated\n",
      "Episode Finish  [14.  0. 24.]\n",
      "Episode: 1571 Total reward: 24.099999999999998 Training loss: 0.0211 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [ 7. 11. 24.]\n",
      "Episode: 1572 Total reward: 11.2 Training loss: 0.0202 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [ 7.  8. 20.]\n",
      "Episode: 1573 Total reward: 10.9 Training loss: 0.0310 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [17.  0.  8.]\n",
      "Episode: 1574 Total reward: 29.9 Training loss: 0.0197 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [15.  0. 24.]\n",
      "Episode: 1575 Total reward: 26.1 Training loss: 0.0214 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [10.  0.  4.]\n",
      "Episode: 1576 Total reward: 15.600000000000001 Training loss: 0.0250 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [14.  0.  4.]\n",
      "Episode: 1577 Total reward: 23.9 Training loss: 0.0181 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [10.  0.  4.]\n",
      "Episode: 1578 Total reward: 15.799999999999999 Training loss: 0.0288 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [13.  0. 24.]\n",
      "Episode: 1579 Total reward: 22.1 Training loss: 0.0163 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [11.  0. 10.]\n",
      "Episode: 1580 Total reward: 18.0 Training loss: 0.0258 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [12.  0. 24.]\n",
      "Episode: 1581 Total reward: 20.1 Training loss: 0.0272 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [9. 0. 2.]\n",
      "Episode: 1582 Total reward: 13.299999999999999 Training loss: 0.0189 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [9. 0. 8.]\n",
      "Episode: 1583 Total reward: 13.8 Training loss: 0.0168 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [10.  0. 10.]\n",
      "Episode: 1584 Total reward: 16.0 Training loss: 0.0198 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [ 6.  6. 16.]\n",
      "Episode: 1585 Total reward: 8.6 Training loss: 0.0191 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [11.  0. 10.]\n",
      "Episode: 1586 Total reward: 18.0 Training loss: 0.0202 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [12.  0. 44.]\n",
      "Episode: 1587 Total reward: 20.2 Training loss: 0.0354 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [12.  0. 36.]\n",
      "Episode: 1588 Total reward: 20.200000000000003 Training loss: 0.0195 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [11.  0.  4.]\n",
      "Episode: 1589 Total reward: 17.799999999999997 Training loss: 0.0210 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [11.  0. 24.]\n",
      "Episode: 1590 Total reward: 17.900000000000002 Training loss: 0.0192 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [13.  0. 24.]\n",
      "Episode: 1591 Total reward: 22.1 Training loss: 0.0204 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [12.  0. 24.]\n",
      "Episode: 1592 Total reward: 20.1 Training loss: 0.0295 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [10.  0.  8.]\n",
      "Episode: 1593 Total reward: 15.6 Training loss: 0.0243 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [10.  0. 12.]\n",
      "Episode: 1594 Total reward: 15.600000000000001 Training loss: 0.0258 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [12.  0.  2.]\n",
      "Episode: 1595 Total reward: 19.5 Training loss: 0.0241 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [13.  0. 24.]\n",
      "Episode: 1596 Total reward: 22.1 Training loss: 0.0228 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [ 8.  0. 10.]\n",
      "Episode: 1597 Total reward: 11.7 Training loss: 0.0213 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [13.  0. 24.]\n",
      "Episode: 1598 Total reward: 22.1 Training loss: 0.0212 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [11.  2.  4.]\n",
      "Episode: 1599 Total reward: 18.200000000000003 Training loss: 0.0167 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [10.  0. 24.]\n",
      "Episode: 1600 Total reward: 16.1 Training loss: 0.0264 Epsilon: 0.0100 Life: 300\n",
      "Model Saved at 1600\n",
      "Stats Saved at 1600\n",
      "Episode Finish  [12.  0. 24.]\n",
      "Episode: 1601 Total reward: 20.1 Training loss: 0.0190 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [10.  0.  4.]\n",
      "Episode: 1602 Total reward: 16.1 Training loss: 0.0145 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [11.  0.  4.]\n",
      "Episode: 1603 Total reward: 17.799999999999997 Training loss: 0.0174 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [11.  0.  6.]\n",
      "Episode: 1604 Total reward: 17.6 Training loss: 0.0277 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [13.  0. 24.]\n",
      "Episode: 1605 Total reward: 22.1 Training loss: 0.0163 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [8. 0. 8.]\n",
      "Episode: 1606 Total reward: 11.799999999999999 Training loss: 0.0194 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [14.  0.  4.]\n",
      "Episode: 1607 Total reward: 24.0 Training loss: 0.0297 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [11.  0.  4.]\n",
      "Episode: 1608 Total reward: 17.8 Training loss: 0.0248 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [ 9.  4. 16.]\n",
      "Episode: 1609 Total reward: 14.5 Training loss: 0.0167 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [8. 0. 8.]\n",
      "Episode: 1610 Total reward: 11.8 Training loss: 0.0247 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [12.  0. 10.]\n",
      "Episode: 1611 Total reward: 20.0 Training loss: 0.0302 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [11.  0. 20.]\n",
      "Episode: 1612 Total reward: 18.2 Training loss: 0.0169 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [11.  0.  8.]\n",
      "Episode: 1613 Total reward: 18.0 Training loss: 0.0176 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [13.  0. 24.]\n",
      "Episode: 1614 Total reward: 21.900000000000002 Training loss: 0.0162 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [13.  0. 24.]\n",
      "Episode: 1615 Total reward: 22.1 Training loss: 0.0148 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [13.  0. 24.]\n",
      "Episode: 1616 Total reward: 22.1 Training loss: 0.0189 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [13.  0. 24.]\n",
      "Episode: 1617 Total reward: 22.099999999999998 Training loss: 0.0216 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [12.  0. 24.]\n",
      "Episode: 1618 Total reward: 20.1 Training loss: 0.0192 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [10.  3. 14.]\n",
      "Episode: 1619 Total reward: 16.3 Training loss: 0.0135 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [13.  0.  6.]\n",
      "Episode: 1620 Total reward: 22.0 Training loss: 0.0239 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [14.  0.  6.]\n",
      "Episode: 1621 Total reward: 24.0 Training loss: 0.0178 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [ 8.  0. 10.]\n",
      "Episode: 1622 Total reward: 12.0 Training loss: 0.0178 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [12.  0.  4.]\n",
      "Episode: 1623 Total reward: 20.0 Training loss: 0.0174 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [ 7.  2. 24.]\n",
      "Episode: 1624 Total reward: 10.300000000000002 Training loss: 0.0230 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [11.  0. 10.]\n",
      "Episode: 1625 Total reward: 18.0 Training loss: 0.0270 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [13.  0.  6.]\n",
      "Episode: 1626 Total reward: 21.4 Training loss: 0.0165 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [12.  0. 10.]\n",
      "Episode: 1627 Total reward: 20.0 Training loss: 0.0187 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [ 9.  3. 20.]\n",
      "Episode: 1628 Total reward: 14.3 Training loss: 0.0224 Epsilon: 0.0100 Life: 300\n",
      "Model updated\n",
      "Episode Finish  [11.  0. 20.]\n",
      "Episode: 1629 Total reward: 17.7 Training loss: 0.0234 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [14.  0. 24.]\n",
      "Episode: 1630 Total reward: 24.099999999999998 Training loss: 0.0269 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [13.  0. 10.]\n",
      "Episode: 1631 Total reward: 22.0 Training loss: 0.0246 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [14.  0.  8.]\n",
      "Episode: 1632 Total reward: 23.799999999999997 Training loss: 0.0277 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [13.  0. 24.]\n",
      "Episode: 1633 Total reward: 22.1 Training loss: 0.0248 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [13.  0. 36.]\n",
      "Episode: 1634 Total reward: 22.1 Training loss: 0.0240 Epsilon: 0.0100 Life: 300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode Finish  [11.  0. 16.]\n",
      "Episode: 1635 Total reward: 17.7 Training loss: 0.0314 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [11.  0. 28.]\n",
      "Episode: 1636 Total reward: 18.1 Training loss: 0.0252 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [11.  0. 12.]\n",
      "Episode: 1637 Total reward: 17.8 Training loss: 0.0260 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [14.  0. 24.]\n",
      "Episode: 1638 Total reward: 24.1 Training loss: 0.0238 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [10.  0. 50.]\n",
      "Episode: 1639 Total reward: 16.0 Training loss: 0.0265 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [15.  0.  6.]\n",
      "Episode: 1640 Total reward: 26.0 Training loss: 0.0186 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [10.  0.  6.]\n",
      "Episode: 1641 Total reward: 15.400000000000002 Training loss: 0.0312 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [17.  0. 20.]\n",
      "Episode: 1642 Total reward: 29.7 Training loss: 0.0190 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [15.  0.  6.]\n",
      "Episode: 1643 Total reward: 25.800000000000004 Training loss: 0.0227 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [13.  0.  4.]\n",
      "Episode: 1644 Total reward: 21.900000000000002 Training loss: 0.0192 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [14.  1. 24.]\n",
      "Episode: 1645 Total reward: 24.2 Training loss: 0.0266 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [12.  1.  4.]\n",
      "Episode: 1646 Total reward: 20.0 Training loss: 0.0233 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [14.  0. 10.]\n",
      "Episode: 1647 Total reward: 23.3 Training loss: 0.0155 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [10.  0.  4.]\n",
      "Episode: 1648 Total reward: 15.7 Training loss: 0.0244 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [12.  0. 34.]\n",
      "Episode: 1649 Total reward: 19.7 Training loss: 0.0172 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [ 8.  7. 20.]\n",
      "Episode: 1650 Total reward: 12.6 Training loss: 0.0162 Epsilon: 0.0100 Life: 300\n",
      "Stats Saved at 1650\n",
      "Episode Finish  [12.  0. 24.]\n",
      "Episode: 1651 Total reward: 20.1 Training loss: 0.0291 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [13.  0.  2.]\n",
      "Episode: 1652 Total reward: 21.400000000000002 Training loss: 0.0180 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [15.  0. 24.]\n",
      "Episode: 1653 Total reward: 26.099999999999998 Training loss: 0.0193 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [14.  0.  8.]\n",
      "Episode: 1654 Total reward: 23.8 Training loss: 0.0136 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [14.  0. 12.]\n",
      "Episode: 1655 Total reward: 23.299999999999997 Training loss: 0.0310 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [12.  0. 24.]\n",
      "Episode: 1656 Total reward: 20.1 Training loss: 0.0179 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [15.  0.  8.]\n",
      "Episode: 1657 Total reward: 25.900000000000006 Training loss: 0.0234 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [12.  0.  4.]\n",
      "Episode: 1658 Total reward: 19.9 Training loss: 0.0223 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [10.  1.  2.]\n",
      "Episode: 1659 Total reward: 16.0 Training loss: 0.0261 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [9. 0. 4.]\n",
      "Episode: 1660 Total reward: 13.900000000000002 Training loss: 0.0181 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [15.  0. 24.]\n",
      "Episode: 1661 Total reward: 26.1 Training loss: 0.0228 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [14.  0. 40.]\n",
      "Episode: 1662 Total reward: 24.199999999999996 Training loss: 0.0169 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [10.  0. 20.]\n",
      "Episode: 1663 Total reward: 15.8 Training loss: 0.0269 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [14.  0. 24.]\n",
      "Episode: 1664 Total reward: 24.1 Training loss: 0.0254 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [13.  0. 24.]\n",
      "Episode: 1665 Total reward: 22.1 Training loss: 0.0245 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [11.  1.  6.]\n",
      "Episode: 1666 Total reward: 17.8 Training loss: 0.0210 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [12.  0. 14.]\n",
      "Episode: 1667 Total reward: 20.0 Training loss: 0.0255 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [14.  0. 12.]\n",
      "Episode: 1668 Total reward: 23.700000000000003 Training loss: 0.0201 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [16.  0. 24.]\n",
      "Episode: 1669 Total reward: 28.099999999999998 Training loss: 0.0302 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [8. 8. 2.]\n",
      "Episode: 1670 Total reward: 12.499999999999998 Training loss: 0.0274 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [14.  0. 24.]\n",
      "Episode: 1671 Total reward: 24.1 Training loss: 0.0266 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [12.  0. 24.]\n",
      "Episode: 1672 Total reward: 20.1 Training loss: 0.0224 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [12.  0. 24.]\n",
      "Episode: 1673 Total reward: 20.099999999999998 Training loss: 0.0236 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [12.  0. 24.]\n",
      "Episode: 1674 Total reward: 20.1 Training loss: 0.0196 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [15.  0. 10.]\n",
      "Episode: 1675 Total reward: 26.0 Training loss: 0.0188 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [11.  0. 16.]\n",
      "Episode: 1676 Total reward: 18.0 Training loss: 0.0254 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [11.  0.  6.]\n",
      "Episode: 1677 Total reward: 17.9 Training loss: 0.0171 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [11.  0.  4.]\n",
      "Episode: 1678 Total reward: 17.799999999999997 Training loss: 0.0230 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [7. 8. 8.]\n",
      "Episode: 1679 Total reward: 10.5 Training loss: 0.0191 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [15.  0. 20.]\n",
      "Episode: 1680 Total reward: 25.9 Training loss: 0.0205 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [13.  0.  8.]\n",
      "Episode: 1681 Total reward: 21.800000000000004 Training loss: 0.0196 Epsilon: 0.0100 Life: 300\n",
      "Model updated\n",
      "Episode Finish  [10.  0.  4.]\n",
      "Episode: 1682 Total reward: 15.8 Training loss: 0.0168 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [12.  0. 24.]\n",
      "Episode: 1683 Total reward: 20.099999999999998 Training loss: 0.0250 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [14.  0. 24.]\n",
      "Episode: 1684 Total reward: 24.1 Training loss: 0.0215 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [13.  0. 24.]\n",
      "Episode: 1685 Total reward: 22.1 Training loss: 0.0188 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [13.  0. 24.]\n",
      "Episode: 1686 Total reward: 22.1 Training loss: 0.0285 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [17.  0.  4.]\n",
      "Episode: 1687 Total reward: 29.9 Training loss: 0.0231 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [16.  0. 32.]\n",
      "Episode: 1688 Total reward: 27.799999999999997 Training loss: 0.0343 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [ 7. 12. 24.]\n",
      "Episode: 1689 Total reward: 11.300000000000002 Training loss: 0.0208 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [12.  0. 10.]\n",
      "Episode: 1690 Total reward: 20.0 Training loss: 0.0229 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [ 9.  1. 24.]\n",
      "Episode: 1691 Total reward: 14.2 Training loss: 0.0212 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [11.  5. 28.]\n",
      "Episode: 1692 Total reward: 18.5 Training loss: 0.0166 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [ 7. 10. 28.]\n",
      "Episode: 1693 Total reward: 11.1 Training loss: 0.0245 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [16.  0.  2.]\n",
      "Episode: 1694 Total reward: 27.5 Training loss: 0.0210 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [13.  1.  8.]\n",
      "Episode: 1695 Total reward: 21.9 Training loss: 0.0281 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [14.  0. 20.]\n",
      "Episode: 1696 Total reward: 23.799999999999997 Training loss: 0.0217 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [12.  0.  8.]\n",
      "Episode: 1697 Total reward: 19.799999999999997 Training loss: 0.0241 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [10.  3. 16.]\n",
      "Episode: 1698 Total reward: 16.299999999999997 Training loss: 0.0254 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [12.  0.  2.]\n",
      "Episode: 1699 Total reward: 19.6 Training loss: 0.0238 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [10.  4. 24.]\n",
      "Episode: 1700 Total reward: 16.5 Training loss: 0.0167 Epsilon: 0.0100 Life: 300\n",
      "Stats Saved at 1700\n",
      "Episode Finish  [ 7. 12. 24.]\n",
      "Episode: 1701 Total reward: 11.3 Training loss: 0.0286 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [8. 4. 4.]\n",
      "Episode: 1702 Total reward: 12.2 Training loss: 0.0208 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [ 8.  9. 24.]\n",
      "Episode: 1703 Total reward: 13.0 Training loss: 0.0222 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [13.  0. 24.]\n",
      "Episode: 1704 Total reward: 22.1 Training loss: 0.0233 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [15.  0.  2.]\n",
      "Episode: 1705 Total reward: 25.799999999999997 Training loss: 0.0300 Epsilon: 0.0100 Life: 300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode Finish  [15.  0. 18.]\n",
      "Episode: 1706 Total reward: 25.7 Training loss: 0.0234 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [12.  0. 24.]\n",
      "Episode: 1707 Total reward: 20.1 Training loss: 0.0297 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [12.  0.  8.]\n",
      "Episode: 1708 Total reward: 20.1 Training loss: 0.0198 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [11.  0. 14.]\n",
      "Episode: 1709 Total reward: 17.799999999999997 Training loss: 0.0249 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [10.  1. 12.]\n",
      "Episode: 1710 Total reward: 16.2 Training loss: 0.0179 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [ 7. 15. 24.]\n",
      "Episode: 1711 Total reward: 11.6 Training loss: 0.0239 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [9. 3. 2.]\n",
      "Episode: 1712 Total reward: 14.200000000000003 Training loss: 0.0200 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [11.  0.  6.]\n",
      "Episode: 1713 Total reward: 17.9 Training loss: 0.0181 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [10.  0. 10.]\n",
      "Episode: 1714 Total reward: 16.0 Training loss: 0.0297 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [ 7. 11. 24.]\n",
      "Episode: 1715 Total reward: 11.200000000000001 Training loss: 0.0228 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [14.  3. 14.]\n",
      "Episode: 1716 Total reward: 24.3 Training loss: 0.0200 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [11.  0. 36.]\n",
      "Episode: 1717 Total reward: 18.0 Training loss: 0.0181 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [11.  1. 20.]\n",
      "Episode: 1718 Total reward: 18.200000000000003 Training loss: 0.0218 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [13.  0.  2.]\n",
      "Episode: 1719 Total reward: 21.499999999999996 Training loss: 0.0375 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [11.  5. 10.]\n",
      "Episode: 1720 Total reward: 18.499999999999996 Training loss: 0.0216 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [12.  0.  6.]\n",
      "Episode: 1721 Total reward: 19.6 Training loss: 0.0247 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [9. 3. 8.]\n",
      "Episode: 1722 Total reward: 14.0 Training loss: 0.0210 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [14.  0. 24.]\n",
      "Episode: 1723 Total reward: 24.1 Training loss: 0.0249 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [15.  0.  8.]\n",
      "Episode: 1724 Total reward: 25.999999999999996 Training loss: 0.0183 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [10.  6. 16.]\n",
      "Episode: 1725 Total reward: 16.7 Training loss: 0.0237 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [10.  3.  8.]\n",
      "Episode: 1726 Total reward: 16.1 Training loss: 0.0144 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [13.  0. 20.]\n",
      "Episode: 1727 Total reward: 22.1 Training loss: 0.0163 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [13.  1. 24.]\n",
      "Episode: 1728 Total reward: 22.200000000000003 Training loss: 0.0296 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [12.  1. 24.]\n",
      "Episode: 1729 Total reward: 20.200000000000003 Training loss: 0.0247 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [15.  0.  6.]\n",
      "Episode: 1730 Total reward: 25.9 Training loss: 0.0237 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [19.  0. 10.]\n",
      "Episode: 1731 Total reward: 34.0 Training loss: 0.0190 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [8. 8. 8.]\n",
      "Episode: 1732 Total reward: 12.399999999999999 Training loss: 0.0192 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [11.  0.  8.]\n",
      "Episode: 1733 Total reward: 17.799999999999997 Training loss: 0.0216 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [10.  0. 10.]\n",
      "Episode: 1734 Total reward: 16.0 Training loss: 0.0265 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [ 5. 20. 24.]\n",
      "Episode: 1735 Total reward: 8.1 Training loss: 0.0223 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [12.  3. 16.]\n",
      "Episode: 1736 Total reward: 20.1 Training loss: 0.0230 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [13.  1. 10.]\n",
      "Episode: 1737 Total reward: 22.1 Training loss: 0.0244 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [ 2. 20. 24.]\n",
      "Episode: 1738 Total reward: 2.1 Training loss: 0.0228 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [12. 10. 14.]\n",
      "Episode: 1739 Total reward: 21.0 Training loss: 0.0157 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [10.  5.  8.]\n",
      "Episode: 1740 Total reward: 16.200000000000003 Training loss: 0.0195 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [17.  0.  4.]\n",
      "Episode: 1741 Total reward: 29.799999999999997 Training loss: 0.0160 Epsilon: 0.0100 Life: 300\n",
      "Model updated\n",
      "Episode Finish  [15.  0. 36.]\n",
      "Episode: 1742 Total reward: 26.200000000000003 Training loss: 0.0247 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [9. 5. 6.]\n",
      "Episode: 1743 Total reward: 14.5 Training loss: 0.0275 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [10.  6.  4.]\n",
      "Episode: 1744 Total reward: 16.7 Training loss: 0.0189 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [9. 1. 4.]\n",
      "Episode: 1745 Total reward: 13.899999999999999 Training loss: 0.0367 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [14.  0. 14.]\n",
      "Episode: 1746 Total reward: 23.700000000000003 Training loss: 0.0265 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [12.  5. 12.]\n",
      "Episode: 1747 Total reward: 20.5 Training loss: 0.0199 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [9. 5. 4.]\n",
      "Episode: 1748 Total reward: 14.100000000000001 Training loss: 0.0254 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [13.  0. 24.]\n",
      "Episode: 1749 Total reward: 22.1 Training loss: 0.0229 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [12.  6. 10.]\n",
      "Episode: 1750 Total reward: 20.6 Training loss: 0.0250 Epsilon: 0.0100 Life: 300\n",
      "Stats Saved at 1750\n",
      "Episode Finish  [ 7.  6. 24.]\n",
      "Episode: 1751 Total reward: 10.7 Training loss: 0.0260 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [11.  5.  4.]\n",
      "Episode: 1752 Total reward: 18.4 Training loss: 0.0423 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [15.  0. 16.]\n",
      "Episode: 1753 Total reward: 25.6 Training loss: 0.0325 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [12.  0. 12.]\n",
      "Episode: 1754 Total reward: 20.0 Training loss: 0.0225 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [13.  0. 24.]\n",
      "Episode: 1755 Total reward: 22.1 Training loss: 0.0260 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [12.  0. 24.]\n",
      "Episode: 1756 Total reward: 20.1 Training loss: 0.0204 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [10.  7. 28.]\n",
      "Episode: 1757 Total reward: 16.8 Training loss: 0.0209 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [14.  0. 24.]\n",
      "Episode: 1758 Total reward: 24.099999999999998 Training loss: 0.0336 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [ 9.  1. 16.]\n",
      "Episode: 1759 Total reward: 13.5 Training loss: 0.0277 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [14.  0. 32.]\n",
      "Episode: 1760 Total reward: 24.0 Training loss: 0.0328 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [11.  7. 24.]\n",
      "Episode: 1761 Total reward: 18.8 Training loss: 0.0189 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [13.  0.  4.]\n",
      "Episode: 1762 Total reward: 21.799999999999997 Training loss: 0.0130 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [11.  0. 12.]\n",
      "Episode: 1763 Total reward: 17.8 Training loss: 0.0261 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [15.  0. 24.]\n",
      "Episode: 1764 Total reward: 26.1 Training loss: 0.0215 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [13.  0. 24.]\n",
      "Episode: 1765 Total reward: 22.1 Training loss: 0.0225 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [13.  0.  4.]\n",
      "Episode: 1766 Total reward: 21.900000000000002 Training loss: 0.0262 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [14.  0. 24.]\n",
      "Episode: 1767 Total reward: 24.099999999999998 Training loss: 0.0184 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [14.  0. 24.]\n",
      "Episode: 1768 Total reward: 24.099999999999998 Training loss: 0.0207 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [10. 10. 10.]\n",
      "Episode: 1769 Total reward: 17.0 Training loss: 0.0204 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [10.  5.  4.]\n",
      "Episode: 1770 Total reward: 16.3 Training loss: 0.0206 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [12.  3.  4.]\n",
      "Episode: 1771 Total reward: 19.8 Training loss: 0.0226 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [11.  0. 28.]\n",
      "Episode: 1772 Total reward: 18.200000000000003 Training loss: 0.0253 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [13.  0.  2.]\n",
      "Episode: 1773 Total reward: 21.700000000000003 Training loss: 0.0212 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [11.  7. 16.]\n",
      "Episode: 1774 Total reward: 18.7 Training loss: 0.0291 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [11.  0. 24.]\n",
      "Episode: 1775 Total reward: 18.1 Training loss: 0.0242 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [16.  0. 24.]\n",
      "Episode: 1776 Total reward: 28.2 Training loss: 0.0244 Epsilon: 0.0100 Life: 300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode Finish  [12.  9.  4.]\n",
      "Episode: 1777 Total reward: 20.700000000000003 Training loss: 0.0377 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [16.  0. 24.]\n",
      "Episode: 1778 Total reward: 28.1 Training loss: 0.0226 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [12.  2.  4.]\n",
      "Episode: 1779 Total reward: 20.099999999999998 Training loss: 0.0238 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [14.  3. 24.]\n",
      "Episode: 1780 Total reward: 25.4 Training loss: 0.0246 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [9. 4. 4.]\n",
      "Episode: 1781 Total reward: 15.4 Training loss: 0.0260 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [13.  1. 24.]\n",
      "Episode: 1782 Total reward: 22.199999999999996 Training loss: 0.0191 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [12.  7. 24.]\n",
      "Episode: 1783 Total reward: 20.8 Training loss: 0.0221 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [ 6. 13. 24.]\n",
      "Episode: 1784 Total reward: 9.4 Training loss: 0.0251 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [13.  3. 24.]\n",
      "Episode: 1785 Total reward: 22.4 Training loss: 0.0322 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [13.  0. 24.]\n",
      "Episode: 1786 Total reward: 22.1 Training loss: 0.0168 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [ 8.  5. 24.]\n",
      "Episode: 1787 Total reward: 12.6 Training loss: 0.0215 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [12.  0.  2.]\n",
      "Episode: 1788 Total reward: 19.799999999999997 Training loss: 0.0190 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [13.  0. 24.]\n",
      "Episode: 1789 Total reward: 22.1 Training loss: 0.0206 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [12.  0.  6.]\n",
      "Episode: 1790 Total reward: 19.8 Training loss: 0.0266 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [12.  6. 12.]\n",
      "Episode: 1791 Total reward: 20.5 Training loss: 0.0198 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [15.  0. 24.]\n",
      "Episode: 1792 Total reward: 26.1 Training loss: 0.0197 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [13.  0. 10.]\n",
      "Episode: 1793 Total reward: 21.6 Training loss: 0.0169 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [10.  5.  2.]\n",
      "Episode: 1794 Total reward: 16.0 Training loss: 0.0219 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [11.  0. 60.]\n",
      "Episode: 1795 Total reward: 18.2 Training loss: 0.0160 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [ 7. 14. 24.]\n",
      "Episode: 1796 Total reward: 11.5 Training loss: 0.0240 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [16.  0. 24.]\n",
      "Episode: 1797 Total reward: 28.1 Training loss: 0.0173 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [15.  0. 16.]\n",
      "Episode: 1798 Total reward: 26.0 Training loss: 0.0244 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [13.  0. 24.]\n",
      "Episode: 1799 Total reward: 22.1 Training loss: 0.0205 Epsilon: 0.0100 Life: 300\n",
      "Model updated\n",
      "Episode Finish  [ 8.  2. 28.]\n",
      "Episode: 1800 Total reward: 12.2 Training loss: 0.0186 Epsilon: 0.0100 Life: 300\n",
      "Model Saved at 1800\n",
      "Stats Saved at 1800\n",
      "Episode Finish  [9. 1. 8.]\n",
      "Episode: 1801 Total reward: 13.899999999999999 Training loss: 0.0203 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [ 7. 11. 14.]\n",
      "Episode: 1802 Total reward: 11.1 Training loss: 0.0213 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [8. 0. 8.]\n",
      "Episode: 1803 Total reward: 12.0 Training loss: 0.0155 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [13.  0. 24.]\n",
      "Episode: 1804 Total reward: 22.1 Training loss: 0.0218 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [ 9.  0. 12.]\n",
      "Episode: 1805 Total reward: 14.0 Training loss: 0.0210 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [10.  0.  8.]\n",
      "Episode: 1806 Total reward: 15.799999999999999 Training loss: 0.0191 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [12.  0. 24.]\n",
      "Episode: 1807 Total reward: 20.099999999999998 Training loss: 0.0242 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [11.  0.  4.]\n",
      "Episode: 1808 Total reward: 17.8 Training loss: 0.0222 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [10.  0.  4.]\n",
      "Episode: 1809 Total reward: 16.0 Training loss: 0.0300 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [ 6. 14. 24.]\n",
      "Episode: 1810 Total reward: 9.5 Training loss: 0.0244 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [10.  2.  6.]\n",
      "Episode: 1811 Total reward: 16.0 Training loss: 0.0251 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [12.  0. 18.]\n",
      "Episode: 1812 Total reward: 20.0 Training loss: 0.0199 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [14.  0.  8.]\n",
      "Episode: 1813 Total reward: 23.800000000000004 Training loss: 0.0274 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [11.  0.  6.]\n",
      "Episode: 1814 Total reward: 17.8 Training loss: 0.0131 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [14.  0. 24.]\n",
      "Episode: 1815 Total reward: 23.799999999999997 Training loss: 0.0235 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [13.  0. 24.]\n",
      "Episode: 1816 Total reward: 22.1 Training loss: 0.0219 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [12.  0. 24.]\n",
      "Episode: 1817 Total reward: 20.1 Training loss: 0.0188 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [12.  2.  2.]\n",
      "Episode: 1818 Total reward: 20.9 Training loss: 0.0166 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [11.  0. 24.]\n",
      "Episode: 1819 Total reward: 18.1 Training loss: 0.0164 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [8. 0. 8.]\n",
      "Episode: 1820 Total reward: 11.8 Training loss: 0.0229 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [12.  0. 24.]\n",
      "Episode: 1821 Total reward: 20.099999999999998 Training loss: 0.0202 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [13.  0. 24.]\n",
      "Episode: 1822 Total reward: 22.1 Training loss: 0.0166 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [ 8.  9. 24.]\n",
      "Episode: 1823 Total reward: 13.000000000000002 Training loss: 0.0234 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [ 8.  7. 32.]\n",
      "Episode: 1824 Total reward: 12.799999999999999 Training loss: 0.0286 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [11.  0. 12.]\n",
      "Episode: 1825 Total reward: 17.7 Training loss: 0.0239 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [11.  3. 10.]\n",
      "Episode: 1826 Total reward: 18.3 Training loss: 0.0178 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [14.  0. 24.]\n",
      "Episode: 1827 Total reward: 24.1 Training loss: 0.0247 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [13.  0. 24.]\n",
      "Episode: 1828 Total reward: 22.1 Training loss: 0.0314 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [13.  0. 24.]\n",
      "Episode: 1829 Total reward: 22.099999999999998 Training loss: 0.0290 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [11.  0.  4.]\n",
      "Episode: 1830 Total reward: 17.799999999999997 Training loss: 0.0231 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [13.  0. 14.]\n",
      "Episode: 1831 Total reward: 21.9 Training loss: 0.0230 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [11.  0. 14.]\n",
      "Episode: 1832 Total reward: 18.0 Training loss: 0.0250 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [10.  0. 10.]\n",
      "Episode: 1833 Total reward: 16.0 Training loss: 0.0179 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [12.  0. 24.]\n",
      "Episode: 1834 Total reward: 20.1 Training loss: 0.0214 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [14.  0.  4.]\n",
      "Episode: 1835 Total reward: 23.700000000000003 Training loss: 0.0241 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [14.  0. 44.]\n",
      "Episode: 1836 Total reward: 24.2 Training loss: 0.0271 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [14.  0. 24.]\n",
      "Episode: 1837 Total reward: 24.1 Training loss: 0.0253 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [10.  9. 20.]\n",
      "Episode: 1838 Total reward: 17.1 Training loss: 0.0272 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [13.  0. 24.]\n",
      "Episode: 1839 Total reward: 22.099999999999998 Training loss: 0.0222 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [8. 7. 4.]\n",
      "Episode: 1840 Total reward: 12.6 Training loss: 0.0195 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [14.  0. 24.]\n",
      "Episode: 1841 Total reward: 24.1 Training loss: 0.0212 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [14.  0.  4.]\n",
      "Episode: 1842 Total reward: 23.7 Training loss: 0.0246 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [10.  2. 14.]\n",
      "Episode: 1843 Total reward: 15.899999999999999 Training loss: 0.0225 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [ 7.  8. 24.]\n",
      "Episode: 1844 Total reward: 10.9 Training loss: 0.0266 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [11.  0. 14.]\n",
      "Episode: 1845 Total reward: 17.9 Training loss: 0.0258 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [13.  0. 36.]\n",
      "Episode: 1846 Total reward: 22.1 Training loss: 0.0186 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [9. 2. 8.]\n",
      "Episode: 1847 Total reward: 14.0 Training loss: 0.0241 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [ 8.  4. 12.]\n",
      "Episode: 1848 Total reward: 12.5 Training loss: 0.0221 Epsilon: 0.0100 Life: 300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode Finish  [10.  0. 24.]\n",
      "Episode: 1849 Total reward: 16.1 Training loss: 0.0221 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [ 9.  4. 24.]\n",
      "Episode: 1850 Total reward: 14.5 Training loss: 0.0188 Epsilon: 0.0100 Life: 300\n",
      "Stats Saved at 1850\n",
      "Episode Finish  [10.  1.  4.]\n",
      "Episode: 1851 Total reward: 15.899999999999999 Training loss: 0.0188 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [ 8.  3. 20.]\n",
      "Episode: 1852 Total reward: 12.2 Training loss: 0.0271 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [12.  0.  4.]\n",
      "Episode: 1853 Total reward: 19.799999999999997 Training loss: 0.0173 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [14.  0.  6.]\n",
      "Episode: 1854 Total reward: 23.9 Training loss: 0.0215 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [12.  0. 24.]\n",
      "Episode: 1855 Total reward: 20.099999999999998 Training loss: 0.0211 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [12.  0. 10.]\n",
      "Episode: 1856 Total reward: 19.400000000000006 Training loss: 0.0288 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [13.  0. 24.]\n",
      "Episode: 1857 Total reward: 22.1 Training loss: 0.0188 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [12.  2. 20.]\n",
      "Episode: 1858 Total reward: 19.9 Training loss: 0.0200 Epsilon: 0.0100 Life: 300\n",
      "Model updated\n",
      "Episode Finish  [12.  0.  2.]\n",
      "Episode: 1859 Total reward: 19.599999999999998 Training loss: 0.0219 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [ 8.  2. 32.]\n",
      "Episode: 1860 Total reward: 12.3 Training loss: 0.0335 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [13.  0. 24.]\n",
      "Episode: 1861 Total reward: 22.1 Training loss: 0.0234 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [12.  0.  4.]\n",
      "Episode: 1862 Total reward: 19.4 Training loss: 0.0190 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [15.  0. 24.]\n",
      "Episode: 1863 Total reward: 26.1 Training loss: 0.0314 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [12.  0. 24.]\n",
      "Episode: 1864 Total reward: 20.099999999999998 Training loss: 0.0185 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [13.  0. 24.]\n",
      "Episode: 1865 Total reward: 22.1 Training loss: 0.0259 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [12.  4. 24.]\n",
      "Episode: 1866 Total reward: 20.499999999999996 Training loss: 0.0297 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [10.  0. 20.]\n",
      "Episode: 1867 Total reward: 15.899999999999999 Training loss: 0.0235 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [11.  0.  6.]\n",
      "Episode: 1868 Total reward: 17.9 Training loss: 0.0189 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [13.  0.  8.]\n",
      "Episode: 1869 Total reward: 21.799999999999997 Training loss: 0.0288 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [8. 2. 6.]\n",
      "Episode: 1870 Total reward: 11.900000000000002 Training loss: 0.0241 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [10.  0. 10.]\n",
      "Episode: 1871 Total reward: 16.0 Training loss: 0.0283 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [11.  0. 28.]\n",
      "Episode: 1872 Total reward: 18.1 Training loss: 0.0191 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [14.  0. 32.]\n",
      "Episode: 1873 Total reward: 24.0 Training loss: 0.0308 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [11.  0. 28.]\n",
      "Episode: 1874 Total reward: 17.9 Training loss: 0.0179 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [14.  0.  2.]\n",
      "Episode: 1875 Total reward: 23.699999999999996 Training loss: 0.0224 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [12.  0.  2.]\n",
      "Episode: 1876 Total reward: 19.7 Training loss: 0.0166 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [14.  0. 24.]\n",
      "Episode: 1877 Total reward: 24.1 Training loss: 0.0170 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [15.  0. 24.]\n",
      "Episode: 1878 Total reward: 26.099999999999998 Training loss: 0.0236 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [13.  0.  2.]\n",
      "Episode: 1879 Total reward: 21.8 Training loss: 0.0352 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [8. 0. 8.]\n",
      "Episode: 1880 Total reward: 11.9 Training loss: 0.0226 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [14.  0. 24.]\n",
      "Episode: 1881 Total reward: 24.1 Training loss: 0.0338 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [12.  0. 24.]\n",
      "Episode: 1882 Total reward: 20.1 Training loss: 0.0172 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [11.  0. 14.]\n",
      "Episode: 1883 Total reward: 18.0 Training loss: 0.0179 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [13.  0. 32.]\n",
      "Episode: 1884 Total reward: 21.9 Training loss: 0.0196 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [ 4. 14. 16.]\n",
      "Episode: 1885 Total reward: 5.3999999999999995 Training loss: 0.0218 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [15.  0. 26.]\n",
      "Episode: 1886 Total reward: 25.9 Training loss: 0.0189 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [12.  4. 24.]\n",
      "Episode: 1887 Total reward: 20.499999999999996 Training loss: 0.0243 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [ 9.  3. 24.]\n",
      "Episode: 1888 Total reward: 14.400000000000002 Training loss: 0.0186 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [12. 10. 24.]\n",
      "Episode: 1889 Total reward: 21.099999999999998 Training loss: 0.0235 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [13.  0. 12.]\n",
      "Episode: 1890 Total reward: 22.1 Training loss: 0.0294 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [12.  3.  4.]\n",
      "Episode: 1891 Total reward: 20.3 Training loss: 0.0246 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [10.  0.  8.]\n",
      "Episode: 1892 Total reward: 15.899999999999999 Training loss: 0.0173 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [12.  0. 24.]\n",
      "Episode: 1893 Total reward: 20.1 Training loss: 0.0205 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [12.  3.  8.]\n",
      "Episode: 1894 Total reward: 20.200000000000003 Training loss: 0.0232 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [7. 0. 8.]\n",
      "Episode: 1895 Total reward: 9.7 Training loss: 0.0254 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [13.  0. 10.]\n",
      "Episode: 1896 Total reward: 22.0 Training loss: 0.0233 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [14.  0. 24.]\n",
      "Episode: 1897 Total reward: 24.099999999999998 Training loss: 0.0253 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [12.  0.  6.]\n",
      "Episode: 1898 Total reward: 19.900000000000002 Training loss: 0.0213 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [8. 3. 2.]\n",
      "Episode: 1899 Total reward: 12.8 Training loss: 0.0228 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [10.  0.  6.]\n",
      "Episode: 1900 Total reward: 15.6 Training loss: 0.0169 Epsilon: 0.0100 Life: 300\n",
      "Stats Saved at 1900\n",
      "Episode Finish  [14.  0. 20.]\n",
      "Episode: 1901 Total reward: 23.9 Training loss: 0.0341 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [12.  0. 24.]\n",
      "Episode: 1902 Total reward: 20.099999999999998 Training loss: 0.0262 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [12.  0.  8.]\n",
      "Episode: 1903 Total reward: 19.8 Training loss: 0.0164 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [13.  0. 24.]\n",
      "Episode: 1904 Total reward: 22.1 Training loss: 0.0171 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [ 9.  4. 16.]\n",
      "Episode: 1905 Total reward: 14.499999999999998 Training loss: 0.0168 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [12.  0. 24.]\n",
      "Episode: 1906 Total reward: 20.1 Training loss: 0.0173 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [13.  0. 24.]\n",
      "Episode: 1907 Total reward: 22.1 Training loss: 0.0220 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [13.  0. 20.]\n",
      "Episode: 1908 Total reward: 22.2 Training loss: 0.0253 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [11.  0. 10.]\n",
      "Episode: 1909 Total reward: 17.7 Training loss: 0.0253 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [17.  0. 10.]\n",
      "Episode: 1910 Total reward: 30.1 Training loss: 0.0235 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [12.  0. 12.]\n",
      "Episode: 1911 Total reward: 20.0 Training loss: 0.0324 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [13.  0. 24.]\n",
      "Episode: 1912 Total reward: 22.099999999999998 Training loss: 0.0287 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [11.  0.  4.]\n",
      "Episode: 1913 Total reward: 18.1 Training loss: 0.0242 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [15.  0. 24.]\n",
      "Episode: 1914 Total reward: 26.1 Training loss: 0.0182 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [ 7.  6. 24.]\n",
      "Episode: 1915 Total reward: 10.7 Training loss: 0.0231 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [13.  0. 24.]\n",
      "Episode: 1916 Total reward: 22.1 Training loss: 0.0190 Epsilon: 0.0100 Life: 300\n",
      "Model updated\n",
      "Episode Finish  [12.  0.  2.]\n",
      "Episode: 1917 Total reward: 19.5 Training loss: 0.0226 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [11.  0.  4.]\n",
      "Episode: 1918 Total reward: 18.0 Training loss: 0.0252 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [14.  0. 24.]\n",
      "Episode: 1919 Total reward: 24.099999999999998 Training loss: 0.0154 Epsilon: 0.0100 Life: 300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode Finish  [16.  0.  8.]\n",
      "Episode: 1920 Total reward: 27.800000000000004 Training loss: 0.0194 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [16.  0. 10.]\n",
      "Episode: 1921 Total reward: 28.000000000000004 Training loss: 0.0335 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [14.  0. 24.]\n",
      "Episode: 1922 Total reward: 24.1 Training loss: 0.0334 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [9. 0. 4.]\n",
      "Episode: 1923 Total reward: 14.100000000000001 Training loss: 0.0208 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [13.  0.  8.]\n",
      "Episode: 1924 Total reward: 21.8 Training loss: 0.0270 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [13.  2.  4.]\n",
      "Episode: 1925 Total reward: 22.1 Training loss: 0.0226 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [ 6.  6. 24.]\n",
      "Episode: 1926 Total reward: 8.700000000000001 Training loss: 0.0293 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [10.  0.  2.]\n",
      "Episode: 1927 Total reward: 15.399999999999999 Training loss: 0.0284 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [13.  0.  6.]\n",
      "Episode: 1928 Total reward: 21.699999999999996 Training loss: 0.0257 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [13.  0. 24.]\n",
      "Episode: 1929 Total reward: 22.1 Training loss: 0.0237 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [ 5. 16. 24.]\n",
      "Episode: 1930 Total reward: 8.7 Training loss: 0.0149 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [ 6. 14. 24.]\n",
      "Episode: 1931 Total reward: 9.500000000000002 Training loss: 0.0246 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [10.  9. 30.]\n",
      "Episode: 1932 Total reward: 16.9 Training loss: 0.0168 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [ 7.  7. 24.]\n",
      "Episode: 1933 Total reward: 10.799999999999999 Training loss: 0.0266 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [15.  0. 10.]\n",
      "Episode: 1934 Total reward: 26.0 Training loss: 0.0167 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [13.  0. 10.]\n",
      "Episode: 1935 Total reward: 22.0 Training loss: 0.0193 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [12.  0.  4.]\n",
      "Episode: 1936 Total reward: 19.9 Training loss: 0.0294 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [13.  1.  4.]\n",
      "Episode: 1937 Total reward: 22.1 Training loss: 0.0289 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [10.  7.  8.]\n",
      "Episode: 1938 Total reward: 16.5 Training loss: 0.0223 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [9. 3. 6.]\n",
      "Episode: 1939 Total reward: 14.0 Training loss: 0.0303 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [12.  0.  2.]\n",
      "Episode: 1940 Total reward: 19.5 Training loss: 0.0304 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [12.  0. 36.]\n",
      "Episode: 1941 Total reward: 20.099999999999998 Training loss: 0.0260 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [ 9.  5. 24.]\n",
      "Episode: 1942 Total reward: 14.600000000000001 Training loss: 0.0271 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [12.  0.  8.]\n",
      "Episode: 1943 Total reward: 19.8 Training loss: 0.0302 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [10.  0. 14.]\n",
      "Episode: 1944 Total reward: 15.700000000000001 Training loss: 0.0203 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [14.  0. 14.]\n",
      "Episode: 1945 Total reward: 23.700000000000003 Training loss: 0.0257 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [12.  4. 24.]\n",
      "Episode: 1946 Total reward: 20.5 Training loss: 0.0282 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [13.  0. 24.]\n",
      "Episode: 1947 Total reward: 22.1 Training loss: 0.0227 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [14.  0. 24.]\n",
      "Episode: 1948 Total reward: 24.099999999999998 Training loss: 0.0160 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [14.  0. 24.]\n",
      "Episode: 1949 Total reward: 24.099999999999998 Training loss: 0.0126 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [ 9.  3. 24.]\n",
      "Episode: 1950 Total reward: 15.399999999999999 Training loss: 0.0188 Epsilon: 0.0100 Life: 300\n",
      "Stats Saved at 1950\n",
      "Episode Finish  [12.  0.  8.]\n",
      "Episode: 1951 Total reward: 19.9 Training loss: 0.0200 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [15.  0. 24.]\n",
      "Episode: 1952 Total reward: 26.1 Training loss: 0.0168 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [11.  0.  8.]\n",
      "Episode: 1953 Total reward: 17.8 Training loss: 0.0286 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [12.  5. 24.]\n",
      "Episode: 1954 Total reward: 20.599999999999998 Training loss: 0.0186 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [11.  6.  2.]\n",
      "Episode: 1955 Total reward: 18.200000000000003 Training loss: 0.0175 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [10.  7. 12.]\n",
      "Episode: 1956 Total reward: 16.7 Training loss: 0.0234 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [12.  0. 28.]\n",
      "Episode: 1957 Total reward: 19.9 Training loss: 0.0412 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [13.  0.  8.]\n",
      "Episode: 1958 Total reward: 21.799999999999997 Training loss: 0.0213 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [ 6. 11. 18.]\n",
      "Episode: 1959 Total reward: 9.0 Training loss: 0.0187 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [12.  7. 16.]\n",
      "Episode: 1960 Total reward: 20.700000000000003 Training loss: 0.0228 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [13.  0. 24.]\n",
      "Episode: 1961 Total reward: 22.1 Training loss: 0.0215 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [13.  0.  4.]\n",
      "Episode: 1962 Total reward: 21.599999999999998 Training loss: 0.0305 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [12.  0.  6.]\n",
      "Episode: 1963 Total reward: 19.5 Training loss: 0.0206 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [ 8. 10.  2.]\n",
      "Episode: 1964 Total reward: 12.599999999999998 Training loss: 0.0291 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [10.  3.  4.]\n",
      "Episode: 1965 Total reward: 16.299999999999997 Training loss: 0.0293 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [15.  0. 22.]\n",
      "Episode: 1966 Total reward: 25.9 Training loss: 0.0229 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [ 9.  5. 16.]\n",
      "Episode: 1967 Total reward: 14.5 Training loss: 0.0260 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [12.  2. 24.]\n",
      "Episode: 1968 Total reward: 20.3 Training loss: 0.0249 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [13.  0. 24.]\n",
      "Episode: 1969 Total reward: 22.1 Training loss: 0.0366 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [ 7.  9. 24.]\n",
      "Episode: 1970 Total reward: 10.999999999999998 Training loss: 0.0184 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [12.  0.  8.]\n",
      "Episode: 1971 Total reward: 19.799999999999997 Training loss: 0.0224 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [14.  0. 24.]\n",
      "Episode: 1972 Total reward: 24.1 Training loss: 0.0246 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [16.  0. 24.]\n",
      "Episode: 1973 Total reward: 28.1 Training loss: 0.0321 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [14.  0.  4.]\n",
      "Episode: 1974 Total reward: 23.4 Training loss: 0.0161 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [13.  0. 24.]\n",
      "Episode: 1975 Total reward: 22.099999999999998 Training loss: 0.0217 Epsilon: 0.0100 Life: 300\n",
      "Model updated\n",
      "Episode Finish  [12.  0. 24.]\n",
      "Episode: 1976 Total reward: 20.1 Training loss: 0.0179 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [15.  0. 24.]\n",
      "Episode: 1977 Total reward: 26.1 Training loss: 0.0339 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [11.  5. 24.]\n",
      "Episode: 1978 Total reward: 18.6 Training loss: 0.0332 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [16.  0.  4.]\n",
      "Episode: 1979 Total reward: 27.999999999999996 Training loss: 0.0243 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [12.  3. 24.]\n",
      "Episode: 1980 Total reward: 20.4 Training loss: 0.0270 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [9. 4. 4.]\n",
      "Episode: 1981 Total reward: 14.2 Training loss: 0.0191 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [18.  0.  4.]\n",
      "Episode: 1982 Total reward: 31.9 Training loss: 0.0305 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [10.  3. 14.]\n",
      "Episode: 1983 Total reward: 16.0 Training loss: 0.0204 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [12.  0. 16.]\n",
      "Episode: 1984 Total reward: 19.900000000000002 Training loss: 0.0214 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [11.  0.  2.]\n",
      "Episode: 1985 Total reward: 17.6 Training loss: 0.0191 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [14.  0.  6.]\n",
      "Episode: 1986 Total reward: 23.6 Training loss: 0.0245 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [15.  0. 24.]\n",
      "Episode: 1987 Total reward: 26.1 Training loss: 0.0249 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [ 8.  8. 24.]\n",
      "Episode: 1988 Total reward: 12.9 Training loss: 0.0248 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [14.  0. 24.]\n",
      "Episode: 1989 Total reward: 24.1 Training loss: 0.0251 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [13.  0.  4.]\n",
      "Episode: 1990 Total reward: 21.5 Training loss: 0.0288 Epsilon: 0.0100 Life: 300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode Finish  [14.  0. 24.]\n",
      "Episode: 1991 Total reward: 24.2 Training loss: 0.0202 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [14.  0.  4.]\n",
      "Episode: 1992 Total reward: 24.1 Training loss: 0.0347 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [12.  3. 24.]\n",
      "Episode: 1993 Total reward: 20.4 Training loss: 0.0211 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [12.  0.  4.]\n",
      "Episode: 1994 Total reward: 19.9 Training loss: 0.0218 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [13.  0.  4.]\n",
      "Episode: 1995 Total reward: 22.0 Training loss: 0.0300 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [15.  0. 12.]\n",
      "Episode: 1996 Total reward: 25.7 Training loss: 0.0184 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [16.  0. 24.]\n",
      "Episode: 1997 Total reward: 28.099999999999998 Training loss: 0.0249 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [11.  7. 12.]\n",
      "Episode: 1998 Total reward: 18.8 Training loss: 0.0213 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [10.  6.  4.]\n",
      "Episode: 1999 Total reward: 16.400000000000002 Training loss: 0.0264 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [11.  0. 24.]\n",
      "Episode: 2000 Total reward: 17.7 Training loss: 0.0119 Epsilon: 0.0100 Life: 300\n",
      "Model Saved at 2000\n",
      "Stats Saved at 2000\n",
      "Episode Finish  [ 7. 10. 24.]\n",
      "Episode: 2001 Total reward: 11.1 Training loss: 0.0243 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [11.  1.  2.]\n",
      "Episode: 2002 Total reward: 17.5 Training loss: 0.0249 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [16.  0. 30.]\n",
      "Episode: 2003 Total reward: 28.099999999999994 Training loss: 0.0314 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [12.  0. 20.]\n",
      "Episode: 2004 Total reward: 20.0 Training loss: 0.0201 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [11.  3. 14.]\n",
      "Episode: 2005 Total reward: 18.0 Training loss: 0.0296 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [18.  0.  8.]\n",
      "Episode: 2006 Total reward: 31.799999999999997 Training loss: 0.0155 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [11.  6. 24.]\n",
      "Episode: 2007 Total reward: 18.700000000000003 Training loss: 0.0189 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [12.  2. 22.]\n",
      "Episode: 2008 Total reward: 19.8 Training loss: 0.0242 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [12.  0. 20.]\n",
      "Episode: 2009 Total reward: 19.900000000000002 Training loss: 0.0210 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [14.  0. 20.]\n",
      "Episode: 2010 Total reward: 23.9 Training loss: 0.0195 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [12.  2.  6.]\n",
      "Episode: 2011 Total reward: 19.8 Training loss: 0.0226 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [15.  0. 20.]\n",
      "Episode: 2012 Total reward: 26.1 Training loss: 0.0125 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [12.  0.  4.]\n",
      "Episode: 2013 Total reward: 19.8 Training loss: 0.0218 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [9. 0. 8.]\n",
      "Episode: 2014 Total reward: 14.0 Training loss: 0.0137 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [13.  0.  6.]\n",
      "Episode: 2015 Total reward: 21.5 Training loss: 0.0242 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [ 7. 11.  6.]\n",
      "Episode: 2016 Total reward: 11.1 Training loss: 0.0263 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [13.  0. 24.]\n",
      "Episode: 2017 Total reward: 22.1 Training loss: 0.0193 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [14.  0. 24.]\n",
      "Episode: 2018 Total reward: 24.1 Training loss: 0.0235 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [13.  0. 24.]\n",
      "Episode: 2019 Total reward: 22.1 Training loss: 0.0216 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [12.  0.  6.]\n",
      "Episode: 2020 Total reward: 19.4 Training loss: 0.0207 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [10.  7. 18.]\n",
      "Episode: 2021 Total reward: 16.6 Training loss: 0.0141 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [11.  4. 10.]\n",
      "Episode: 2022 Total reward: 18.4 Training loss: 0.0207 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [13.  1. 10.]\n",
      "Episode: 2023 Total reward: 21.8 Training loss: 0.0194 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [ 9.  2. 14.]\n",
      "Episode: 2024 Total reward: 13.900000000000002 Training loss: 0.0224 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [ 6. 15. 24.]\n",
      "Episode: 2025 Total reward: 9.6 Training loss: 0.0246 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [8. 6. 8.]\n",
      "Episode: 2026 Total reward: 12.4 Training loss: 0.0254 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [ 8.  6. 10.]\n",
      "Episode: 2027 Total reward: 13.4 Training loss: 0.0182 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [ 7. 11. 24.]\n",
      "Episode: 2028 Total reward: 11.2 Training loss: 0.0232 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [ 8.  5. 28.]\n",
      "Episode: 2029 Total reward: 12.600000000000001 Training loss: 0.0224 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [13.  0. 16.]\n",
      "Episode: 2030 Total reward: 21.8 Training loss: 0.0190 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [14.  0. 20.]\n",
      "Episode: 2031 Total reward: 24.0 Training loss: 0.0197 Epsilon: 0.0100 Life: 300\n",
      "Model updated\n",
      "Episode Finish  [15.  0. 12.]\n",
      "Episode: 2032 Total reward: 25.9 Training loss: 0.0247 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [14.  0. 24.]\n",
      "Episode: 2033 Total reward: 24.1 Training loss: 0.0157 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [15.  0. 16.]\n",
      "Episode: 2034 Total reward: 25.8 Training loss: 0.0206 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [14.  2. 24.]\n",
      "Episode: 2035 Total reward: 24.3 Training loss: 0.0274 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [14.  0. 24.]\n",
      "Episode: 2036 Total reward: 24.1 Training loss: 0.0353 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [15.  0. 10.]\n",
      "Episode: 2037 Total reward: 26.0 Training loss: 0.0300 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [10.  0. 16.]\n",
      "Episode: 2038 Total reward: 15.8 Training loss: 0.0219 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [13.  2.  6.]\n",
      "Episode: 2039 Total reward: 23.0 Training loss: 0.0249 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [11.  2.  4.]\n",
      "Episode: 2040 Total reward: 18.200000000000003 Training loss: 0.0208 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [12.  0.  8.]\n",
      "Episode: 2041 Total reward: 20.0 Training loss: 0.0261 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [19.  0. 12.]\n",
      "Episode: 2042 Total reward: 34.0 Training loss: 0.0265 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [13.  0. 10.]\n",
      "Episode: 2043 Total reward: 21.900000000000002 Training loss: 0.0269 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [13.  0.  2.]\n",
      "Episode: 2044 Total reward: 22.0 Training loss: 0.0324 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [12.  0. 36.]\n",
      "Episode: 2045 Total reward: 20.000000000000004 Training loss: 0.0290 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [14.  0.  8.]\n",
      "Episode: 2046 Total reward: 23.4 Training loss: 0.0181 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [12.  7. 40.]\n",
      "Episode: 2047 Total reward: 21.6 Training loss: 0.0298 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [13.  0.  4.]\n",
      "Episode: 2048 Total reward: 21.5 Training loss: 0.0249 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [10.  0.  4.]\n",
      "Episode: 2049 Total reward: 15.7 Training loss: 0.0267 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [16.  0. 24.]\n",
      "Episode: 2050 Total reward: 28.099999999999998 Training loss: 0.0304 Epsilon: 0.0100 Life: 300\n",
      "Stats Saved at 2050\n",
      "Episode Finish  [ 9.  6. 24.]\n",
      "Episode: 2051 Total reward: 14.7 Training loss: 0.0144 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [15.  0. 24.]\n",
      "Episode: 2052 Total reward: 26.1 Training loss: 0.0268 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [12.  0. 24.]\n",
      "Episode: 2053 Total reward: 20.1 Training loss: 0.0217 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [14.  0. 20.]\n",
      "Episode: 2054 Total reward: 23.9 Training loss: 0.0189 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [13.  0.  8.]\n",
      "Episode: 2055 Total reward: 21.700000000000003 Training loss: 0.0148 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [12.  2. 14.]\n",
      "Episode: 2056 Total reward: 19.800000000000004 Training loss: 0.0239 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [14.  0. 28.]\n",
      "Episode: 2057 Total reward: 24.2 Training loss: 0.0171 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [10.  2. 12.]\n",
      "Episode: 2058 Total reward: 15.9 Training loss: 0.0225 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [13.  0.  4.]\n",
      "Episode: 2059 Total reward: 21.9 Training loss: 0.0267 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [12.  0.  4.]\n",
      "Episode: 2060 Total reward: 19.799999999999997 Training loss: 0.0472 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [ 9. 12. 24.]\n",
      "Episode: 2061 Total reward: 15.3 Training loss: 0.0243 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [12.  0.  6.]\n",
      "Episode: 2062 Total reward: 20.0 Training loss: 0.0190 Epsilon: 0.0100 Life: 300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode Finish  [14.  0. 14.]\n",
      "Episode: 2063 Total reward: 24.0 Training loss: 0.0294 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [14.  0. 26.]\n",
      "Episode: 2064 Total reward: 23.9 Training loss: 0.0256 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [12.  0. 12.]\n",
      "Episode: 2065 Total reward: 19.7 Training loss: 0.0290 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [12.  0.  8.]\n",
      "Episode: 2066 Total reward: 19.8 Training loss: 0.0329 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [ 7. 11. 10.]\n",
      "Episode: 2067 Total reward: 11.1 Training loss: 0.0149 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [13.  0. 32.]\n",
      "Episode: 2068 Total reward: 22.0 Training loss: 0.0211 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [ 8. 12. 16.]\n",
      "Episode: 2069 Total reward: 13.100000000000001 Training loss: 0.0197 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [14.  0.  2.]\n",
      "Episode: 2070 Total reward: 23.299999999999997 Training loss: 0.0253 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [15.  0. 20.]\n",
      "Episode: 2071 Total reward: 26.1 Training loss: 0.0288 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [ 9.  7. 24.]\n",
      "Episode: 2072 Total reward: 14.8 Training loss: 0.0209 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [15.  0.  2.]\n",
      "Episode: 2073 Total reward: 25.7 Training loss: 0.0281 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [10.  0.  8.]\n",
      "Episode: 2074 Total reward: 15.799999999999999 Training loss: 0.0324 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [ 7. 11. 24.]\n",
      "Episode: 2075 Total reward: 11.200000000000001 Training loss: 0.0212 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [12.  0. 16.]\n",
      "Episode: 2076 Total reward: 19.8 Training loss: 0.0258 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [12.  0. 10.]\n",
      "Episode: 2077 Total reward: 20.0 Training loss: 0.0254 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [14.  0. 14.]\n",
      "Episode: 2078 Total reward: 24.0 Training loss: 0.0179 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [10.  1. 32.]\n",
      "Episode: 2079 Total reward: 15.9 Training loss: 0.0220 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [12.  0. 12.]\n",
      "Episode: 2080 Total reward: 19.9 Training loss: 0.0282 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [10.  7.  6.]\n",
      "Episode: 2081 Total reward: 16.200000000000003 Training loss: 0.0137 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [12.  5. 24.]\n",
      "Episode: 2082 Total reward: 20.6 Training loss: 0.0242 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [12.  3. 24.]\n",
      "Episode: 2083 Total reward: 20.4 Training loss: 0.0226 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [16.  0. 10.]\n",
      "Episode: 2084 Total reward: 28.0 Training loss: 0.0175 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [14.  0. 24.]\n",
      "Episode: 2085 Total reward: 24.1 Training loss: 0.0308 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [ 6.  9. 24.]\n",
      "Episode: 2086 Total reward: 9.100000000000001 Training loss: 0.0256 Epsilon: 0.0100 Life: 300\n",
      "Model updated\n",
      "Episode Finish  [8. 1. 8.]\n",
      "Episode: 2087 Total reward: 11.799999999999999 Training loss: 0.0291 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [9. 2. 6.]\n",
      "Episode: 2088 Total reward: 13.799999999999999 Training loss: 0.0327 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [11.  0. 10.]\n",
      "Episode: 2089 Total reward: 18.0 Training loss: 0.0189 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [8. 3. 4.]\n",
      "Episode: 2090 Total reward: 11.8 Training loss: 0.0223 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [12.  0. 24.]\n",
      "Episode: 2091 Total reward: 20.099999999999998 Training loss: 0.0206 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [9. 3. 8.]\n",
      "Episode: 2092 Total reward: 14.399999999999999 Training loss: 0.0245 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [11.  1.  4.]\n",
      "Episode: 2093 Total reward: 17.9 Training loss: 0.0286 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [14.  0. 24.]\n",
      "Episode: 2094 Total reward: 24.099999999999998 Training loss: 0.0294 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [8. 2. 2.]\n",
      "Episode: 2095 Total reward: 12.0 Training loss: 0.0231 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [12.  0. 32.]\n",
      "Episode: 2096 Total reward: 20.0 Training loss: 0.0128 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [ 8.  3. 16.]\n",
      "Episode: 2097 Total reward: 12.8 Training loss: 0.0186 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [10. 12. 24.]\n",
      "Episode: 2098 Total reward: 17.3 Training loss: 0.0218 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [ 7. 14. 24.]\n",
      "Episode: 2099 Total reward: 11.499999999999998 Training loss: 0.0170 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [13.  0. 14.]\n",
      "Episode: 2100 Total reward: 21.799999999999997 Training loss: 0.0248 Epsilon: 0.0100 Life: 300\n",
      "Stats Saved at 2100\n",
      "Episode Finish  [ 5. 14. 24.]\n",
      "Episode: 2101 Total reward: 7.5 Training loss: 0.0214 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [12.  2.  8.]\n",
      "Episode: 2102 Total reward: 19.7 Training loss: 0.0158 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [12.  6. 24.]\n",
      "Episode: 2103 Total reward: 20.7 Training loss: 0.0270 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [13.  0.  8.]\n",
      "Episode: 2104 Total reward: 21.799999999999997 Training loss: 0.0273 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [12.  0.  8.]\n",
      "Episode: 2105 Total reward: 20.0 Training loss: 0.0199 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [8. 0. 2.]\n",
      "Episode: 2106 Total reward: 11.599999999999998 Training loss: 0.0266 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [15.  1.  4.]\n",
      "Episode: 2107 Total reward: 25.9 Training loss: 0.0223 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [9. 0. 6.]\n",
      "Episode: 2108 Total reward: 13.8 Training loss: 0.0182 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [14.  0.  8.]\n",
      "Episode: 2109 Total reward: 23.8 Training loss: 0.0245 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [7. 0. 4.]\n",
      "Episode: 2110 Total reward: 9.700000000000001 Training loss: 0.0243 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [11.  0. 24.]\n",
      "Episode: 2111 Total reward: 18.1 Training loss: 0.0205 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [10.  3. 24.]\n",
      "Episode: 2112 Total reward: 16.4 Training loss: 0.0191 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [14.  0. 14.]\n",
      "Episode: 2113 Total reward: 23.599999999999998 Training loss: 0.0168 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [13.  0. 20.]\n",
      "Episode: 2114 Total reward: 21.9 Training loss: 0.0188 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [ 9.  2. 24.]\n",
      "Episode: 2115 Total reward: 14.3 Training loss: 0.0208 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [11.  0. 20.]\n",
      "Episode: 2116 Total reward: 18.0 Training loss: 0.0254 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [9. 7. 8.]\n",
      "Episode: 2117 Total reward: 14.5 Training loss: 0.0169 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [9. 8. 4.]\n",
      "Episode: 2118 Total reward: 14.600000000000001 Training loss: 0.0223 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [12.  0.  2.]\n",
      "Episode: 2119 Total reward: 19.7 Training loss: 0.0258 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [ 8.  0. 36.]\n",
      "Episode: 2120 Total reward: 12.2 Training loss: 0.0203 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [ 7.  2. 10.]\n",
      "Episode: 2121 Total reward: 9.8 Training loss: 0.0186 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [11.  0.  4.]\n",
      "Episode: 2122 Total reward: 17.7 Training loss: 0.0298 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [13.  0.  8.]\n",
      "Episode: 2123 Total reward: 21.8 Training loss: 0.0298 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [11.  0. 28.]\n",
      "Episode: 2124 Total reward: 18.1 Training loss: 0.0195 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [13.  0. 26.]\n",
      "Episode: 2125 Total reward: 21.9 Training loss: 0.0267 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [14.  0.  6.]\n",
      "Episode: 2126 Total reward: 23.9 Training loss: 0.0211 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [11.  0. 10.]\n",
      "Episode: 2127 Total reward: 18.0 Training loss: 0.0166 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [10.  4.  2.]\n",
      "Episode: 2128 Total reward: 15.899999999999999 Training loss: 0.0113 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [12.  0. 10.]\n",
      "Episode: 2129 Total reward: 20.0 Training loss: 0.0196 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [12.  3.  4.]\n",
      "Episode: 2130 Total reward: 20.1 Training loss: 0.0180 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [12.  5. 24.]\n",
      "Episode: 2131 Total reward: 20.6 Training loss: 0.0266 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [ 7.  9. 10.]\n",
      "Episode: 2132 Total reward: 10.899999999999999 Training loss: 0.0215 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [9. 0. 4.]\n",
      "Episode: 2133 Total reward: 13.899999999999999 Training loss: 0.0230 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [14.  0. 24.]\n",
      "Episode: 2134 Total reward: 24.1 Training loss: 0.0155 Epsilon: 0.0100 Life: 300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode Finish  [11.  0.  2.]\n",
      "Episode: 2135 Total reward: 17.3 Training loss: 0.0217 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [ 8.  9. 12.]\n",
      "Episode: 2136 Total reward: 13.0 Training loss: 0.0232 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [11.  0.  2.]\n",
      "Episode: 2137 Total reward: 17.999999999999996 Training loss: 0.0218 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [11.  0.  4.]\n",
      "Episode: 2138 Total reward: 17.799999999999997 Training loss: 0.0167 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [13.  0.  4.]\n",
      "Episode: 2139 Total reward: 22.0 Training loss: 0.0235 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [15.  0. 10.]\n",
      "Episode: 2140 Total reward: 25.999999999999996 Training loss: 0.0155 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [ 7.  9. 14.]\n",
      "Episode: 2141 Total reward: 10.799999999999999 Training loss: 0.0146 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [14.  0.  2.]\n",
      "Episode: 2142 Total reward: 23.9 Training loss: 0.0211 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [ 8.  0. 14.]\n",
      "Episode: 2143 Total reward: 11.600000000000001 Training loss: 0.0305 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [10.  0. 16.]\n",
      "Episode: 2144 Total reward: 15.700000000000001 Training loss: 0.0213 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [12.  0. 24.]\n",
      "Episode: 2145 Total reward: 20.1 Training loss: 0.0247 Epsilon: 0.0100 Life: 300\n",
      "Model updated\n",
      "Episode Finish  [ 9.  2. 10.]\n",
      "Episode: 2146 Total reward: 14.1 Training loss: 0.0242 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [ 6.  3. 12.]\n",
      "Episode: 2147 Total reward: 7.8999999999999995 Training loss: 0.0145 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [14.  0. 10.]\n",
      "Episode: 2148 Total reward: 24.0 Training loss: 0.0236 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [11.  0.  2.]\n",
      "Episode: 2149 Total reward: 17.7 Training loss: 0.0231 Epsilon: 0.0100 Life: 300\n",
      "Episode Finish  [10.  0. 20.]\n",
      "Episode: 2150 Total reward: 16.1 Training loss: 0.0225 Epsilon: 0.0100 Life: 300\n",
      "Stats Saved at 2150\n",
      "Episode Finish  [13.  0. 66.]\n",
      "Episode: 2151 Total reward: 23.0 Training loss: 0.0180 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [10.  0.  4.]\n",
      "Episode: 2152 Total reward: 16.1 Training loss: 0.0255 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [12.  0. 18.]\n",
      "Episode: 2153 Total reward: 19.6 Training loss: 0.0106 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [13.  0.  4.]\n",
      "Episode: 2154 Total reward: 21.799999999999997 Training loss: 0.0196 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [10.  0. 40.]\n",
      "Episode: 2155 Total reward: 16.1 Training loss: 0.0259 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [13.  0. 24.]\n",
      "Episode: 2156 Total reward: 22.1 Training loss: 0.0265 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [15.  0. 10.]\n",
      "Episode: 2157 Total reward: 25.999999999999996 Training loss: 0.0201 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [16.  0. 24.]\n",
      "Episode: 2158 Total reward: 28.1 Training loss: 0.0184 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [ 8.  8. 24.]\n",
      "Episode: 2159 Total reward: 12.899999999999999 Training loss: 0.0290 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [8. 0. 6.]\n",
      "Episode: 2160 Total reward: 12.0 Training loss: 0.0536 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [ 8.  9. 24.]\n",
      "Episode: 2161 Total reward: 13.0 Training loss: 0.0191 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [ 9.  4. 36.]\n",
      "Episode: 2162 Total reward: 14.4 Training loss: 0.0185 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [12.  0.  6.]\n",
      "Episode: 2163 Total reward: 19.9 Training loss: 0.0225 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [11.  0. 20.]\n",
      "Episode: 2164 Total reward: 17.9 Training loss: 0.0272 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [9. 2. 4.]\n",
      "Episode: 2165 Total reward: 13.799999999999999 Training loss: 0.0345 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [12.  0. 24.]\n",
      "Episode: 2166 Total reward: 20.0 Training loss: 0.0262 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [10.  2. 24.]\n",
      "Episode: 2167 Total reward: 16.299999999999997 Training loss: 0.0180 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [11.  4. 12.]\n",
      "Episode: 2168 Total reward: 18.0 Training loss: 0.0174 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [10.  0. 24.]\n",
      "Episode: 2169 Total reward: 16.0 Training loss: 0.0201 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [11.  1. 24.]\n",
      "Episode: 2170 Total reward: 18.2 Training loss: 0.0291 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [12.  2. 24.]\n",
      "Episode: 2171 Total reward: 20.299999999999997 Training loss: 0.0301 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [14.  0.  4.]\n",
      "Episode: 2172 Total reward: 23.799999999999997 Training loss: 0.0187 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [10.  1. 24.]\n",
      "Episode: 2173 Total reward: 16.2 Training loss: 0.0212 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [14.  0. 24.]\n",
      "Episode: 2174 Total reward: 24.1 Training loss: 0.0292 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [12.  0. 24.]\n",
      "Episode: 2175 Total reward: 20.1 Training loss: 0.0315 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [10.  0.  6.]\n",
      "Episode: 2176 Total reward: 15.700000000000001 Training loss: 0.0207 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [10.  0. 16.]\n",
      "Episode: 2177 Total reward: 15.899999999999999 Training loss: 0.0309 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [ 9.  4. 24.]\n",
      "Episode: 2178 Total reward: 15.5 Training loss: 0.0213 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [ 7.  5. 16.]\n",
      "Episode: 2179 Total reward: 11.4 Training loss: 0.0273 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [12.  0.  4.]\n",
      "Episode: 2180 Total reward: 19.6 Training loss: 0.0247 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [11.  0. 24.]\n",
      "Episode: 2181 Total reward: 18.1 Training loss: 0.0301 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [13.  0. 44.]\n",
      "Episode: 2182 Total reward: 22.1 Training loss: 0.0227 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [12.  0. 24.]\n",
      "Episode: 2183 Total reward: 20.1 Training loss: 0.0223 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [8. 0. 4.]\n",
      "Episode: 2184 Total reward: 12.0 Training loss: 0.0233 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [8. 0. 8.]\n",
      "Episode: 2185 Total reward: 12.1 Training loss: 0.0197 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [9. 0. 8.]\n",
      "Episode: 2186 Total reward: 13.8 Training loss: 0.0149 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [10.  0. 10.]\n",
      "Episode: 2187 Total reward: 16.0 Training loss: 0.0256 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [13.  0.  4.]\n",
      "Episode: 2188 Total reward: 21.8 Training loss: 0.0179 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [11.  0. 10.]\n",
      "Episode: 2189 Total reward: 18.0 Training loss: 0.0216 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [11.  0. 24.]\n",
      "Episode: 2190 Total reward: 18.1 Training loss: 0.0281 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [12.  0.  8.]\n",
      "Episode: 2191 Total reward: 19.8 Training loss: 0.0258 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [11.  0.  8.]\n",
      "Episode: 2192 Total reward: 17.9 Training loss: 0.0248 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [ 4. 12. 24.]\n",
      "Episode: 2193 Total reward: 5.299999999999999 Training loss: 0.0224 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [13.  0. 24.]\n",
      "Episode: 2194 Total reward: 22.1 Training loss: 0.0359 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [14.  0. 24.]\n",
      "Episode: 2195 Total reward: 24.1 Training loss: 0.0233 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [10.  0. 16.]\n",
      "Episode: 2196 Total reward: 15.499999999999998 Training loss: 0.0259 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [11.  0. 28.]\n",
      "Episode: 2197 Total reward: 17.7 Training loss: 0.0213 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [16.  0. 24.]\n",
      "Episode: 2198 Total reward: 28.099999999999998 Training loss: 0.0363 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [10.  4. 24.]\n",
      "Episode: 2199 Total reward: 16.5 Training loss: 0.0190 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [11.  8.  2.]\n",
      "Episode: 2200 Total reward: 18.400000000000002 Training loss: 0.0219 Epsilon: 0.0100 Life: 524\n",
      "Model Saved at 2200\n",
      "Stats Saved at 2200\n",
      "Episode Finish  [13.  0. 10.]\n",
      "Episode: 2201 Total reward: 22.0 Training loss: 0.0139 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [13.  0. 12.]\n",
      "Episode: 2202 Total reward: 21.4 Training loss: 0.0310 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [ 9.  6. 16.]\n",
      "Episode: 2203 Total reward: 14.7 Training loss: 0.0255 Epsilon: 0.0100 Life: 524\n",
      "Model updated\n",
      "Episode Finish  [12.  0. 24.]\n",
      "Episode: 2204 Total reward: 20.099999999999998 Training loss: 0.0310 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [12.  0.  8.]\n",
      "Episode: 2205 Total reward: 19.9 Training loss: 0.0264 Epsilon: 0.0100 Life: 524\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode Finish  [16.  0. 28.]\n",
      "Episode: 2206 Total reward: 27.9 Training loss: 0.0208 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [14.  0. 24.]\n",
      "Episode: 2207 Total reward: 24.1 Training loss: 0.0265 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [11.  0.  8.]\n",
      "Episode: 2208 Total reward: 18.0 Training loss: 0.0152 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [16.  0. 24.]\n",
      "Episode: 2209 Total reward: 27.8 Training loss: 0.0275 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [12.  0. 40.]\n",
      "Episode: 2210 Total reward: 20.1 Training loss: 0.0251 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [10.  0. 18.]\n",
      "Episode: 2211 Total reward: 15.7 Training loss: 0.0371 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [10.  0.  4.]\n",
      "Episode: 2212 Total reward: 15.799999999999999 Training loss: 0.0219 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [ 9. 11. 24.]\n",
      "Episode: 2213 Total reward: 15.1 Training loss: 0.0244 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [13.  0.  8.]\n",
      "Episode: 2214 Total reward: 21.799999999999997 Training loss: 0.0227 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [ 8.  5. 24.]\n",
      "Episode: 2215 Total reward: 12.6 Training loss: 0.0194 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [16.  0. 24.]\n",
      "Episode: 2216 Total reward: 28.099999999999998 Training loss: 0.0184 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [13.  3. 24.]\n",
      "Episode: 2217 Total reward: 22.4 Training loss: 0.0166 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [11.  0. 12.]\n",
      "Episode: 2218 Total reward: 18.1 Training loss: 0.0220 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [11.  0. 10.]\n",
      "Episode: 2219 Total reward: 18.0 Training loss: 0.0185 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [12.  0. 24.]\n",
      "Episode: 2220 Total reward: 20.1 Training loss: 0.0181 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [11.  0. 18.]\n",
      "Episode: 2221 Total reward: 17.700000000000003 Training loss: 0.0221 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [13.  0. 10.]\n",
      "Episode: 2222 Total reward: 22.0 Training loss: 0.0244 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [14.  0.  8.]\n",
      "Episode: 2223 Total reward: 23.8 Training loss: 0.0324 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [10.  6.  4.]\n",
      "Episode: 2224 Total reward: 16.4 Training loss: 0.0225 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [10.  0. 14.]\n",
      "Episode: 2225 Total reward: 15.7 Training loss: 0.0154 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [11.  4. 12.]\n",
      "Episode: 2226 Total reward: 18.1 Training loss: 0.0215 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [12.  0. 10.]\n",
      "Episode: 2227 Total reward: 20.099999999999998 Training loss: 0.0209 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [13.  0. 24.]\n",
      "Episode: 2228 Total reward: 22.1 Training loss: 0.0235 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [13.  0. 36.]\n",
      "Episode: 2229 Total reward: 22.1 Training loss: 0.0200 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [14.  0.  4.]\n",
      "Episode: 2230 Total reward: 23.799999999999997 Training loss: 0.0385 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [ 7.  7. 28.]\n",
      "Episode: 2231 Total reward: 10.299999999999999 Training loss: 0.0255 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [13.  0.  4.]\n",
      "Episode: 2232 Total reward: 22.099999999999998 Training loss: 0.0249 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [11.  0. 10.]\n",
      "Episode: 2233 Total reward: 17.9 Training loss: 0.0230 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [14.  0.  8.]\n",
      "Episode: 2234 Total reward: 23.799999999999997 Training loss: 0.0350 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [13.  0. 24.]\n",
      "Episode: 2235 Total reward: 22.1 Training loss: 0.0174 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [13.  0. 16.]\n",
      "Episode: 2236 Total reward: 21.799999999999997 Training loss: 0.0200 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [ 8.  0. 20.]\n",
      "Episode: 2237 Total reward: 11.900000000000002 Training loss: 0.0247 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [15.  0. 10.]\n",
      "Episode: 2238 Total reward: 26.0 Training loss: 0.0179 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [11.  0. 22.]\n",
      "Episode: 2239 Total reward: 17.6 Training loss: 0.0133 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [15.  0.  6.]\n",
      "Episode: 2240 Total reward: 25.8 Training loss: 0.0200 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [11.  0. 10.]\n",
      "Episode: 2241 Total reward: 18.0 Training loss: 0.0237 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [13.  0.  8.]\n",
      "Episode: 2242 Total reward: 21.8 Training loss: 0.0293 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [15.  0. 24.]\n",
      "Episode: 2243 Total reward: 26.099999999999998 Training loss: 0.0189 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [14.  0.  8.]\n",
      "Episode: 2244 Total reward: 23.9 Training loss: 0.0165 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [14.  0.  2.]\n",
      "Episode: 2245 Total reward: 23.999999999999996 Training loss: 0.0226 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [13.  0.  4.]\n",
      "Episode: 2246 Total reward: 21.9 Training loss: 0.0222 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [11.  0. 14.]\n",
      "Episode: 2247 Total reward: 17.6 Training loss: 0.0255 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [14.  0. 24.]\n",
      "Episode: 2248 Total reward: 24.099999999999998 Training loss: 0.0159 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [13.  0. 12.]\n",
      "Episode: 2249 Total reward: 21.900000000000002 Training loss: 0.0253 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [12.  0. 12.]\n",
      "Episode: 2250 Total reward: 19.700000000000003 Training loss: 0.0212 Epsilon: 0.0100 Life: 524\n",
      "Stats Saved at 2250\n",
      "Episode Finish  [12.  3. 24.]\n",
      "Episode: 2251 Total reward: 20.4 Training loss: 0.0212 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [10.  0.  2.]\n",
      "Episode: 2252 Total reward: 15.7 Training loss: 0.0215 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [11.  0. 24.]\n",
      "Episode: 2253 Total reward: 18.1 Training loss: 0.0231 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [12.  0. 24.]\n",
      "Episode: 2254 Total reward: 20.099999999999998 Training loss: 0.0190 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [11.  0.  6.]\n",
      "Episode: 2255 Total reward: 17.700000000000003 Training loss: 0.0221 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [10.  4. 24.]\n",
      "Episode: 2256 Total reward: 16.5 Training loss: 0.0221 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [16.  0.  2.]\n",
      "Episode: 2257 Total reward: 27.700000000000003 Training loss: 0.0215 Epsilon: 0.0100 Life: 524\n",
      "Model updated\n",
      "Episode Finish  [ 4. 18. 24.]\n",
      "Episode: 2258 Total reward: 5.9 Training loss: 0.0317 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [15.  0.  8.]\n",
      "Episode: 2259 Total reward: 25.9 Training loss: 0.0273 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [15.  0. 24.]\n",
      "Episode: 2260 Total reward: 26.1 Training loss: 0.0189 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [12.  0. 10.]\n",
      "Episode: 2261 Total reward: 20.0 Training loss: 0.0228 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [11.  0.  2.]\n",
      "Episode: 2262 Total reward: 17.700000000000003 Training loss: 0.0243 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [12.  0. 24.]\n",
      "Episode: 2263 Total reward: 20.1 Training loss: 0.0285 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [8. 8. 2.]\n",
      "Episode: 2264 Total reward: 12.6 Training loss: 0.0145 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [13.  0. 24.]\n",
      "Episode: 2265 Total reward: 22.1 Training loss: 0.0217 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [17.  0. 20.]\n",
      "Episode: 2266 Total reward: 30.1 Training loss: 0.0238 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [10.  0. 12.]\n",
      "Episode: 2267 Total reward: 15.9 Training loss: 0.0249 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [11.  0. 24.]\n",
      "Episode: 2268 Total reward: 18.1 Training loss: 0.0150 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [11.  2.  4.]\n",
      "Episode: 2269 Total reward: 18.200000000000003 Training loss: 0.0186 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [ 9. 11. 24.]\n",
      "Episode: 2270 Total reward: 15.199999999999998 Training loss: 0.0260 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [14.  0.  4.]\n",
      "Episode: 2271 Total reward: 24.0 Training loss: 0.0220 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [14.  0. 24.]\n",
      "Episode: 2272 Total reward: 24.099999999999998 Training loss: 0.0331 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [10.  0. 24.]\n",
      "Episode: 2273 Total reward: 16.099999999999998 Training loss: 0.0269 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [13.  0.  8.]\n",
      "Episode: 2274 Total reward: 21.3 Training loss: 0.0210 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [11.  0. 28.]\n",
      "Episode: 2275 Total reward: 17.6 Training loss: 0.0278 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [10.  0.  2.]\n",
      "Episode: 2276 Total reward: 15.7 Training loss: 0.0319 Epsilon: 0.0100 Life: 524\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode Finish  [15.  0. 44.]\n",
      "Episode: 2277 Total reward: 26.2 Training loss: 0.0191 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [ 8.  1. 16.]\n",
      "Episode: 2278 Total reward: 12.100000000000001 Training loss: 0.0283 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [16.  0.  4.]\n",
      "Episode: 2279 Total reward: 27.900000000000006 Training loss: 0.0205 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [13.  0. 10.]\n",
      "Episode: 2280 Total reward: 21.900000000000002 Training loss: 0.0326 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [11.  3. 40.]\n",
      "Episode: 2281 Total reward: 18.4 Training loss: 0.0266 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [12.  1. 24.]\n",
      "Episode: 2282 Total reward: 20.2 Training loss: 0.0229 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [ 9.  8. 28.]\n",
      "Episode: 2283 Total reward: 15.0 Training loss: 0.0328 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [15.  0. 10.]\n",
      "Episode: 2284 Total reward: 26.0 Training loss: 0.0292 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [12.  0. 24.]\n",
      "Episode: 2285 Total reward: 20.099999999999998 Training loss: 0.0209 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [9. 2. 4.]\n",
      "Episode: 2286 Total reward: 14.0 Training loss: 0.0242 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [12.  1.  2.]\n",
      "Episode: 2287 Total reward: 19.700000000000003 Training loss: 0.0181 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [10.  1. 10.]\n",
      "Episode: 2288 Total reward: 16.099999999999998 Training loss: 0.0218 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [8. 7. 8.]\n",
      "Episode: 2289 Total reward: 12.7 Training loss: 0.0210 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [12.  0. 24.]\n",
      "Episode: 2290 Total reward: 20.1 Training loss: 0.0199 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [ 9.  0. 14.]\n",
      "Episode: 2291 Total reward: 13.900000000000002 Training loss: 0.0267 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [14.  0. 12.]\n",
      "Episode: 2292 Total reward: 24.0 Training loss: 0.0195 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [12.  0.  6.]\n",
      "Episode: 2293 Total reward: 19.9 Training loss: 0.0138 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [ 8.  0. 12.]\n",
      "Episode: 2294 Total reward: 11.5 Training loss: 0.0204 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [10.  5. 24.]\n",
      "Episode: 2295 Total reward: 16.6 Training loss: 0.0263 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [13.  0. 14.]\n",
      "Episode: 2296 Total reward: 22.000000000000004 Training loss: 0.0266 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [ 9.  3. 24.]\n",
      "Episode: 2297 Total reward: 14.399999999999999 Training loss: 0.0213 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [13.  0. 16.]\n",
      "Episode: 2298 Total reward: 22.0 Training loss: 0.0191 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [11.  0. 10.]\n",
      "Episode: 2299 Total reward: 18.0 Training loss: 0.0175 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [8. 2. 4.]\n",
      "Episode: 2300 Total reward: 13.2 Training loss: 0.0285 Epsilon: 0.0100 Life: 524\n",
      "Stats Saved at 2300\n",
      "Episode Finish  [10.  3. 24.]\n",
      "Episode: 2301 Total reward: 16.4 Training loss: 0.0203 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [10.  2. 24.]\n",
      "Episode: 2302 Total reward: 16.299999999999997 Training loss: 0.0184 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [ 5. 10. 12.]\n",
      "Episode: 2303 Total reward: 6.9 Training loss: 0.0174 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [11.  3.  4.]\n",
      "Episode: 2304 Total reward: 18.1 Training loss: 0.0256 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [10.  0. 20.]\n",
      "Episode: 2305 Total reward: 16.0 Training loss: 0.0229 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [ 4. 15. 24.]\n",
      "Episode: 2306 Total reward: 5.6 Training loss: 0.0139 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [9. 0. 8.]\n",
      "Episode: 2307 Total reward: 13.700000000000001 Training loss: 0.0189 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [13.  0. 24.]\n",
      "Episode: 2308 Total reward: 22.1 Training loss: 0.0171 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [10.  7. 24.]\n",
      "Episode: 2309 Total reward: 16.799999999999997 Training loss: 0.0194 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [11.  0.  4.]\n",
      "Episode: 2310 Total reward: 17.9 Training loss: 0.0230 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [15.  0. 24.]\n",
      "Episode: 2311 Total reward: 26.1 Training loss: 0.0185 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [10.  0.  2.]\n",
      "Episode: 2312 Total reward: 15.299999999999999 Training loss: 0.0239 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [15.  0.  8.]\n",
      "Episode: 2313 Total reward: 25.8 Training loss: 0.0240 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [10.  4.  8.]\n",
      "Episode: 2314 Total reward: 16.200000000000003 Training loss: 0.0151 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [13.  0. 20.]\n",
      "Episode: 2315 Total reward: 22.0 Training loss: 0.0247 Epsilon: 0.0100 Life: 524\n",
      "Model updated\n",
      "Episode Finish  [11.  0.  8.]\n",
      "Episode: 2316 Total reward: 17.799999999999997 Training loss: 0.0254 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [ 7.  5. 32.]\n",
      "Episode: 2317 Total reward: 10.5 Training loss: 0.0218 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [13.  0. 24.]\n",
      "Episode: 2318 Total reward: 22.1 Training loss: 0.0266 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [ 4. 12. 36.]\n",
      "Episode: 2319 Total reward: 5.3 Training loss: 0.0152 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [12.  1.  8.]\n",
      "Episode: 2320 Total reward: 19.6 Training loss: 0.0270 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [18.  0. 24.]\n",
      "Episode: 2321 Total reward: 32.1 Training loss: 0.0191 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [10.  0. 14.]\n",
      "Episode: 2322 Total reward: 15.900000000000002 Training loss: 0.0178 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [11.  0. 24.]\n",
      "Episode: 2323 Total reward: 18.1 Training loss: 0.0257 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [ 8.  7. 12.]\n",
      "Episode: 2324 Total reward: 13.7 Training loss: 0.0167 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [9. 9. 6.]\n",
      "Episode: 2325 Total reward: 14.4 Training loss: 0.0257 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [13.  0.  4.]\n",
      "Episode: 2326 Total reward: 21.4 Training loss: 0.0186 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [13.  0. 16.]\n",
      "Episode: 2327 Total reward: 21.700000000000003 Training loss: 0.0229 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [12.  0.  8.]\n",
      "Episode: 2328 Total reward: 20.1 Training loss: 0.0266 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [12.  0. 16.]\n",
      "Episode: 2329 Total reward: 19.6 Training loss: 0.0220 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [ 6. 12.  4.]\n",
      "Episode: 2330 Total reward: 10.000000000000002 Training loss: 0.0167 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [13.  0.  8.]\n",
      "Episode: 2331 Total reward: 21.8 Training loss: 0.0103 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [13.  0. 24.]\n",
      "Episode: 2332 Total reward: 22.1 Training loss: 0.0194 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [10.  2. 12.]\n",
      "Episode: 2333 Total reward: 16.3 Training loss: 0.0129 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [17.  0. 24.]\n",
      "Episode: 2334 Total reward: 30.1 Training loss: 0.0233 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [17.  0.  8.]\n",
      "Episode: 2335 Total reward: 29.799999999999997 Training loss: 0.0218 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [14.  0. 24.]\n",
      "Episode: 2336 Total reward: 24.1 Training loss: 0.0124 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [11.  0. 16.]\n",
      "Episode: 2337 Total reward: 17.9 Training loss: 0.0184 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [14.  0. 10.]\n",
      "Episode: 2338 Total reward: 24.0 Training loss: 0.0141 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [13.  0. 30.]\n",
      "Episode: 2339 Total reward: 21.8 Training loss: 0.0234 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [12.  0. 16.]\n",
      "Episode: 2340 Total reward: 19.7 Training loss: 0.0211 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [14.  0. 18.]\n",
      "Episode: 2341 Total reward: 23.900000000000002 Training loss: 0.0182 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [13.  0. 24.]\n",
      "Episode: 2342 Total reward: 22.1 Training loss: 0.0175 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [11.  0.  6.]\n",
      "Episode: 2343 Total reward: 17.9 Training loss: 0.0203 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [14.  0. 10.]\n",
      "Episode: 2344 Total reward: 23.6 Training loss: 0.0198 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [12.  3. 24.]\n",
      "Episode: 2345 Total reward: 20.400000000000002 Training loss: 0.0214 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [10.  1. 10.]\n",
      "Episode: 2346 Total reward: 15.999999999999998 Training loss: 0.0153 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [17.  0. 14.]\n",
      "Episode: 2347 Total reward: 29.7 Training loss: 0.0132 Epsilon: 0.0100 Life: 524\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode Finish  [13.  0.  6.]\n",
      "Episode: 2348 Total reward: 21.599999999999998 Training loss: 0.0195 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [10.  0.  8.]\n",
      "Episode: 2349 Total reward: 15.599999999999998 Training loss: 0.0153 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [13.  0. 24.]\n",
      "Episode: 2350 Total reward: 22.099999999999998 Training loss: 0.0167 Epsilon: 0.0100 Life: 524\n",
      "Stats Saved at 2350\n",
      "Episode Finish  [9. 0. 6.]\n",
      "Episode: 2351 Total reward: 13.9 Training loss: 0.0169 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [14.  0. 10.]\n",
      "Episode: 2352 Total reward: 24.0 Training loss: 0.0194 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [12.  0.  8.]\n",
      "Episode: 2353 Total reward: 20.0 Training loss: 0.0265 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [11.  0.  2.]\n",
      "Episode: 2354 Total reward: 17.200000000000003 Training loss: 0.0201 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [12.  0.  6.]\n",
      "Episode: 2355 Total reward: 19.7 Training loss: 0.0174 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [14.  0. 24.]\n",
      "Episode: 2356 Total reward: 24.1 Training loss: 0.0189 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [12.  4. 24.]\n",
      "Episode: 2357 Total reward: 20.5 Training loss: 0.0326 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [15.  0. 24.]\n",
      "Episode: 2358 Total reward: 26.1 Training loss: 0.0230 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [10.  0. 12.]\n",
      "Episode: 2359 Total reward: 16.1 Training loss: 0.0241 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [13.  0. 24.]\n",
      "Episode: 2360 Total reward: 22.1 Training loss: 0.0146 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [14.  0.  6.]\n",
      "Episode: 2361 Total reward: 23.1 Training loss: 0.0183 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [13.  0. 24.]\n",
      "Episode: 2362 Total reward: 22.1 Training loss: 0.0156 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [14.  0.  4.]\n",
      "Episode: 2363 Total reward: 24.0 Training loss: 0.0235 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [15.  0.  6.]\n",
      "Episode: 2364 Total reward: 25.900000000000002 Training loss: 0.0195 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [ 9.  5. 24.]\n",
      "Episode: 2365 Total reward: 14.6 Training loss: 0.0143 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [ 8.  0. 10.]\n",
      "Episode: 2366 Total reward: 11.799999999999997 Training loss: 0.0247 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [15.  0. 24.]\n",
      "Episode: 2367 Total reward: 26.1 Training loss: 0.0165 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [12.  0.  4.]\n",
      "Episode: 2368 Total reward: 19.5 Training loss: 0.0287 Epsilon: 0.0100 Life: 524\n",
      "Model updated\n",
      "Episode Finish  [14.  0. 30.]\n",
      "Episode: 2369 Total reward: 23.799999999999997 Training loss: 0.0222 Epsilon: 0.0100 Life: 524\n",
      "Episode Finish  [13.  0. 12.]\n",
      "Episode: 2370 Total reward: 22.0 Training loss: 0.0173 Epsilon: 0.0100 Life: 524\n"
     ]
    }
   ],
   "source": [
    "# Saver will help us to save our model\n",
    "saver = tf.train.Saver()\n",
    "GAME = 0\n",
    "t = 0\n",
    "max_life = 0    # Maximum episode life (Proxy for agent performance)\n",
    "life = 0\n",
    "stats_store = []\n",
    "\n",
    "# Buffer to compute rolling statistics \n",
    "tot_reward_buffer, life_buffer, ammo_buffer, kills_buffer, mavg_score, var_score, mavg_ammo_left, mavg_kill_counts, mavg_tot_rewards = [], [] , [], [], [], [], [], [], [] \n",
    "losses_buffer, epsilon_buffer = [], []\n",
    "\n",
    "game_state = game.get_state()\n",
    "misc = game_state.game_variables  # [KILLCOUNT, AMMO, HEALTH]\n",
    "prev_misc = misc\n",
    "\n",
    "\n",
    "if training == True:\n",
    "    with tf.Session() as sess:\n",
    "        # Initialize the variables\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        # Initialize the decay rate (that will use to reduce epsilon) \n",
    "        decay_step = 0\n",
    "        \n",
    "        # Set tau = 0\n",
    "        tau = 0\n",
    "\n",
    "        # Init the game\n",
    "        game.init()\n",
    "        \n",
    "        # Update the parameters of our TargetNetwork with DQN_weights\n",
    "        update_target = update_target_graph()\n",
    "        sess.run(update_target)\n",
    "        \n",
    "        for episode in range(total_episodes):\n",
    "            # Set step to 0\n",
    "            step = 0\n",
    "            # set the reward\n",
    "            reward = 0\n",
    "            \n",
    "            # Initialize the rewards of the episode\n",
    "            episode_rewards = []\n",
    "            \n",
    "            # Make a new episode and observe the first state\n",
    "            game.new_episode(r\".\\ep_rec_last\\ep\" + str(episode) + \"_rec.lmp\")\n",
    "            \n",
    "            state = game.get_state().screen_buffer\n",
    "            if episode == 0:\n",
    "                misc = game.get_state().game_variables  # [KILLCOUNT, AMMO, HEALTH]\n",
    "                prev_misc = misc\n",
    "            \n",
    "            # Remember that stack frame function also call our preprocess function.\n",
    "            state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
    "            \n",
    "            \n",
    "            while step < max_steps:\n",
    "                step += 1\n",
    "                \n",
    "                # Increase the C step\n",
    "                tau += 1\n",
    "                \n",
    "                # Increase decay_step\n",
    "                decay_step += 1\n",
    "                \n",
    "                # With œµ select a random action atat, otherwise select a = argmaxQ(st,a)\n",
    "                action, explore_probability = predict_action(explore_start, explore_stop, decay_rate, decay_step, state, possible_actions)\n",
    "\n",
    "                # Do the action\n",
    "                # Add by Karim\n",
    "                misc = game.get_state().game_variables  # [KILLCOUNT, AMMO, HEALTH]\n",
    "                game.set_action(action)\n",
    "                # 4 for the skip rate as we stack 4 frames, we ask to take the same action four times\n",
    "                game.advance_action(4)\n",
    "                game_state = game.get_state()\n",
    "                # Original version\n",
    "                # reward = game.make_action(action)\n",
    "                reward = game.get_last_reward()\n",
    "                # Look if the episode is finished\n",
    "                done = game.is_episode_finished()\n",
    "                # Add by Karim   \n",
    "                reward = shape_reward(reward, misc, prev_misc)\n",
    "                # Add the reward to total reward\n",
    "                episode_rewards.append(reward)\n",
    "                \n",
    "                # If the game is finished\n",
    "                if done:\n",
    "                    # Add by Karim\n",
    "                    if life > max_life:\n",
    "                        max_life = life\n",
    "                    GAME += 1\n",
    "                    life_buffer.append(life)\n",
    "                    ammo_buffer.append(misc[1])\n",
    "                    kills_buffer.append(misc[0])\n",
    "                    life = 0\n",
    "                    print(\"Episode Finish \", misc)\n",
    "                    \n",
    "                    # the episode ends so no next state\n",
    "                    # 120, 140 => 64, 64\n",
    "                    next_state = np.zeros((120,140), dtype=np.int)\n",
    "                    next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
    "\n",
    "                    # Set step = max_steps to end the episode\n",
    "                    step = max_steps\n",
    "\n",
    "                    # Get the total reward of the episode\n",
    "                    total_reward = np.sum(episode_rewards)\n",
    "                    tot_reward_buffer.append(total_reward)\n",
    "                    \n",
    "                    losses_buffer.append(loss)\n",
    "                    epsilon_buffer.append(explore_probability)\n",
    "                    \n",
    "                    print('Episode: {}'.format(episode),\n",
    "                          'Total reward: {}'.format(total_reward),\n",
    "                          'Training loss: {:.4f}'.format(loss),\n",
    "                          'Epsilon: {:.4f}'.format(explore_probability),\n",
    "                          'Life: {}'.format(max_life)\n",
    "                         )\n",
    "                    \n",
    "                    # Set reward to 0\n",
    "                    reward = 0\n",
    "                    # Add experience to memory\n",
    "                    experience = state, action, reward, next_state, done\n",
    "                    memory.store(experience)\n",
    "\n",
    "                else:\n",
    "                    # Get the next state\n",
    "                    next_state = game.get_state().screen_buffer\n",
    "                    \n",
    "                    # Stack the frame of the next_state\n",
    "                    next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
    "                    \n",
    "\n",
    "                    # Add experience to memory\n",
    "                    experience = state, action, reward, next_state, done\n",
    "                    memory.store(experience)\n",
    "                    \n",
    "                    # st+1 is now our current state\n",
    "                    state = next_state\n",
    "                    life += 1\n",
    "\n",
    "                # print(\"Misc: \" + str(misc) + \"\\tPrev Misc: \" + str(prev_misc))\n",
    "                # Add by Karim\n",
    "                prev_misc = misc\n",
    "\n",
    "                ### LEARNING PART            \n",
    "                # Obtain random mini-batch from memory\n",
    "                tree_idx, batch, ISWeights_mb = memory.sample(batch_size)\n",
    "                \n",
    "                states_mb = np.array([each[0][0] for each in batch], ndmin=3)\n",
    "                actions_mb = np.array([each[0][1] for each in batch])\n",
    "                rewards_mb = np.array([each[0][2] for each in batch]) \n",
    "                next_states_mb = np.array([each[0][3] for each in batch], ndmin=3)\n",
    "                dones_mb = np.array([each[0][4] for each in batch])\n",
    "\n",
    "                target_Qs_batch = []\n",
    "\n",
    "                \n",
    "                ### DOUBLE DQN Logic\n",
    "                # Use DQNNetwork to select the action to take at next_state (a') (action with the highest Q-value)\n",
    "                # Use TargetNetwork to calculate the Q_val of Q(s',a')\n",
    "                \n",
    "                # Get Q values for next_state \n",
    "                q_next_state = sess.run(DQNetwork.output, feed_dict = {DQNetwork.inputs_: next_states_mb})\n",
    "                \n",
    "                # Calculate Qtarget for all actions that state\n",
    "                q_target_next_state = sess.run(TargetNetwork.output, feed_dict = {TargetNetwork.inputs_: next_states_mb})\n",
    "                \n",
    "                \n",
    "                # Set Q_target = r if the episode ends at s+1, otherwise set Q_target = r + gamma * Qtarget(s',a') \n",
    "                for i in range(0, len(batch)):\n",
    "                    terminal = dones_mb[i]\n",
    "                    \n",
    "                    # We got a'\n",
    "                    action = np.argmax(q_next_state[i])\n",
    "\n",
    "                    # If we are in a terminal state, only equals reward\n",
    "                    if terminal:\n",
    "                        target_Qs_batch.append(rewards_mb[i])\n",
    "                        \n",
    "                    else:\n",
    "                        # Take the Qtarget for action a'\n",
    "                        target = rewards_mb[i] + gamma * q_target_next_state[i][action]\n",
    "                        target_Qs_batch.append(target)\n",
    "                        \n",
    "\n",
    "                targets_mb = np.array([each for each in target_Qs_batch])\n",
    "\n",
    "                \n",
    "                _, loss, absolute_errors = sess.run([DQNetwork.optimizer, DQNetwork.loss, DQNetwork.absolute_errors],\n",
    "                                    feed_dict={DQNetwork.inputs_: states_mb,\n",
    "                                               DQNetwork.target_Q: targets_mb,\n",
    "                                               DQNetwork.actions_: actions_mb,\n",
    "                                              DQNetwork.ISWeights_: ISWeights_mb})\n",
    "              \n",
    "                \n",
    "                \n",
    "                # Update priority\n",
    "                memory.batch_update(tree_idx, absolute_errors)\n",
    "                \n",
    "                \n",
    "                # Write TF Summaries\n",
    "                summary = sess.run(write_op, feed_dict={DQNetwork.inputs_: states_mb,\n",
    "                                                   DQNetwork.target_Q: targets_mb,\n",
    "                                                   DQNetwork.actions_: actions_mb,\n",
    "                                              DQNetwork.ISWeights_: ISWeights_mb})\n",
    "                writer.add_summary(summary, episode)\n",
    "                writer.flush()\n",
    "                \n",
    "                if tau > max_tau:\n",
    "                    # Update the parameters of our TargetNetwork with DQN_weights\n",
    "                    update_target = update_target_graph()\n",
    "                    sess.run(update_target)\n",
    "                    tau = 0\n",
    "                    print(\"Model updated\")\n",
    "\n",
    "            # Save model every 200 episodes\n",
    "            if episode % 200 == 0:\n",
    "                save_path = saver.save(sess, \"./models/model.ckpt\")\n",
    "                print(\"Model Saved at \" + str(episode))\n",
    "                \n",
    "            # Save stats every 50 episodes by Karim\n",
    "            if episode % 50 == 0:\n",
    "                print(\"Stats Saved at \" + str(episode))\n",
    "                mavg_tot_rewards.append(np.mean(np.array(tot_reward_buffer)))\n",
    "                mavg_score.append(np.mean(np.array(life_buffer)))\n",
    "                var_score.append(np.var(np.array(life_buffer)))\n",
    "                mavg_ammo_left.append(np.mean(np.array(ammo_buffer)))\n",
    "                mavg_kill_counts.append(np.mean(np.array(kills_buffer)))\n",
    "                with open(r\".\\ddqn_pr_steps_stats.txt\", \"w\") as stats_file:\n",
    "                    stats_file.write('Game: ' + str(GAME) + '\\n')\n",
    "                    stats_file.write('Max Score: ' + str(max_life) + '\\n')\n",
    "                    stats_file.write('mavg_score: ' + str(mavg_score) + '\\n')\n",
    "                    stats_file.write('var_score: ' + str(var_score) + '\\n')\n",
    "                    stats_file.write('mavg_ammo_left: ' + str(mavg_ammo_left) + '\\n')\n",
    "                    stats_file.write('mavg_kill_counts: ' + str(mavg_kill_counts) + '\\n')\n",
    "                    stats_file.write('mavg_rewards: ' + str(total_reward) + \"\\n\")\n",
    "                with open(r\".\\ddqn_pr_steps_stats\" + str(episode) + \".pickle\", 'wb') as handle:\n",
    "                    pickle.dump(stats_store.append({'game': GAME, 'max_score': max_life, 'mavg_score': mavg_score, \n",
    "                                                   'var_score': var_score, 'mavg_ammo_left': mavg_ammo_left,\n",
    "                                                   'mavg_kill_counts': mavg_kill_counts,\n",
    "                                                   'mavg_tot_rewards': mavg_tot_rewards,\n",
    "                                                   'life_buffer': life_buffer, 'ammo_buffer': ammo_buffer, \n",
    "                                                   'kills_buffer': kills_buffer, 'tot_reward_buffer': tot_reward_buffer, \n",
    "                                                   'losses': losses_buffer, 'epsilon': epsilon_buffer}), \n",
    "                                handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "                                        \n",
    "    with open(r\".\\buffer_dic_data.pickle\", 'wb') as handle:\n",
    "        pickle.dump(stats_store.append({'life_buffer': life_buffer, 'ammo_buffer': ammo_buffer, \n",
    "                                        'kills_buffer': kills_buffer, 'tot_reward_buffer': tot_reward_buffer, \n",
    "                                        'losses': losses_buffer, 'epsilon': epsilon_buffer}), \n",
    "                    handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Episode: 0 Total reward: -89.26519775390625 Training loss: 0.6859 Explore P: 0.9919\n",
    "Model Saved\n",
    "Episode: 1 Total reward: -115.13249206542969 Training loss: 1.0044 Explore P: 0.9872\n",
    "Episode: 2 Total reward: -88.6678466796875 Training loss: 30.3196 Explore P: 0.9760\n",
    "Episode: 3 Total reward: -79.75584411621094 Training loss: 0.4285 Explore P: 0.9687\n",
    "Episode: 4 Total reward: -112.888916015625 Training loss: 17.6260 Explore P: 0.9616\n",
    "Episode: 5 Total reward: -72.01809692382812 Training loss: 0.3325 Explore P: 0.9566\n",
    "Model Saved\n",
    "Episode: 6 Total reward: -91.27947998046875 Training loss: 11.2775 Explore P: 0.9489\n",
    "Episode: 7 Total reward: -96.70150756835938 Training loss: 0.8339 Explore P: 0.9412\n",
    "Episode: 8 Total reward: -89.98007202148438 Training loss: 10.4413 Explore P: 0.9368\n",
    "Episode: 9 Total reward: -78.67872619628906 Training loss: 13.1729 Explore P: 0.9292\n",
    "Episode: 10 Total reward: -114.67742919921875 Training loss: 30.3574 Explore P: 0.9192\n",
    "Model Saved\n",
    "Episode: 11 Total reward: -96.40922546386719 Training loss: 1.3338 Explore P: 0.9128\n",
    "Episode: 12 Total reward: -55.94306945800781 Training loss: 1.0281 Explore P: 0.9030\n",
    "Episode: 13 Total reward: -90.61083984375 Training loss: 17.3349 Explore P: 0.8908\n",
    "Episode: 14 Total reward: -110.54817199707031 Training loss: 11.5495 Explore P: 0.8845\n",
    "Episode: 15 Total reward: -86.7437744140625 Training loss: 8.5952 Explore P: 0.8773\n",
    "Model Saved\n",
    "Episode: 16 Total reward: -108.89964294433594 Training loss: 1.6884 Explore P: 0.8730\n",
    "Episode: 17 Total reward: -59.06085205078125 Training loss: 1.4325 Explore P: 0.8659\n",
    "Episode: 18 Total reward: -102.50300598144531 Training loss: 1.0175 Explore P: 0.8593\n",
    "Episode: 19 Total reward: -100.752685546875 Training loss: 7.5007 Explore P: 0.8556\n",
    "Episode: 20 Total reward: -111.81524658203125 Training loss: 5.0652 Explore P: 0.8487\n",
    "Model Saved\n",
    "Episode: 21 Total reward: -104.21478271484375 Training loss: 1.5522 Explore P: 0.8395\n",
    "Episode: 22 Total reward: -112.78564453125 Training loss: 5.5151 Explore P: 0.8359\n",
    "Episode: 23 Total reward: -82.05340576171875 Training loss: 1.2886 Explore P: 0.8292\n",
    "Episode: 24 Total reward: -111.15492248535156 Training loss: 0.5443 Explore P: 0.8225\n",
    "Episode: 25 Total reward: -73.76707458496094 Training loss: 1.5218 Explore P: 0.8156\n",
    "Model Saved\n",
    "Episode: 26 Total reward: -93.35487365722656 Training loss: 0.9257 Explore P: 0.8088\n",
    "Episode: 27 Total reward: -104.47758483886719 Training loss: 2.1761 Explore P: 0.8023\n",
    "Episode: 28 Total reward: -81.60890197753906 Training loss: 0.5991 Explore P: 0.7958\n",
    "Episode: 29 Total reward: -100.63589477539062 Training loss: 1.7395 Explore P: 0.7894\n",
    "Episode: 30 Total reward: -88.62884521484375 Training loss: 8.3556 Explore P: 0.7830\n",
    "Model Saved\n",
    "Episode: 31 Total reward: -105.23612976074219 Training loss: 0.8298 Explore P: 0.7767\n",
    "Episode: 32 Total reward: -111.5128173828125 Training loss: 0.6878 Explore P: 0.7711\n",
    "Episode: 33 Total reward: -107.63644409179688 Training loss: 0.7860 Explore P: 0.7651\n",
    "Episode: 34 Total reward: -99.78999328613281 Training loss: 1.9388 Explore P: 0.7567\n",
    "Episode: 35 Total reward: -107.68731689453125 Training loss: 3.5948 Explore P: 0.7481\n",
    "Model Saved\n",
    "Episode: 36 Total reward: -112.137451171875 Training loss: 0.4563 Explore P: 0.7421\n",
    "Episode: 37 Total reward: -50.57890319824219 Training loss: 0.5308 Explore P: 0.7361\n",
    "Episode: 38 Total reward: -73.00382995605469 Training loss: 1.9759 Explore P: 0.7302\n",
    "Episode: 39 Total reward: -80.82208251953125 Training loss: 0.2969 Explore P: 0.7243\n",
    "Episode: 40 Total reward: -97.41578674316406 Training loss: 16.1484 Explore P: 0.7185\n",
    "Model Saved\n",
    "Episode: 41 Total reward: -77.568115234375 Training loss: 0.2420 Explore P: 0.7128\n",
    "Episode: 42 Total reward: -103.93637084960938 Training loss: 0.1838 Explore P: 0.7026\n",
    "Episode: 43 Total reward: -81.61286926269531 Training loss: 0.3259 Explore P: 0.6948\n",
    "Episode: 44 Total reward: -91.02716064453125 Training loss: 0.3337 Explore P: 0.6859\n",
    "Episode: 45 Total reward: -98.70729064941406 Training loss: 2.1673 Explore P: 0.6804\n",
    "Model Saved\n",
    "Episode: 46 Total reward: -115.98574829101562 Training loss: 14.9863 Explore P: 0.6726\n",
    "Episode: 47 Total reward: -100.81024169921875 Training loss: 2.0342 Explore P: 0.6654\n",
    "Episode: 48 Total reward: -60.25152587890625 Training loss: 0.2753 Explore P: 0.6569\n",
    "Episode: 49 Total reward: -67.41098022460938 Training loss: 0.3018 Explore P: 0.6486\n",
    "Episode: 50 Total reward: -105.46267700195312 Training loss: 1.0995 Explore P: 0.6413\n",
    "Model Saved\n",
    "Episode: 51 Total reward: -73.07460021972656 Training loss: 0.1813 Explore P: 0.6362\n",
    "Episode: 52 Total reward: -96.30844116210938 Training loss: 0.2939 Explore P: 0.6310\n",
    "Episode: 53 Total reward: -94.21073913574219 Training loss: 0.4776 Explore P: 0.6284\n",
    "Episode: 54 Total reward: -65.328125 Training loss: 0.2104 Explore P: 0.6233\n",
    "Episode: 55 Total reward: -66.21479797363281 Training loss: 3.2012 Explore P: 0.6183\n",
    "Model Saved\n",
    "Episode: 56 Total reward: -94.83515930175781 Training loss: 0.5179 Explore P: 0.6136\n",
    "Episode: 57 Total reward: -92.63566589355469 Training loss: 7.6108 Explore P: 0.6068\n",
    "Episode: 58 Total reward: -114.22836303710938 Training loss: 0.1981 Explore P: 0.5979\n",
    "Episode: 59 Total reward: -109.301025390625 Training loss: 0.1633 Explore P: 0.5931\n",
    "Episode: 60 Total reward: -69.18382263183594 Training loss: 0.3027 Explore P: 0.5883\n",
    "Model Saved\n",
    "Episode: 61 Total reward: -96.5882568359375 Training loss: 0.2388 Explore P: 0.5856\n",
    "Episode: 62 Total reward: -115.95585632324219 Training loss: 0.2598 Explore P: 0.5815\n",
    "Episode: 63 Total reward: -91.42893981933594 Training loss: 3.1792 Explore P: 0.5768\n",
    "Episode: 64 Total reward: -78.47196960449219 Training loss: 0.1737 Explore P: 0.5722\n",
    "Episode: 65 Total reward: -33.51860046386719 Training loss: 16.5782 Explore P: 0.5676\n",
    "Model Saved\n",
    "Episode: 66 Total reward: -52.46026611328125 Training loss: 0.7277 Explore P: 0.5630\n",
    "Episode: 67 Total reward: -104.60054016113281 Training loss: 0.1622 Explore P: 0.5585\n",
    "Episode: 68 Total reward: -77.99497985839844 Training loss: 2.5138 Explore P: 0.5540\n",
    "Episode: 69 Total reward: -54.47041320800781 Training loss: 0.1590 Explore P: 0.5496\n",
    "Episode: 70 Total reward: -63.22991943359375 Training loss: 0.1965 Explore P: 0.5452\n",
    "Model Saved\n",
    "Episode: 71 Total reward: -87.78546142578125 Training loss: 0.3122 Explore P: 0.5375\n",
    "Episode: 72 Total reward: -96.14764404296875 Training loss: 0.1515 Explore P: 0.5351\n",
    "Episode: 73 Total reward: -69.32623291015625 Training loss: 2.8430 Explore P: 0.5308\n",
    "Episode: 74 Total reward: -13.840484619140625 Training loss: 0.2721 Explore P: 0.5266\n",
    "Episode: 75 Total reward: -89.6734619140625 Training loss: 0.1506 Explore P: 0.5213\n",
    "Model Saved\n",
    "Episode: 76 Total reward: -64.53419494628906 Training loss: 1.8367 Explore P: 0.5171\n",
    "Episode: 77 Total reward: -106.41300964355469 Training loss: 0.3183 Explore P: 0.5072\n",
    "Episode: 78 Total reward: -50.4837646484375 Training loss: 0.2255 Explore P: 0.5033\n",
    "Episode: 79 Total reward: -34.91241455078125 Training loss: 0.1923 Explore P: 0.4976\n",
    "Episode: 80 Total reward: -115.21119689941406 Training loss: 0.1336 Explore P: 0.4950\n",
    "Model Saved\n",
    "Episode: 81 Total reward: -73.21771240234375 Training loss: 0.1376 Explore P: 0.4911\n",
    "Episode: 82 Total reward: -62.74360656738281 Training loss: 0.6687 Explore P: 0.4871\n",
    "Episode: 83 Total reward: -15.30194091796875 Training loss: 0.1503 Explore P: 0.4778\n",
    "Episode: 84 Total reward: -74.79470825195312 Training loss: 0.1727 Explore P: 0.4740\n",
    "Episode: 85 Total reward: -54.167205810546875 Training loss: 0.1432 Explore P: 0.4702\n",
    "Model Saved\n",
    "Episode: 86 Total reward: -62.83433532714844 Training loss: 0.1632 Explore P: 0.4665\n",
    "Episode: 87 Total reward: -82.97991943359375 Training loss: 0.1923 Explore P: 0.4644\n",
    "Episode: 88 Total reward: -72.07733154296875 Training loss: 0.2274 Explore P: 0.4607\n",
    "Episode: 89 Total reward: -55.19401550292969 Training loss: 0.1261 Explore P: 0.4570\n",
    "Episode: 90 Total reward: -76.98689270019531 Training loss: 0.7601 Explore P: 0.4505\n",
    "Model Saved\n",
    "Episode: 91 Total reward: -65.32528686523438 Training loss: 0.3138 Explore P: 0.4469\n",
    "Episode: 92 Total reward: -50.588714599609375 Training loss: 0.2203 Explore P: 0.4435\n",
    "Episode: 93 Total reward: -70.39730834960938 Training loss: 1.2486 Explore P: 0.4415\n",
    "\n",
    "Episode: 94 Total reward: 70.74258422851562 Training loss: 0.4045 Explore P: 0.4366\n",
    "Episode: 95 Total reward: -11.190460205078125 Training loss: 0.2244 Explore P: 0.4331\n",
    "Model Saved\n",
    "Episode: 96 Total reward: -22.803070068359375 Training loss: 0.4332 Explore P: 0.4297\n",
    "Episode: 97 Total reward: -43.600616455078125 Training loss: 2.4079 Explore P: 0.4265\n",
    "Episode: 98 Total reward: -74.661376953125 Training loss: 0.3113 Explore P: 0.4246\n",
    "Episode: 99 Total reward: -32.23060607910156 Training loss: 0.1899 Explore P: 0.4212\n",
    "Episode: 100 Total reward: -66.32485961914062 Training loss: 0.1400 Explore P: 0.4167\n",
    "Model Saved\n",
    "Episode: 101 Total reward: -15.644882202148438 Training loss: 0.0826 Explore P: 0.4134\n",
    "Episode: 102 Total reward: 44.1182861328125 Training loss: 0.1348 Explore P: 0.4101\n",
    "Episode: 103 Total reward: -61.74578857421875 Training loss: 0.6734 Explore P: 0.4058\n",
    "Episode: 104 Total reward: -87.16415405273438 Training loss: 0.2358 Explore P: 0.4026\n",
    "Episode: 105 Total reward: -90.69143676757812 Training loss: 0.4390 Explore P: 0.3939\n",
    "Model Saved\n",
    "Episode: 106 Total reward: -56.23359680175781 Training loss: 0.1456 Explore P: 0.3908\n",
    "Episode: 107 Total reward: -41.05461120605469 Training loss: 0.9647 Explore P: 0.3877\n",
    "Episode: 108 Total reward: -1.7525482177734375 Training loss: 0.4109 Explore P: 0.3846\n",
    "Episode: 109 Total reward: -37.95100402832031 Training loss: 0.2784 Explore P: 0.3815\n",
    "Episode: 110 Total reward: -71.89024353027344 Training loss: 0.1012 Explore P: 0.3786\n",
    "Model Saved\n",
    "Episode: 111 Total reward: -72.90853881835938 Training loss: 1.4025 Explore P: 0.3756\n",
    "Model updated\n",
    "Episode: 112 Total reward: -56.199127197265625 Training loss: 7.5684 Explore P: 0.3727\n",
    "Episode: 113 Total reward: -77.53300476074219 Training loss: 3.6123 Explore P: 0.3698\n",
    "Episode: 114 Total reward: -50.253692626953125 Training loss: 6.0007 Explore P: 0.3668\n",
    "Episode: 115 Total reward: 18.208023071289062 Training loss: 6.2701 Explore P: 0.3639\n",
    "Model Saved\n",
    "Episode: 116 Total reward: -74.686767578125 Training loss: 7.9382 Explore P: 0.3610\n",
    "Episode: 117 Total reward: -76.70317077636719 Training loss: 3.9754 Explore P: 0.3593\n",
    "Episode: 118 Total reward: 18.843551635742188 Training loss: 1.0298 Explore P: 0.3554\n",
    "Episode: 119 Total reward: 1.3499298095703125 Training loss: 1.5573 Explore P: 0.3525\n",
    "Episode: 120 Total reward: -0.566131591796875 Training loss: 0.4084 Explore P: 0.3497\n",
    "Model Saved\n",
    "Episode: 121 Total reward: 20.053070068359375 Training loss: 0.6762 Explore P: 0.3470\n",
    "Episode: 122 Total reward: -79.74948120117188 Training loss: 0.5085 Explore P: 0.3443\n",
    "Episode: 123 Total reward: -68.07794189453125 Training loss: 0.6844 Explore P: 0.3416\n",
    "Episode: 124 Total reward: 20.166915893554688 Training loss: 0.2775 Explore P: 0.3389\n",
    "Episode: 125 Total reward: -87.4755859375 Training loss: 0.3127 Explore P: 0.3364\n",
    "Model Saved\n",
    "Episode: 126 Total reward: -17.0537109375 Training loss: 0.3796 Explore P: 0.3337\n",
    "Episode: 127 Total reward: 5.201812744140625 Training loss: 0.6150 Explore P: 0.3311\n",
    "Episode: 128 Total reward: -32.572784423828125 Training loss: 0.2595 Explore P: 0.3285\n",
    "Episode: 129 Total reward: -43.18853759765625 Training loss: 0.4992 Explore P: 0.3259\n",
    "Episode: 130 Total reward: -84.01849365234375 Training loss: 0.3338 Explore P: 0.3226\n",
    "Model Saved\n",
    "Episode: 131 Total reward: -99.23286437988281 Training loss: 1.2294 Explore P: 0.3200\n",
    "Episode: 132 Total reward: -27.938064575195312 Training loss: 0.9042 Explore P: 0.3175\n",
    "Episode: 133 Total reward: 2.96868896484375 Training loss: 0.3110 Explore P: 0.3151\n",
    "Episode: 134 Total reward: -49.97503662109375 Training loss: 0.4291 Explore P: 0.3119\n",
    "Episode: 135 Total reward: 8.848037719726562 Training loss: 0.9113 Explore P: 0.3095\n",
    "Model Saved\n",
    "Episode: 136 Total reward: -78.30146789550781 Training loss: 1.1113 Explore P: 0.3064\n",
    "Episode: 137 Total reward: -35.61848449707031 Training loss: 0.2758 Explore P: 0.3039\n",
    "Episode: 138 Total reward: -80.23164367675781 Training loss: 1.1325 Explore P: 0.3015\n",
    "Episode: 139 Total reward: -41.44696044921875 Training loss: 0.2293 Explore P: 0.2993\n",
    "Episode: 140 Total reward: -63.55998229980469 Training loss: 0.5988 Explore P: 0.2969\n",
    "Model Saved\n",
    "Episode: 141 Total reward: -74.58718872070312 Training loss: 0.3622 Explore P: 0.2956\n",
    "Episode: 142 Total reward: -44.1854248046875 Training loss: 0.8818 Explore P: 0.2933\n",
    "Episode: 143 Total reward: -43.17417907714844 Training loss: 0.6441 Explore P: 0.2918\n",
    "Episode: 144 Total reward: -35.05082702636719 Training loss: 0.1932 Explore P: 0.2885\n",
    "Episode: 145 Total reward: 2.6080322265625 Training loss: 0.2974 Explore P: 0.2857\n",
    "Model Saved\n",
    "Episode: 146 Total reward: -75.66334533691406 Training loss: 0.2797 Explore P: 0.2828\n",
    "Episode: 147 Total reward: -79.89767456054688 Training loss: 14.5457 Explore P: 0.2805\n",
    "Episode: 148 Total reward: -65.21456909179688 Training loss: 0.7638 Explore P: 0.2783\n",
    "Episode: 149 Total reward: 13.195510864257812 Training loss: 0.3936 Explore P: 0.2761\n",
    "Episode: 150 Total reward: 60.77146911621094 Training loss: 1.1485 Explore P: 0.2739\n",
    "Model Saved\n",
    "Episode: 151 Total reward: -67.01502990722656 Training loss: 1.1541 Explore P: 0.2710\n",
    "Episode: 152 Total reward: 7.119903564453125 Training loss: 0.4257 Explore P: 0.2689\n",
    "Episode: 153 Total reward: 13.754486083984375 Training loss: 0.4931 Explore P: 0.2639\n",
    "Episode: 154 Total reward: -67.7314453125 Training loss: 0.5301 Explore P: 0.2618\n",
    "Episode: 155 Total reward: -61.25654602050781 Training loss: 0.3877 Explore P: 0.2599\n",
    "Model Saved\n",
    "Episode: 156 Total reward: -1.2131805419921875 Training loss: 0.3397 Explore P: 0.2579\n",
    "Episode: 157 Total reward: -26.2254638671875 Training loss: 0.1870 Explore P: 0.2558\n",
    "Episode: 158 Total reward: 71.63455200195312 Training loss: 0.3283 Explore P: 0.2538\n",
    "Episode: 159 Total reward: -41.72747802734375 Training loss: 0.6035 Explore P: 0.2520\n",
    "Episode: 160 Total reward: -75.83839416503906 Training loss: 0.5253 Explore P: 0.2488\n",
    "Model Saved\n",
    "Episode: 161 Total reward: 3.0420074462890625 Training loss: 0.8875 Explore P: 0.2468\n",
    "Episode: 162 Total reward: -21.011383056640625 Training loss: 0.2739 Explore P: 0.2449\n",
    "Episode: 163 Total reward: -19.587127685546875 Training loss: 1.2479 Explore P: 0.2431\n",
    "Episode: 164 Total reward: -53.40458679199219 Training loss: 1.2350 Explore P: 0.2413\n",
    "Episode: 165 Total reward: -59.686767578125 Training loss: 0.4527 Explore P: 0.2395\n",
    "Model Saved\n",
    "Episode: 166 Total reward: -53.43865966796875 Training loss: 12.8202 Explore P: 0.2384\n",
    "Episode: 167 Total reward: 4.73968505859375 Training loss: 0.2532 Explore P: 0.2366\n",
    "Episode: 168 Total reward: -42.3804931640625 Training loss: 0.6826 Explore P: 0.2347\n",
    "Episode: 169 Total reward: -1.4572296142578125 Training loss: 0.5197 Explore P: 0.2329\n",
    "Episode: 170 Total reward: -39.27558898925781 Training loss: 11.5407 Explore P: 0.2311\n",
    "Model Saved\n",
    "Episode: 171 Total reward: 8.362579345703125 Training loss: 0.2713 Explore P: 0.2295\n",
    "Episode: 172 Total reward: 14.519943237304688 Training loss: 7.7963 Explore P: 0.2277\n",
    "Episode: 173 Total reward: -58.884429931640625 Training loss: 0.3072 Explore P: 0.2259\n",
    "Episode: 174 Total reward: -93.07179260253906 Training loss: 0.6735 Explore P: 0.2235\n",
    "Episode: 175 Total reward: -60.440277099609375 Training loss: 0.6426 Explore P: 0.2218\n",
    "Model Saved\n",
    "Episode: 176 Total reward: 24.163375854492188 Training loss: 0.5932 Explore P: 0.2201\n",
    "Episode: 177 Total reward: -74.15121459960938 Training loss: 0.1940 Explore P: 0.2191\n",
    "Episode: 178 Total reward: -47.54103088378906 Training loss: 0.9826 Explore P: 0.2174\n",
    "Episode: 179 Total reward: -88.96371459960938 Training loss: 0.6407 Explore P: 0.2157\n",
    "Episode: 180 Total reward: 86.02571105957031 Training loss: 0.3157 Explore P: 0.2134\n",
    "Model Saved\n",
    "Episode: 181 Total reward: -8.269500732421875 Training loss: 1.0492 Explore P: 0.2118\n",
    "Episode: 182 Total reward: 37.916839599609375 Training loss: 0.3531 Explore P: 0.2102\n",
    "Episode: 183 Total reward: 28.824462890625 Training loss: 0.3685 Explore P: 0.2086\n",
    "Episode: 184 Total reward: -103.504150390625 Training loss: 0.8678 Explore P: 0.2077\n",
    "Episode: 185 Total reward: -33.638336181640625 Training loss: 0.5436 Explore P: 0.2062\n",
    "Model Saved\n",
    "Episode: 186 Total reward: -46.80809020996094 Training loss: 0.8421 Explore P: 0.2046\n",
    "\n",
    "Episode: 187 Total reward: 4.5064849853515625 Training loss: 0.2865 Explore P: 0.2030\n",
    "Episode: 188 Total reward: -10.029891967773438 Training loss: 0.4644 Explore P: 0.2014\n",
    "Episode: 189 Total reward: -35.31138610839844 Training loss: 0.3323 Explore P: 0.1999\n",
    "Episode: 190 Total reward: 22.30352783203125 Training loss: 0.6971 Explore P: 0.1984\n",
    "Model Saved\n",
    "Episode: 191 Total reward: -54.252655029296875 Training loss: 0.7283 Explore P: 0.1968\n",
    "Episode: 192 Total reward: -94.67848205566406 Training loss: 1.4658 Explore P: 0.1959\n",
    "Episode: 193 Total reward: -38.33479309082031 Training loss: 0.2945 Explore P: 0.1944\n",
    "Episode: 194 Total reward: -96.05851745605469 Training loss: 0.2530 Explore P: 0.1929\n",
    "Episode: 195 Total reward: -16.951339721679688 Training loss: 0.8220 Explore P: 0.1914\n",
    "Model Saved\n",
    "Episode: 196 Total reward: -104.72447204589844 Training loss: 0.4501 Explore P: 0.1900\n",
    "Episode: 197 Total reward: -3.453094482421875 Training loss: 0.4974 Explore P: 0.1886\n",
    "Episode: 198 Total reward: -26.187362670898438 Training loss: 0.2195 Explore P: 0.1872\n",
    "Episode: 199 Total reward: -98.55648803710938 Training loss: 0.2501 Explore P: 0.1864\n",
    "Episode: 200 Total reward: -16.166595458984375 Training loss: 0.3163 Explore P: 0.1850\n",
    "Model Saved"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Watch our Agent play üëÄ\n",
    "Now that we trained our agent, we can test it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    \n",
    "    game = DoomGame()\n",
    "    \n",
    "    # Load the correct configuration (TESTING)\n",
    "    game.load_config(\"defend_the_center.cfg\")\n",
    "    \n",
    "    # Load the correct scenario (in our case deadly_corridor scenario)\n",
    "    game.set_doom_scenario_path(\"defend_the_center.wad\")\n",
    "    \n",
    "    game.init()    \n",
    "    \n",
    "    # Load the model\n",
    "    saver.restore(sess, \"./models/model.ckpt\")\n",
    "    game.init()\n",
    "    \n",
    "    for i in range(10):\n",
    "        \n",
    "        game.new_episode()\n",
    "        state = game.get_state().screen_buffer\n",
    "        state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
    "    \n",
    "        while not game.is_episode_finished():\n",
    "            ## EPSILON GREEDY STRATEGY\n",
    "            # Choose action a from state s using epsilon greedy.\n",
    "            ## First we randomize a number\n",
    "            exp_exp_tradeoff = np.random.rand()\n",
    "            \n",
    "\n",
    "            explore_probability = 0.01\n",
    "    \n",
    "            if (explore_probability > exp_exp_tradeoff):\n",
    "                # Make a random action (exploration)\n",
    "                action = random.choice(possible_actions)\n",
    "        \n",
    "            else:\n",
    "                # Get action from Q-network (exploitation)\n",
    "                # Estimate the Qs values state\n",
    "                Qs = sess.run(DQNetwork.output, feed_dict = {DQNetwork.inputs_: state.reshape((1, *state.shape))})\n",
    "        \n",
    "                # Take the biggest Q value (= the best action)\n",
    "                choice = np.argmax(Qs)\n",
    "                action = possible_actions[int(choice)]\n",
    "            \n",
    "            game.make_action(action)\n",
    "            done = game.is_episode_finished()\n",
    "        \n",
    "            if done:\n",
    "                break  \n",
    "                \n",
    "            else:\n",
    "                next_state = game.get_state().screen_buffer\n",
    "                next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
    "                state = next_state\n",
    "        \n",
    "        score = game.get_total_reward()\n",
    "        print(\"Score: \", score)\n",
    "    \n",
    "    game.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (rl-tf-gpu-opengym)",
   "language": "python",
   "name": "rl-tf-gpu-opengym"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
